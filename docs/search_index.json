[["index.html", "DEQ R Methods Encyclopedia Chapter 1 Background", " DEQ R Methods Encyclopedia DEQ R Development Team 2022-03-17 Chapter 1 Background The purpose of this book is to describe common tasks undertaken by DEQ staff and clearly outline similar methodologies in R in order to promote learning opportunities for new R users. This project is an example of a bookdown report built using R and RStudios R Markdown. Many authors have contributed to this effort: Emma Jones (emma.jones@deq.virginia.gov) more one day soon Feel free to explore different chapters to learn about specific tasks and how to complete them efficiently and transparently in R. "],["want-to-contribute.html", "1.1 Want to Contribute?", " 1.1 Want to Contribute? We are always looking for collaborators. All reprex (reproducible examples) must be published in R markdown using the bookdown package and bookdown output style. Contributions are not limited to R as R markdown natively utilizes many languages including python and SQL. To get started with bookdown, see Yihui Xies technical references book bookdown: Authoring Books and Technical Documents with R Markdown or watch his webinar introducing bookdown. "],["queryInternalDataSources.html", "Chapter 2 Query Internal Data Sources", " Chapter 2 Query Internal Data Sources Accessing internal DEQ data sources has been a longtime goal of many R programmers. Connecting a local R instance to raw data allows for increased automation and reduces redundant data copies on local drives. However, querying data directly from internal resources into ones local environment is a privilege and requires a number of R skills and approval from OIS. Database operations rely on the SQL language. Users must first familiarize themselves with the basics of database operations with SQL. The DataCamp Introduction to SQL course is a good starting point. Additionally, the DataCarpentry SQL databases and R course provides is a good practice before seeking a direct, read-only connection to ODS (the SQLServer back end of CEDS). After you have *demonstrated proficiency** manipulating smaller, local data sources in R, you may submit a Special Network Access Form to OIS seeking access to ODS data view using your Microsoft credentials. After OIS approval, you may access ODS from R following the connection instructions in the Connect to ODS module. Alternatively, data pinned on the R server are available to all staff (on the internal network or VPN) without OIS authorization. Many pre-analyzed data sources are available to be pulled into your local R environment to assist with projects. See the Connecting to R Connect (for Pinned Data) module for more information on connecting your local environment to the R server to query these internal data resources. "],["connectToConnectPins.html", "2.1 Connecting to R Connect (for Pinned Data)", " 2.1 Connecting to R Connect (for Pinned Data) Section Contact: Emma Jones (emma.jones@deq.virginia.gov) Pinned data are powerful internal data sources that are developed by R programmers that expedite analyses by offering pre-analyzed data for others to use. The data products are often the end result of multiple analytical steps that one would need to repeat each time certain datasets are needed for analyses. Because these common datasets could prove useful for many end users, these datasets are pinned to the R server to expedite (and standardize) the acquisition of common data needs. Examples of pinned data are VSCI/VCPMI scores, station level geospatial data, station level Water Quality Standards (WQS) information, and many more. To access this data, you must first link your local R environment to the R Connect server. YOU MUST BE ON THE DEQ NETWORK OR VPN IN ORDER TO ACCESS ANY INTERNAL DATA RESOURCES Additionally, you must access pinned data using the pins library version 0.4.3. More recent versions of the pins package will not successfully connect to the version of pins on the R server. #install.packages(&quot;https://cran.r-project.org/src/contrib/Archive/pins/pins_0.4.3.tar.gz&quot;, repos=NULL, type=&quot;source&quot;) library(tidyverse) library(pins) library(config) Each time you wish to access pinned data, you must connect to the R server like you might connect to a database. To gain access to the R server, you need to have the API key information. Obtain this information by emailing Emma Jones (emma.jones@deq.virginia.gov) and specify you want access to pinned data on the R server. 2.1.1 API Keys To connect to the R server, you must use the appropriate API key. NEVER HARD CODE ACCESS CODES INTO YOUR SCRIPTS This means you should NEVER place the actual API key into your code ANYWHERE. You must source this information from a secret DEQconfig.yml file (obtained from Emma Jones and stored locally on your computer) in order to access data from the R server in your local R environment. Read more about config files. 2.1.2 Connect to the R server Use the following script to source the server API key and connect to the R server. # Server connection things conn &lt;- config::get(file = &quot;PINSconfig.yml&quot;, &quot;connectionSettings&quot;) # get configuration settings board_register_rsconnect(key = conn$CONNECT_API_KEY, server = conn$CONNECT_SERVER) 2.1.3 Browse Available Pins Once you are connected to the R server, you can view available pins and metadata. as_tibble(pin_find(board = &#39;rsconnect&#39;)) 2.1.4 Access a Pin To bring a particular pin into your local R environment, simply create an object and call the pin from the R server. totalHabitat &lt;- pin_get(&#39;ejones/totalHabitatScore&#39;, board = &#39;rsconnect&#39;) head(totalHabitat) ## # A tibble: 6 x 8 ## StationID `Collection Date` HabSampID `Field Team` `HabSample Comm~ Gradient ## &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2-LIJ003~ 2021-10-18 10:00:00 2-LIJ165~ bvw, rtt, j~ &lt;NA&gt; High ## 2 2-XUL000~ 2013-04-09 09:30:00 2-XUL GJD &lt;NA&gt; High ## 3 6BTHC000~ 2020-05-12 12:30:00 6BTHC159~ LLS &lt;NA&gt; High ## 4 6CLAE001~ 2020-06-04 12:30:00 6CLAE159~ LLS &lt;NA&gt; High ## 5 1AABR000~ 2012-10-23 14:30:00 ABR1577 rtt &lt;NA&gt; High ## 6 6AABR000~ 2021-04-07 14:45:00 ABR16496 LLS &lt;NA&gt; High ## # ... with 2 more variables: Season &lt;chr&gt;, `Total Habitat Score` &lt;dbl&gt; 2.1.5 Query (some of) a Pin Sometimes you do not want to bring an entire dataset into your environment. You can query just the information you want to bring back from a pinned data source by using simple dplyr verbs. pin_get(&#39;ejones/VSCIresults&#39;, board = &#39;rsconnect&#39;) %&gt;% # Query one station between a set date range filter(StationID == &#39;2-JKS023.61&#39; &amp; between(as.Date(`Collection Date`), as.Date(&#39;2015-01-01&#39;), as.Date(&#39;2020-12-31&#39;))) %&gt;% # only bring back rarified Samples filter(`Target Count` == 110) %&gt;% dplyr::select(StationID, `Collection Date`, everything()) %&gt;% datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) "],["connectToODS.html", "2.2 Connect to ODS", " 2.2 Connect to ODS Section Contact: Emma Jones (emma.jones@deq.virginia.gov) Coming Soon! "],["queryDataFromODS.html", "2.3 Querying Data from ODS", " 2.3 Querying Data from ODS Coming Soon! "],["commonWQMqueries.html", "2.4 Common WQM queries", " 2.4 Common WQM queries Coming Soon! "],["commonWQAqueries.html", "2.5 Common WQA queries", " 2.5 Common WQA queries Coming Soon! "],["commonTMDLqueries.html", "2.6 Common TMDL queries", " 2.6 Common TMDL queries Coming Soon! "],["spatialAnalysis.html", "Chapter 3 Spatial Analysis", " Chapter 3 Spatial Analysis R is a powerful tool for spatial analysis. Among the many benefits to using R as a GIS are the reproducibility of codified methods, which make sharing procedures and updating results after underlying data updates easy and efficient. There are many resources for learning how to use R for geospatial operations. A favorite is the Geocomputation with R online book (free) by Robin Lovelace. To familiarize yourself with common geospatial procedures in R, the following chapter has been developed by DEQ staff to help colleagues with typical geospatial tasks. This is by no means a comprehensive introduction to geospatial techniques in R. Please see other resources for more background information on underlying geospatial principles and methods in R. "],["spatialDataFromTabularData.html", "3.1 Spatial Data from Tabular Data", " 3.1 Spatial Data from Tabular Data Section Contact: Emma Jones (emma.jones@deq.virginia.gov) Location information can be critical to efficiently analyzing a dataset. Often, spatial data is included into spreadsheets as Latitude and Longitude fields, which can quickly be turned into spatial objects in R for further spatial analysis. This short example demonstrates a method of turning tabular data into spatial data, but this is not the only way to do so. First, load in the necessary packages. library(tidyverse) # for tidy data manipulation library(sf) # for spatial analysis ## Linking to GEOS 3.6.1, GDAL 2.2.3, PROJ 4.9.3 library(leaflet) # for interactive mapping library(inlmisc) # for interactive mapping on top of leaflet Create an example tabular dataset of stations that we want to make into a spatial dataset. Note: we are using the tibble::tribble() function to define a tibble row by row, which makes for easy visualization of test datasets. This is just one of many ways to create a tibble. exampleSites &lt;- tribble( ~StationID, ~Latitude, ~Longitude, &quot;Station_1&quot;, 37.812840, -80.063946, &quot;Station_2&quot;, 37.782322, -79.961449, &quot;Station_3&quot;, 37.801644, -79.968441) exampleSites ## # A tibble: 3 x 3 ## StationID Latitude Longitude ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Station_1 37.8 -80.1 ## 2 Station_2 37.8 -80.0 ## 3 Station_3 37.8 -80.0 Using the sf package, we can use the Latitude and Longitude fields to convert this object into a spatial object. Note we are using EPSG 4326 for our coordinate reference system. We chose that CRS exampleSites_sf &lt;- exampleSites %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) # Now look at the difference in the object. The geometry listcolumn is where all the spatial magic is stored. exampleSites_sf ## Simple feature collection with 3 features and 3 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -80.06395 ymin: 37.78232 xmax: -79.96145 ymax: 37.81284 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## # A tibble: 3 x 4 ## StationID Latitude Longitude geometry ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;POINT [°]&gt; ## 1 Station_1 37.8 -80.1 (-80.06395 37.81284) ## 2 Station_2 37.8 -80.0 (-79.96145 37.78232) ## 3 Station_3 37.8 -80.0 (-79.96844 37.80164) You can operate on a spatial dataset created using this method just like any other tidy object and the sticky geometry will come along for the ride. justOneSite &lt;- filter(exampleSites_sf, StationID == &#39;Station_1&#39;) Lets plot the result to see what happened. CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addCircleMarkers(data = exampleSites_sf, color=&#39;yellow&#39;, fillColor=&#39;black&#39;, radius = 5, fillOpacity = 1, opacity=1,weight = 1,stroke=T, group=&quot;All Sites&quot;, label = ~StationID, popup=leafpop::popupTable(exampleSites_sf)) %&gt;% addCircleMarkers(data = justOneSite, color=&#39;orange&#39;, fillColor=&#39;orange&#39;, radius = 5, fillOpacity = 1, opacity=1,weight = 1,stroke=T, group=&quot;Just One Site&quot;, label = ~StationID, popup=leafpop::popupTable(justOneSite)) %&gt;% addLayersControl(baseGroups=c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), overlayGroups = c(&#39;Just One Site&#39;, &#39;All Sites&#39;), options=layersControlOptions(collapsed=T), position=&#39;topleft&#39;) 3.1.1 Removing Spatial Information Sometimes we need to remove spatial information in order to operate on a dataset (e.g. save just the tabular data as a csv). We can use sf to remove just the spatial information. exampleSites_noSpatial &lt;- exampleSites_sf %&gt;% st_drop_geometry() exampleSites_noSpatial ## # A tibble: 3 x 3 ## StationID Latitude Longitude ## * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Station_1 37.8 -80.1 ## 2 Station_2 37.8 -80.0 ## 3 Station_3 37.8 -80.0 "],["usingSpatialDatasets.html", "3.2 Using Spatial Datasets", " 3.2 Using Spatial Datasets Coming Soon! "],["interactiveMapping.html", "3.3 Interactive Mapping", " 3.3 Interactive Mapping with leaflet. Coming soon! "],["watershed-delineation.html", "3.4 Watershed Delineation", " 3.4 Watershed Delineation Section Contact: Emma Jones (emma.jones@deq.virginia.gov) This module overviews the basics of delineating watersheds directly in your local R environment by scraping USGSs StreamStats API. General web scraping techniques are beyond the scope of this module, but the basics can be gleaned by unpacking the referenced functions. All watersheds delineated using this technique use USGSs StreamStats delineation techniques from a 1:24k NHD. You may manually explore the tool here. First, load in the necessary packages and functions to complete this task. library(tidyverse) # for tidy data manipulation library(sf) # for spatial analysis library(leaflet) # for interactive mapping library(inlmisc) # for interactive mapping on top of leaflet You will also need to use a custom function built for scraping the StreamStats API. This function is contained in the sourced script StreamStatsAutoDelineation.R which be downloaded for sourcing locally in your environment. source(&#39;StreamStatsAutoDelineation.R&#39;) # for custom web scraping tool to hit USGS StreamStats API Create an example dataset of stations that we want to delineate. Note: we are using the tibble::tribble() function to define a tibble row by row, which makes for easy visualization of test datasets. This is just one of many ways to create a tibble. exampleSites &lt;- tribble( ~StationID, ~Latitude, ~Longitude, &quot;Station_1&quot;, 37.812840, -80.063946, &quot;Station_2&quot;, 37.782322, -79.961449, &quot;Station_3&quot;, 37.801644, -79.968441) exampleSites ## # A tibble: 3 x 3 ## StationID Latitude Longitude ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Station_1 37.8 -80.1 ## 2 Station_2 37.8 -80.0 ## 3 Station_3 37.8 -80.0 The above dataset is simply tabular data. One could use a spatial dataset for this task by stripping out the coordinate information from the geometry listcolumn. See Spatial Data from Tabular Data for more information on this topic. 3.4.1 Single Station Delineation Next lets use USGSs automated delineation tools to delineate single station. We will first select only one site from our exampleSites object we created in the previous step and name it stationToDelineate. Then, we will feed our streamStats_Delineation() function (called into our environment when we sourced the StreamStatsAutoDelineation.R script) the necessary location information (state, longitude, and latitude arguments). The state argument tells StreamStats which state NHD we want to use for delineation, VA (Virginia) for our example. Lastly, the UID argument is the unique identifier we wish to associate with the spatial information we pull back from StreamStats. If we do not provide this information, we will not know which watersheds belong to which sites as we start to batch process these jobs. stationToDelineate &lt;- filter(exampleSites, StationID == &#39;Station_1&#39;) stationDelineation &lt;- streamStats_Delineation(state= &#39;VA&#39;, longitude = stationToDelineate$Longitude, latitude = stationToDelineate$Latitude, UID = stationToDelineate$StationID) The information returned from StreamStats is a list object containing point information (the location we supplied to delineate from) and polygon information (the resultant upstream shape returned from the pour point). We can easily unpack this information into easily used objects using the script below. stationDelineationWatershed &lt;- stationDelineation$polygon %&gt;% reduce(rbind) %&gt;% arrange(UID) stationDelineationPoint &lt;- stationDelineation$point %&gt;% reduce(rbind) %&gt;% arrange(UID) We can plot the results quickly to verify the desired watershed was returned. StreamStats returns a watershed for the input coordinates to the best of its ability; however, the accuracy of the coordinates, datum, projection, etc. can influence the accuracy of the returned watershed. It is best practice to always review the returned watershed. Below is a minimal example of how to do this in R with an interactive map. Note: you can switch basemaps to the USGS Hydrography layer for further information using the layers button in the top left corner. The following interactive map is created with the inlmisc package, see the package authors article for a detailed tutorial. The interactive mapping section covers some basics with leaflet. CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data= stationDelineationWatershed, color = &#39;black&#39;, weight = 1, fillColor=&#39;blue&#39;, fillOpacity = 0.4,stroke=0.1, group=&quot;Watershed&quot;, popup=leafpop::popupTable(stationDelineationWatershed, zcol=c(&#39;UID&#39;))) %&gt;% addCircleMarkers(data = stationDelineationPoint, color=&#39;orange&#39;, fillColor=&#39;black&#39;, radius = 5, fillOpacity = 1, opacity=1,weight = 1,stroke=T, group=&quot;Station&quot;, label = ~UID, popup=leafpop::popupTable(stationDelineationPoint)) %&gt;% addLayersControl(baseGroups=c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), overlayGroups = c(&#39;Station&#39;,&#39;Watershed&#39;), options=layersControlOptions(collapsed=T), position=&#39;topleft&#39;) 3.4.2 Multiple Station Delineation Using the dataset created above (exampleSites), we will now batch process the sites to StreamStats for an efficient data workflow. Remember, you are hitting the USGS API repeatedly, so there can be losses in connectivity resulting in missed watersheds. We will overview the QA process after we receive the watershed information back from USGS. multistationDelineation &lt;- streamStats_Delineation(state= &#39;VA&#39;, longitude = exampleSites$Longitude, latitude = exampleSites$Latitude, UID = exampleSites$StationID) The information returned from StreamStats is a list object containing point information (the location we supplied to delineate from) and polygon information (the resultant upstream shape returned from the pour point). We can easily unpack this information into easily used objects using the script below. watersheds &lt;- multistationDelineation$polygon %&gt;% reduce(rbind) %&gt;% arrange(UID) points &lt;- multistationDelineation$point %&gt;% reduce(rbind) %&gt;% arrange(UID) The next chunk overviews how to efficiently check to make sure all the desired sites were in fact delineated. If there are missing sites, the script will run back out to StreamStats to get anyone that is missing and smash that into the original dataset. # fix anything that is missing if(nrow(points) != nrow(watersheds) | nrow(exampleSites) != nrow(watersheds)){ missing &lt;- unique( c(as.character(points$UID[!(points$UID %in% watersheds$UID)]), as.character(exampleSites$StationID[!(exampleSites$StationID %in% watersheds$UID)]))) missingDat &lt;- filter(exampleSites, StationID %in% missing) #remove missing site from the paired dataset points &lt;- filter(points, ! UID %in% missing) watersheds &lt;- filter(watersheds, ! UID %in% missing) dat &lt;- streamStats_Delineation(state= &#39;VA&#39;, longitude = missingDat$Long, latitude = missingDat$Lat, UID = missingDat$StationID) watersheds_missing &lt;- dat$polygon %&gt;% reduce(rbind) points_missing &lt;- dat$point %&gt;% reduce(rbind) watersheds &lt;- rbind(watersheds, watersheds_missing) %&gt;% arrange(UID) points &lt;- rbind(points, points_missing) %&gt;% arrange(UID) rm(missingDat); rm(dat); rm(watersheds_missing); rm(points_missing) } Now lets map our results to ensure StreamStats delineated the correct watersheds. CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data= watersheds, color = &#39;black&#39;, weight = 1, fillColor=&#39;blue&#39;, fillOpacity = 0.4,stroke=0.1, group=&quot;Watershed&quot;, popup=leafpop::popupTable(watersheds, zcol=c(&#39;UID&#39;))) %&gt;% addCircleMarkers(data = points, color=&#39;orange&#39;, fillColor=&#39;black&#39;, radius = 5, fillOpacity = 1, opacity=1,weight = 1,stroke=T, group=&quot;Station&quot;, label = ~UID, popup=leafpop::popupTable(points)) %&gt;% addLayersControl(baseGroups=c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), overlayGroups = c(&#39;Station&#39;,&#39;Watershed&#39;), options=layersControlOptions(collapsed=T), position=&#39;topleft&#39;) 3.4.3 QA Should any of the returned watersheds prove incorrect based on visual analysis, you must remove that watershed from your dataset, manually delineate the watershed using StreamStats, and include that new watershed into your polygon dataset. See the Using Spatial Datasets section for example workflows. "],["landcover-analysis.html", "3.5 Landcover analysis", " 3.5 Landcover analysis Coming Soon "],["spatial-joins.html", "3.6 Spatial joins", " 3.6 Spatial joins Coming Soon "],["converting-tabular-data-to-spatial-data.html", "3.7 Converting tabular data to spatial data", " 3.7 Converting tabular data to spatial data Coming Soon "],["shinyAppHelp.html", "Chapter 4 Shiny App Pro Tips", " Chapter 4 Shiny App Pro Tips Shiny apps are interactive web based applications built in R to increase accessibility to analytical tools, workflows, and visualizations to non-R users. DEQ relies on these tools (hosted on the internal Connect platform) to extend data querying, manipulation, analysis, visualization, and reporting techniques to all staff, regardless of programming experience. Complicated workflows can be programmed in R using the shiny package to develop an easy to use front end interface, all in the R language. Shiny apps are easy to build. Many tutorials are available, but the best starting point is to follow the shiny tutorial by RStudio. More advanced shiny techniques employed regularly by DEQ staff to improve application responsiveness, user experience, or back end development are outlined below. Coming soon: Shiny file organization - Using functions and modules to improve code organization, how to integrate nested modules without namespace issues Shiny Tricks: Using HTML to help reset a complex UI How to integrate tool-tip HTML without calling new packages Escaping characters in user inputs Markdown and User-Friendly Output Files: readxlsx tips for formatting excel outputs Markdown files with \"child sub-markdown files Markdown tips in general for TinyTex and HTML "],["shiny-file-organization.html", "4.1 Shiny file organization", " 4.1 Shiny file organization Coming Soon! "],["shiny-tricks.html", "4.2 Shiny Tricks", " 4.2 Shiny Tricks Coming Soon! "],["markdown-and-user-friendly-output-files.html", "4.3 Markdown and User-Friendly Output Files", " 4.3 Markdown and User-Friendly Output Files Coming Soon! "],["miscTips.html", "Chapter 5 Miscellaeous Tips and Tricks ", " Chapter 5 Miscellaeous Tips and Tricks "],["regular-expression-and-pattern-matching.html", "5.1 Regular expression and pattern matching", " 5.1 Regular expression and pattern matching "],["non-standard-evaluation.html", "5.2 Non standard evaluation", " 5.2 Non standard evaluation "]]
