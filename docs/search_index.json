[["index.html", "DEQ R Methods Encyclopedia Chapter 1 Background", " DEQ R Methods Encyclopedia DEQ R Development Team 2022-05-25 Chapter 1 Background The purpose of this book is to describe common tasks undertaken by DEQ staff and clearly outline similar methodologies in R in order to promote learning opportunities for various skill levels of R users. This project is an example of a bookdown report built using R and RStudios R Markdown. Many authors have contributed to this effort: Emma Jones (emma.jones@deq.virginia.gov) Connor Brogan (connor.brogan@deq.virginia.gov) Rex Robichaux (rex.robichaux@deq.virginia.gov) Joe Famularo (joseph.famularo@deq.virginia.gov) Feel free to explore different chapters to learn about specific tasks and how to complete them efficiently and transparently in R. "],["whyR.html", "1.1 Why R?", " 1.1 Why R? "],["gettingStartedWithR.html", "1.2 Getting Started With R", " 1.2 Getting Started With R Section Contact: Joe Famularo (joseph.famularo@deq.virginia.gov) Welcome! Were glad youre interested in using R in your role at DEQ. Below there are a series of instructions that will help guide you through the process for downloading and configuring R and RStudio. Feel free to reach out to Joe with any questions (joseph.famularo@deq.virginia.gov). 1.2.1 Downloading R and RStudio Download Base R. This is the software that contains the R programming language. Once you have Base R installed, download RStudio. Executing the installation for RStudio will require elevation (see step 3). RStudio is the standard integrated development environment (IDE) that is used for writing R code, but you can use it to write in different languages (e.g., python, SQL). RStudio also allows you to connect to databases (e.g., ODS), visualize your data and working environment, and publish R products (e.g., excel files, plots, markdown documents, shiny applications, etc.). After downloading the RStudio executable, submit a VCCC ticket. This will get the ball rolling on resolving the elevation requirement for the installation. 1.2.2 Configuring RStudio After installing R and RStudio, its a best practice to configure your settings. Begin by verifying the version of RStudio that you are using. Open RStudio. Navigate to Tools &gt; Global Options &gt; General &gt; Change Select [64-bit] option located in Documents sub-directory. Click OK until you are out of the Global Options menu. Restart R. Set the CRAN repository. CRAN is the repository that stores R packages and documentation. There are CRAN mirrors around the world. We will be selecting the one located at Oak Ridge National Lab. Open RStudio. Navigate to Tools &gt; Global Options &gt; Packages &gt; Change Select USA (TN) [https] - National Institute for Computational Sciences, Oak Ridge, TN. Select Apply. Click OK. Set your IDE appearance. This gives you the option to personalize RStudio, which can be helpful if youre spending hours writing or reviewing code: Navigate to Tools &gt; Global Options &gt; Appearance &gt; Editor theme. 1.2.3 Downloading Packages Like most operations in R, there are multiple approaches to downloading packages. Call the function install.packages()in your console. Packages can be installed using the Packages tab. Click on the Packages tab. Select Install. Type package name(s). Click Install. We recommend downloading the following packages to get started. Tidyverse tmap sp sf dataRetrevial "],["wantToContribute.html", "1.3 Want to Contribute?", " 1.3 Want to Contribute? We are always looking for collaborators. All reprex (reproducible examples) must be published in R markdown using the bookdown package and bookdown output style. Contributions are not limited to R as R markdown natively utilizes many languages including python and SQL. To get started with bookdown, see Yihui Xies technical references book bookdown: Authoring Books and Technical Documents with R Markdown or watch his webinar introducing bookdown. "],["queryInternalDataSources.html", "Chapter 2 Query Internal Data Sources", " Chapter 2 Query Internal Data Sources Accessing internal DEQ data sources has been a longtime goal of many R programmers. Connecting a local R instance to raw data allows for increased automation and reduces redundant data copies on local drives. However, querying data directly from internal resources into ones local environment is a privilege and requires a number of R skills and approval from OIS. Database operations rely on the SQL language. Users must first familiarize themselves with the basics of database operations with SQL. The DataCamp Introduction to SQL course is a good starting point. Additionally, the DataCarpentry SQL databases and R course provides is a good practice before seeking a direct, read-only connection to ODS (the SQLServer back end of CEDS). After you have demonstrated proficiency manipulating smaller, local data sources in R, you may submit a Special Network Access Form to OIS seeking access to ODS data view using your Microsoft credentials. After OIS approval, you may access ODS from R following the connection instructions in the Connect to ODS module. Alternatively, data pinned on the R server are available to all staff (on the internal network or VPN) without OIS authorization. Many pre-analyzed data sources are available to be pulled into your local R environment to assist with projects. See the Connecting to R Connect (for Pinned Data) module for more information on connecting your local environment to the R server to query these internal data resources. "],["connectToConnectPins.html", "2.1 Connecting to R Connect (for Pinned Data)", " 2.1 Connecting to R Connect (for Pinned Data) Section Contact: Emma Jones (emma.jones@deq.virginia.gov) Pinned data are powerful internal data sources that are developed by R programmers that expedite analyses by offering pre-analyzed data for others to use. The data products are often the end result of multiple analytical steps that one would need to repeat each time certain datasets are needed for analyses. Because these common datasets could prove useful for many end users, these datasets are pinned to the R server to expedite (and standardize) the acquisition of common data needs. Examples of pinned data are VSCI/VCPMI scores, station level geospatial data, station level Water Quality Standards (WQS) information, and many more. To access this data, you must first link your local R environment to the R Connect server. YOU MUST BE ON THE DEQ NETWORK OR VPN IN ORDER TO ACCESS ANY INTERNAL DATA RESOURCES Additionally, you must access pinned data using the pins library version 0.4.3. More recent versions of the pins package will not successfully connect to the version of pins on the R server. #install.packages(&quot;https://cran.r-project.org/src/contrib/Archive/pins/pins_0.4.3.tar.gz&quot;, repos=NULL, type=&quot;source&quot;) #install.packages(&quot;https://cran.r-project.org/src/contrib/filelock_1.0.2.tar.gz&quot;, repos=NULL, type=&quot;source&quot;) library(tidyverse) library(pins) library(config) Each time you wish to access pinned data, you must connect to the R server like you might connect to a database. To gain access to the R server, you need to have the API key information. Obtain this information by emailing Emma Jones (emma.jones@deq.virginia.gov) and specify you want access to pinned data on the R server. 2.1.1 API Keys To connect to the R server, you must use the appropriate API key. NEVER HARD CODE ACCESS CODES INTO YOUR SCRIPTS This means you should NEVER place the actual API key into your code ANYWHERE. You must source this information from a secret DEQconfig.yml file (obtained from Emma Jones and stored locally on your computer) in order to access data from the R server in your local R environment. Read more about config files. 2.1.2 Connect to the R server Use the following script to source the server API key and connect to the R server. # Server connection things conn &lt;- config::get(file = &quot;PINSconfig.yml&quot;, &quot;connectionSettings&quot;) # get configuration settings board_register_rsconnect(key = conn$CONNECT_API_KEY, server = conn$CONNECT_SERVER) 2.1.3 Browse Available Pins Once you are connected to the R server, you can view available pins and metadata. as_tibble(pin_find(board = &#39;rsconnect&#39;)) 2.1.4 Access a Pin To bring a particular pin into your local R environment, simply create an object and call the pin from the R server. totalHabitat &lt;- pin_get(&#39;ejones/totalHabitatScore&#39;, board = &#39;rsconnect&#39;) head(totalHabitat) ## # A tibble: 6 x 8 ## StationID `Collection Date` HabSampID `Field Team` `HabSample Comm~ Gradient ## &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2-LIJ003~ 2021-10-18 10:00:00 2-LIJ165~ bvw, rtt, j~ &lt;NA&gt; High ## 2 2-XUL000~ 2013-04-09 09:30:00 2-XUL GJD &lt;NA&gt; High ## 3 6BTHC000~ 2020-05-12 12:30:00 6BTHC159~ LLS &lt;NA&gt; High ## 4 6CLAE001~ 2020-06-04 12:30:00 6CLAE159~ LLS &lt;NA&gt; High ## 5 1AABR000~ 2012-10-23 14:30:00 ABR1577 rtt &lt;NA&gt; High ## 6 6AABR000~ 2021-04-07 14:45:00 ABR16496 LLS &lt;NA&gt; High ## # ... with 2 more variables: Season &lt;chr&gt;, `Total Habitat Score` &lt;dbl&gt; 2.1.5 Query (some of) a Pin Sometimes you do not want to bring an entire dataset into your environment. You can query just the information you want to bring back from a pinned data source by using simple dplyr verbs. pin_get(&#39;ejones/VSCIresults&#39;, board = &#39;rsconnect&#39;) %&gt;% # Query one station between a set date range filter(StationID == &#39;2-JKS023.61&#39; &amp; between(as.Date(`Collection Date`), as.Date(&#39;2015-01-01&#39;), as.Date(&#39;2020-12-31&#39;))) %&gt;% # only bring back rarified Samples filter(`Target Count` == 110) %&gt;% dplyr::select(StationID, `Collection Date`, everything()) %&gt;% datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) "],["connectToODS.html", "2.2 Connect to ODS", " 2.2 Connect to ODS Section Contact: Emma Jones (emma.jones@deq.virginia.gov) The ODS environment allows read-only access from your local R instance to the overnight copy of data stored in CEDS. The Oracle based CEDS environment is transferred to a SQLServer database each evening, ensuring data entered into CEDS is available for querying from ODS the next morning. 2.2.1 Credentials (Local vs Remote) Your Microsoft (MS) credentials are used for verifying you have access rights to this database. Details on acquiring access to this data source are available in the Query Internal Data Sources chapter. If you are building a report or application that relies on querying data from ODS at prescribed intervals, your personal MS credentials will not work when your data product is deployed to the R server. Instead, you must perform all data product testing locally with your personal MS credentials. Upon pushing your data product to the R server, you must switch the ODS credentials to the R servers unique credentials for the program to work on the remote server. To obtain the R servers ODS credentials, please contact Emma Jones (emma.jones@deq.virginia.gov) with specifics on exactly which areas of ODS your data product require access, how frequently your product hits ODS, example data retrieved from the environment, and general application purpose, use case, and audience information. 2.2.2 Required Packages To access ODS, you will need to use specific R packages that enable database access in addition to the recommended tidyverse package for general data management. The chunk below specifies which packages are required for the two methods for connecting to ODS overviewed in this report. The pool method requires the R packages pool and dbplyr while the DBI method requires the odbc and DBI R packages. Like all tasks completed in R, it is good be be aware that there are multiple methods to perform various operations, but in time users tend to prefer certain methods over others for their regular business practices. library(tidyverse) # &quot;pool method&quot; required packages library(pool) library(dbplyr) # &quot;DBI method&quot; required packages library(odbc) library(DBI) 2.2.2.1 ODSprod vs ODStest There are two ODS environments for you to be aware of: ODS (production or prod, DEQ-SQLODS-PROD) and ODStest (DEQ-SQLODS-TEST). The ODS environment is the production environment that offers data views as they are stored in CEDS. The ODStest environment is a testing environment (sandbox) that is used for testing data architecture and database functions and may not always contain data meant for querying for business applications. Which should you connect to? If you are querying data for business applications, reports, etc. you must ensure you are connected to the ODS production environment. If you are learning how to use databases, building test queries, or generally exploring, use the ODStest environment. It is the responsibility of each data analyst to know which data source they are connected to while querying data and the implications of data sources on resultant data. 2.2.3 DBI Method for ODS Connection The most basic way of connecting to a database uses the odbc and DBI R packages. library(odbc) library(DBI) We connect to a database by establishing a connection object with a very specific connection string. There are a few things to note in this connection string: 1. The driver method used is the ODBC driver from the odbc R package 2. The driver version is unique to your computer. You must input the exact ODBC driver software version you have installed on your machine. Examples of drivers that work with ODS are ODBC Driver 12 for SQL Server, ODBC Driver 11 for SQL Server, and SQL Server Native Client 11.0 3. The server argument specifies the server name (DEQ-SQLODS-PROD) and port (50000) that you are using for the connection 4. The database argument specifies which database you wish to connect to (ODS) 5. The trusted_connection argument must always be yes con &lt;- dbConnect(odbc::odbc(), .connection_string = &quot;driver={ODBC Driver 11 for SQL Server};server={DEQ-SQLODS-PROD,50000};database={ODS};trusted_connection=yes&quot;) The above script establishes a database connection named con in your environment. This connection will be passed into other query strings to query the database, requiring an open database connection each time you query the database. To test the connection, we will query an entire data view that is small from the WQM area of ODS. We use SQL to tell ODS to select everything from the Edas_Benthic_Master_Taxa_View data view. Note that we must provide the querying function the con connection string we established above. masterTaxaGenus &lt;- dbGetQuery(con, &quot;SELECT * FROM wqm.Edas_Benthic_Master_Taxa_View&quot;) head(masterTaxaGenus) ## # A tibble: 6 x 19 ## Phylum Class Subclass Order Suborder Superfamily Family Subfamily Tribe Genus ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Arthr~ Inse~ &lt;NA&gt; Dipt~ &lt;NA&gt; &lt;NA&gt; Culic~ &lt;NA&gt; &lt;NA&gt; Aedes ## 2 Arthr~ Inse~ &lt;NA&gt; Odon~ Anisopt~ &lt;NA&gt; Aeshn~ &lt;NA&gt; &lt;NA&gt; Aesh~ ## 3 Arthr~ Inse~ &lt;NA&gt; Cole~ &lt;NA&gt; &lt;NA&gt; Dytis~ Agabinae Agab~ Agab~ ## 4 Arthr~ Inse~ &lt;NA&gt; Tric~ &lt;NA&gt; &lt;NA&gt; Gloss~ Agapetin~ &lt;NA&gt; Agap~ ## 5 Arthr~ Inse~ &lt;NA&gt; Tric~ &lt;NA&gt; &lt;NA&gt; Seric~ &lt;NA&gt; &lt;NA&gt; Agar~ ## 6 Arthr~ Inse~ &lt;NA&gt; Plec~ &lt;NA&gt; &lt;NA&gt; Perli~ Perlinae Perl~ Agne~ ## # ... with 9 more variables: Species &lt;chr&gt;, `Final VA Family ID` &lt;chr&gt;, ## # FinalID &lt;chr&gt;, TolVal &lt;dbl&gt;, FFG &lt;chr&gt;, Habit &lt;chr&gt;, FamFFG &lt;chr&gt;, ## # FamTolVal &lt;dbl&gt;, FamHabit &lt;chr&gt; We can use this DBI method to send SQL statements directly to the database. SQL will not be covered in this section, please see DataCamp Introduction to SQL course for a basic primer on the SQL language. dbGetQuery(con, &quot;SELECT TOP 10 * FROM wqm.WQM_Stations_View&quot;) ## # A tibble: 10 x 103 ## Sta_Id Sta_Desc Sta_Cbp_Name Sta_Fic_County Sta_Fic_State Sta_Rec_Code ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2-WPK~ Winterp~ &lt;NA&gt; 041 51 Piedmont ## 2 2-WPK~ Winterp~ &lt;NA&gt; 041 51 Piedmont ## 3 2-XVE~ UT to W~ &lt;NA&gt; 041 51 Piedmont ## 4 2-WPK~ Winterp~ &lt;NA&gt; 041 51 Piedmont ## 5 2-WPK~ Winterp~ &lt;NA&gt; 041 51 Piedmont ## 6 2-XVF~ UT to W~ &lt;NA&gt; 041 51 Piedmont ## 7 2-WPK~ UT to W~ &lt;NA&gt; 041 51 Piedmont ## 8 2-XVG~ UT to W~ &lt;NA&gt; 041 51 Piedmont ## 9 2-WPK~ Winterp~ &lt;NA&gt; 041 51 Piedmont ## 10 2-XVJ~ UT to W~ &lt;NA&gt; 041 51 Piedmont ## # ... with 97 more variables: Admin_Region &lt;chr&gt;, Sta_Lat_Deg &lt;dbl&gt;, ## # Sta_Lat_Min &lt;dbl&gt;, Sta_Lat_Sec &lt;dbl&gt;, Sta_Long_Deg &lt;dbl&gt;, ## # Sta_Long_Min &lt;dbl&gt;, Sta_Long_Sec &lt;dbl&gt;, Sta_Lv1_Code &lt;chr&gt;, ## # Lv1_Description &lt;chr&gt;, Sta_Lv2_Code &lt;chr&gt;, Lv2_Description &lt;chr&gt;, ## # Sta_Lv3_Code &lt;chr&gt;, Lv3_Description &lt;chr&gt;, Sta_Lv4_Code &lt;chr&gt;, ## # Lv4_Description &lt;chr&gt;, Sta_Lv5_Code &lt;chr&gt;, Lv5_Description &lt;chr&gt;, ## # Sta_Stream_Section &lt;chr&gt;, Sta_Stream_Name &lt;chr&gt;, Sta_Huc_Code &lt;chr&gt;, ## # Sta_Huc_Sbc_Bsc_Code &lt;chr&gt;, Sta_Huc_Sbc_Code &lt;chr&gt;, Sta_Wsh_Code &lt;chr&gt;, ## # Sta_Toc_Map_Num &lt;chr&gt;, Sta_Sampling_Frequency &lt;chr&gt;, Sfc_Desc &lt;chr&gt;, ## # Sta_First_Sample_Date &lt;dttm&gt;, Sta_Last_Sample_Date &lt;dttm&gt;, ## # Sta_Size_Sampling_Area &lt;chr&gt;, Sta_Size_Sampling_Area_Unit &lt;chr&gt;, ## # Sau_Desc &lt;chr&gt;, Sta_Straher_Order &lt;chr&gt;, Sta_Shreve_Order &lt;chr&gt;, ## # Sta_Wadable &lt;chr&gt;, Sta_Tod_Code &lt;chr&gt;, Sta_Tbc_Code &lt;chr&gt;, ## # Sta_305B_Code &lt;chr&gt;, Sta_Water_Quality_Standards &lt;chr&gt;, Wqs_Desc &lt;chr&gt;, ## # Sta_Salinity_Area_Type &lt;chr&gt;, Sat_Desc &lt;chr&gt;, ## # Sta_Habitat_Descriptors &lt;chr&gt;, Sta_Comment &lt;chr&gt;, Sta_Storet_Bound &lt;chr&gt;, ## # Sta_Stored_Storet_Date &lt;dttm&gt;, Sta_Inserted_Date &lt;dttm&gt;, ## # Sta_Inserted_By &lt;chr&gt;, Sta_Changed_Date &lt;dttm&gt;, Sta_Changed_By &lt;chr&gt;, ## # Rep_Timestamp &lt;dttm&gt;, Sta_Str_Stream_Code &lt;chr&gt;, ## # Sta_Wqm_Wat_Shed_Code &lt;chr&gt;, Sta_Num_Of_Visits &lt;dbl&gt;, Sta_Type &lt;chr&gt;, ## # Sta_Old_Id &lt;chr&gt;, Sta_Fi_Permit_Id &lt;chr&gt;, Sta_Fi_Outfall_No &lt;chr&gt;, ## # Sta_Fi_Type &lt;chr&gt;, Sta_River_Mile &lt;chr&gt;, Sta_Milepoint_On &lt;chr&gt;, ## # Sta_His_Data_Revw_Flag &lt;chr&gt;, Sta_His_Data_Revw_Note &lt;chr&gt;, ## # Sta_Spa_Dist_Flag &lt;chr&gt;, Sta_Spa_Dist_Note &lt;chr&gt;, Sta_Spec_Prob_Flag &lt;chr&gt;, ## # Sta_Spec_Prob_Note &lt;chr&gt;, Sta_Maj_Tributary_Flag &lt;chr&gt;, ## # Sta_Maj_Tributary_Note &lt;chr&gt;, Sta_Maj_Fish_Flag &lt;chr&gt;, ## # Sta_Maj_Fish_Note &lt;chr&gt;, Sta_Ext_Recomm_Flag &lt;chr&gt;, ## # Sta_Ext_Recomm_Note &lt;chr&gt;, Sta_Source_Scale &lt;dbl&gt;, Sta_Hmv &lt;chr&gt;, ## # Sta_Hmuc &lt;chr&gt;, Sta_Hcm &lt;chr&gt;, Hcm_Short_Desc &lt;chr&gt;, Hcm_Full_Desc &lt;chr&gt;, ## # Sta_Hcrs &lt;chr&gt;, Hcrs_Short_Desc &lt;chr&gt;, Hcrs_Full_Desc &lt;chr&gt;, Sta_Vmv &lt;chr&gt;, ## # Sta_Vmuc &lt;chr&gt;, Sta_Vcm &lt;chr&gt;, Vcm_Desc &lt;chr&gt;, Sta_Vcrs &lt;chr&gt;, ## # Vcrs_Short_Desc &lt;chr&gt;, Vcrs_Full_Desc &lt;chr&gt;, Sta_Country &lt;chr&gt;, ## # Country_Desc &lt;chr&gt;, Sta_Mlt &lt;chr&gt;, Mlt_Description &lt;chr&gt;, ## # Sd_First_Sample_Date &lt;dttm&gt;, Sd_Last_Sample_Date &lt;dttm&gt;, ## # Sd_Num_Visits &lt;dbl&gt;, Loaded_Date &lt;dttm&gt;, Loaded_By &lt;chr&gt; 2.2.4 Pool Method for ODS Connection Generally speaking, the pool method is preferred for most business practices, especially any dynamic content (e.g. shiny apps). The pool package creates a connection to a specified database, even if the database connection is temporarily lost or idle for a period of time. The instability of connecting to ODS of VPN makes the pool package especially useful for DEQ business practices. The concepts of open database connections are important when interacting with databases, read more on this topic here. The pool method requires the pool and dbplyr R packages to be loaded into the user environment. library(pool) library(dbplyr) To connect to a database, you must establish a pool object. This object stores your connection information and may easily be piped into query requests. You may name your pool object anything you want, but most programmers name it pool by convention to ease the transfer of scripts from one project/programmer to the next. There are a few things to note in this connection string: 1. The driver method used is the ODBC driver from the odbc R package 2. The specific Driver version is unique to your computer. You must input the exact ODBC driver software version you have installed on your machine. Examples of drivers that work with ODS are ODBC Driver 12 for SQL Server, ODBC Driver 11 for SQL Server, and SQL Server Native Client 11.0 3. The Server argument specifies the server name and port (50000) you are using for the connection 4. The dbname argument specifies which database you wish to connect to (ODS) 5. The trusted_connection argument must always be yes pool &lt;- dbPool( drv = odbc::odbc(), # (1) Driver = &quot;ODBC Driver 11 for SQL Server&quot;, # (2) Server= &quot;DEQ-SQLODS-PROD,50000&quot;, # (3) dbname = &quot;ODS&quot;, # (4) trusted_connection = &quot;yes&quot; # (5) ) After running the above chunk, you should have a list object named pool in your environment. We will use this object to build a minimal query to test our local connection to the ODS production environment. 2.2.5 Pool Method: Query Data Once a pool object is created, you may test it by querying the database. The below example is a good test of querying an entire data view in ODS and bringing that information back into your local environment because the view is relatively small. If you successfully bring back the benthic macroinvertebrate master taxa list, then you have successfully connected to ODS. masterTaxaGenus &lt;- pool %&gt;% tbl(in_schema(&quot;wqm&quot;, &quot;Edas_Benthic_Master_Taxa_View&quot;)) %&gt;% as_tibble() head(masterTaxaGenus) ## # A tibble: 6 x 19 ## Phylum Class Subclass Order Suborder Superfamily Family Subfamily Tribe Genus ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Arthr~ Inse~ &lt;NA&gt; Dipt~ &lt;NA&gt; &lt;NA&gt; Culic~ &lt;NA&gt; &lt;NA&gt; Aedes ## 2 Arthr~ Inse~ &lt;NA&gt; Odon~ Anisopt~ &lt;NA&gt; Aeshn~ &lt;NA&gt; &lt;NA&gt; Aesh~ ## 3 Arthr~ Inse~ &lt;NA&gt; Cole~ &lt;NA&gt; &lt;NA&gt; Dytis~ Agabinae Agab~ Agab~ ## 4 Arthr~ Inse~ &lt;NA&gt; Tric~ &lt;NA&gt; &lt;NA&gt; Gloss~ Agapetin~ &lt;NA&gt; Agap~ ## 5 Arthr~ Inse~ &lt;NA&gt; Tric~ &lt;NA&gt; &lt;NA&gt; Seric~ &lt;NA&gt; &lt;NA&gt; Agar~ ## 6 Arthr~ Inse~ &lt;NA&gt; Plec~ &lt;NA&gt; &lt;NA&gt; Perli~ Perlinae Perl~ Agne~ ## # ... with 9 more variables: Species &lt;chr&gt;, `Final VA Family ID` &lt;chr&gt;, ## # FinalID &lt;chr&gt;, TolVal &lt;dbl&gt;, FFG &lt;chr&gt;, Habit &lt;chr&gt;, FamFFG &lt;chr&gt;, ## # FamTolVal &lt;dbl&gt;, FamHabit &lt;chr&gt; A few notes on querying using the pool method. The data schema are very important for ensuring the query runs successfully. In the above example, we are querying the data view entitled Edas_Benthic_Master_Taxa_View that lies inside the wqm area of ODS. Depending on your data needs (and information specified in your Special Network Access Form), you will be granted access to specific areas on the ODS environment. Water Quality Monitoring information is stored in the wqm area, Water Quality Assessment data is stored in the wqa area, TMDL data is stored in the TMDL area, etc. If you do not have access to the above area in ODS, then the above script will not work for you. The pool method allows for more complex queries to be piped into a single call. This is very powerful because it allows you to use simple dplyr verbs (through the dbplyr package) to force the database to perform these operations instead of bringing a lot of data into your environment for local filtering and manipulation operations. More on this concept will be covered in Querying Data from ODS but the importance of this cannot be understated. It is always more efficient to have the database perform querying operations to minimize the amount of data sent to your local R environment. "],["queryDataFromODS.html", "2.3 Querying Data from ODS", " 2.3 Querying Data from ODS Section Contact: Emma Jones (emma.jones@deq.virginia.gov) There are a few general concepts to be aware of when querying data from ODS. These include connecting and disconnecting from a database, understanding the underlying data schema, how data Views interact with one another, and data update frequencies. This module deals exclusively with data from the ODS production environment. Please see the Connect to ODS section for information about the test vs production environments. In an ideal world, one would practice querying data against a test environment and only query the production environment for official data retrievals. This is especially true when building automated reports or applications that could repeatedly tax the database with frequent large queries during the development process. However, due to frequency of data updates to the ODStest environment and potential schema differences, we do not encourage users to rely on data in this environment. 2.3.1 Connecting and Disconnecting from a Database It is imperative to understand that each time a user connects to a database it is that users responsibility to disconnect from the database once they have finished their database operations. Forgotten open database connections (or leaked connections) unnecessarily tax a database and are poor practice as programmers. Below are example connection and disconnection statements based on how a user may connect to a database. # Connect using the DBI library con &lt;- dbConnect(odbc::odbc(), .connection_string = &quot;driver={ODBC Driver 11 for SQL Server};server={DEQ-SQLODS-PROD,50000};database={ODS};trusted_connection=yes&quot;) # Disconnect using the DBI library dbDisconnect(con) The Connect to ODS article emphasizes the benefits of using the pool package to manage database connections. Besides being useful for piping and dplyr/dbplyr functionality, pool handles the closing of unused connections without requiring user input. This is very important. For more information on pooled connections, see the R pool package and read more generally about pooled database connections. 2.3.1.1 Standalone Reports/Applications that Query ODS When building any shiny application or report that queries data from ODS, it is best to use the pool package instead of opening and closing individual database connections with each query. It is also important to include a the following script to immediately close any pooled connections immediately when a user closes an application/report. Place the connection/disconnection scripts inside the global.R file of a multifile shiny app. library(pool) # open pool connection, best stored in global.R file pool &lt;- dbPool( drv = odbc::odbc(), Driver = &quot;ODBC Driver 11 for SQL Server&quot;,#&quot;SQL Server Native Client 11.0&quot;, Server= &quot;DEQ-SQLODS-PROD,50000&quot;, dbname = &quot;ODS&quot;, trusted_connection = &quot;yes&quot; ) # close pool connection, best stored in global.R file onStop(function() { poolClose(pool) }) 2.3.2 Data Schema ODS is divided into multiple program areas. These areas are turned on for certain users depending on their data needs. To request access to certain areas of ODS, you need to submit a Special Network Access Form to OIS seeking access to specific areas of ODS. Once access to one or more areas of ODS are granted, you may explore the available areas using code ( e.g. using the DBI package ) or interactively with a GUI ( e.g. using the built in GUI functionality in the RStudio IDE ). 2.3.2.1 Data Exporation: DBI After connecting to ODS using the config information (see Connect to ODS), the DBI::dbListTables() allows users to see what data views are available in certain areas of ODS. Depending on your data needs, you may have an extensive list of available tables. For presentation purposes, the dbListTables() results were piped into a head() call with an n argument of 20 to only show the top 20 results. library(DBI) con &lt;- dbConnect(odbc::odbc(), .connection_string = &quot;driver={ODBC Driver 11 for SQL Server};server={DEQ-SQLODS-PROD,50000};database={ODS};trusted_connection=yes&quot;) dbListTables(con) %&gt;% head(20) ## [1] &quot;WP_WATER_PERMITS_VIEW&quot; &quot;trace_xe_action_map&quot; ## [3] &quot;trace_xe_event_map&quot; &quot;Enf_Admin_Proceed_View&quot; ## [5] &quot;Enf_ECM_Docs_View&quot; &quot;Enf_enforcement_Cases_View&quot; ## [7] &quot;Enf_Facilities_View&quot; &quot;Enf_Respon_Parties_View&quot; ## [9] &quot;Enf_Settle_Details_View&quot; &quot;CHECK_CONSTRAINTS&quot; ## [11] &quot;COLUMN_DOMAIN_USAGE&quot; &quot;COLUMN_PRIVILEGES&quot; ## [13] &quot;COLUMNS&quot; &quot;CONSTRAINT_COLUMN_USAGE&quot; ## [15] &quot;CONSTRAINT_TABLE_USAGE&quot; &quot;DOMAIN_CONSTRAINTS&quot; ## [17] &quot;DOMAINS&quot; &quot;KEY_COLUMN_USAGE&quot; ## [19] &quot;PARAMETERS&quot; &quot;REFERENTIAL_CONSTRAINTS&quot; This method is good for identifying all data views available to a user with their current permissions, but it is not efficient for understanding the overall database structure, e.g. where the views referenced above exist in the ODS schema as a whole. 2.3.2.2 Data Exporation: RStudio Connections Pane The built in Connections pane is a better way to understand how the pieces of the database fit together and quickly view the top 1000 rows of a selected data view. To access this feature of RStudio, click the Connections tab in the Environment pane in your IDE. If you have already connected to the ODS environment using the DBI package, you will automatically see this as an available option. If not, see Connect to ODS for more information on making this connection for the first time. Click on the ODS database connection to show the connection information. Use the Connect drop down to choose how you want to connect to the database. For this example we will choose R Console but other options are helpful for different use cases. The available environments are displayed once the connection is made. This happens almost instantaneously. Click on the drop down arrow next to ODS to show the database structure and what areas you have access to. Your connection may not look identical to the example. Click on any of the areas to preview what data views are available in each area. You can click the drop down arrow next to each data view for details on the variables within the data view as well as each data format. Or you can click the name of the data view to open a preview of the data view (up to first 1,000 rows) in a tabular form in the Viewer Pane. Below we are looking at the first 1,000 rows of the Edas_Benthic_Master_Taxa_View. It is important to tell R in your data query not only which data view you are querying but also the area of ODS the data view lives in. Using the pool and dbplyr packages this is very straightforward. Note the in_schema() nested function inside the tbl() function in the example below. library(tidyverse) library(pool) library(dbplyr) ## Connect to ODS production pool &lt;- dbPool( drv = odbc::odbc(), Driver = &quot;ODBC Driver 11 for SQL Server&quot;,#&quot;SQL Server Native Client 11.0&quot;, Server= &quot;DEQ-SQLODS-PROD,50000&quot;, dbname = &quot;ODS&quot;, trusted_connection = &quot;yes&quot; ) # Query the entire Edas_Benthic_Master_Taxa_View data view from the wqm area of ODS masterTaxaGenus &lt;- pool %&gt;% tbl(in_schema(&quot;wqm&quot;, &quot;Edas_Benthic_Master_Taxa_View&quot;)) %&gt;% as_tibble() # preview the top 6 rows of the data pulled back from ODS head(masterTaxaGenus) ## # A tibble: 6 x 19 ## Phylum Class Subclass Order Suborder Superfamily Family Subfamily Tribe Genus ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Arthr~ Inse~ &lt;NA&gt; Dipt~ &lt;NA&gt; &lt;NA&gt; Culic~ &lt;NA&gt; &lt;NA&gt; Aedes ## 2 Arthr~ Inse~ &lt;NA&gt; Odon~ Anisopt~ &lt;NA&gt; Aeshn~ &lt;NA&gt; &lt;NA&gt; Aesh~ ## 3 Arthr~ Inse~ &lt;NA&gt; Cole~ &lt;NA&gt; &lt;NA&gt; Dytis~ Agabinae Agab~ Agab~ ## 4 Arthr~ Inse~ &lt;NA&gt; Tric~ &lt;NA&gt; &lt;NA&gt; Gloss~ Agapetin~ &lt;NA&gt; Agap~ ## 5 Arthr~ Inse~ &lt;NA&gt; Tric~ &lt;NA&gt; &lt;NA&gt; Seric~ &lt;NA&gt; &lt;NA&gt; Agar~ ## 6 Arthr~ Inse~ &lt;NA&gt; Plec~ &lt;NA&gt; &lt;NA&gt; Perli~ Perlinae Perl~ Agne~ ## # ... with 9 more variables: Species &lt;chr&gt;, `Final VA Family ID` &lt;chr&gt;, ## # FinalID &lt;chr&gt;, TolVal &lt;dbl&gt;, FFG &lt;chr&gt;, Habit &lt;chr&gt;, FamFFG &lt;chr&gt;, ## # FamTolVal &lt;dbl&gt;, FamHabit &lt;chr&gt; As emphasized before, it is prudent to close all database connections established using DBI (the connection method the RStudio Connections pane utilizes). To do this using the GUI, click the Disconnect From A Connection button highlighted in red in the picture below. 2.3.3 Data Views Interaction Another component of any database schema involves how data can be efficiently combined from the various areas of the database. Generally speaking, the way we use data locally in spreadsheets is not how a database stores said data. It is the responsibility of the data query-er to link the datasets they need to make it usable. A full tour of each area of ODS is beyond the scope of these articles, but short overviews of specific ODS areas are discussed in subsequent articles. These areas include: Common WQM queries Common Benthic queries Common WQA queries Common TMDL queries 2.3.4 Data Update Frequency ODS production is update nightly reflecting any data entered into CEDS the evening before. If you need data immediately after it is entered into CEDS, then CEDS is the only location users can view the data. ODStest is updated infrequently and should not be used for official data queries. "],["commonWQMqueries.html", "2.4 Common WQM queries", " 2.4 Common WQM queries Coming Soon! "],["commonBenthicqueries.html", "2.5 Common Benthic queries", " 2.5 Common Benthic queries Coming Soon! "],["commonWQAqueries.html", "2.6 Common WQA queries", " 2.6 Common WQA queries Coming Soon! "],["commonTMDLqueries.html", "2.7 Common TMDL queries", " 2.7 Common TMDL queries Coming Soon! "],["spatialAnalysis.html", "Chapter 3 Spatial Analysis", " Chapter 3 Spatial Analysis R is a powerful tool for spatial analysis. Among the many benefits to using R as a GIS are the reproducibility of codified methods, which make sharing procedures and updating results after underlying data updates easy and efficient. There are many resources for learning how to use R for geospatial operations. A favorite is the Geocomputation with R online book (free) by Robin Lovelace. To familiarize yourself with common geospatial procedures in R, the following chapter has been developed by DEQ staff to help colleagues with typical geospatial tasks. This is by no means a comprehensive introduction to geospatial techniques in R. Please see other resources for more background information on underlying geospatial principles and methods in R. "],["spatialDataFromTabularData.html", "3.1 Spatial Data from Tabular Data", " 3.1 Spatial Data from Tabular Data Section Contact: Emma Jones (emma.jones@deq.virginia.gov) Location information can be critical to efficiently analyzing a dataset. Often, spatial data is included into spreadsheets as Latitude and Longitude fields, which can quickly be turned into spatial objects in R for further spatial analysis. This short example demonstrates a method of turning tabular data into spatial data, but this is not the only way to do so. First, load in the necessary packages. library(tidyverse) # for tidy data manipulation library(sf) # for spatial analysis ## Linking to GEOS 3.6.1, GDAL 2.2.3, PROJ 4.9.3 library(leaflet) # for interactive mapping library(inlmisc) # for interactive mapping on top of leaflet Create an example tabular dataset of stations that we want to make into a spatial dataset. Note: we are using the tibble::tribble() function to define a tibble row by row, which makes for easy visualization of test datasets. This is just one of many ways to create a tibble. exampleSites &lt;- tribble( ~StationID, ~Latitude, ~Longitude, &quot;Station_1&quot;, 37.812840, -80.063946, &quot;Station_2&quot;, 37.782322, -79.961449, &quot;Station_3&quot;, 37.801644, -79.968441) exampleSites ## # A tibble: 3 x 3 ## StationID Latitude Longitude ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Station_1 37.8 -80.1 ## 2 Station_2 37.8 -80.0 ## 3 Station_3 37.8 -80.0 Using the sf package, we can use the Latitude and Longitude fields to convert this object into a spatial object. Note we are using EPSG 4326 for our coordinate reference system. We chose that CRS exampleSites_sf &lt;- exampleSites %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) # Now look at the difference in the object. The geometry listcolumn is where all the spatial magic is stored. exampleSites_sf ## Simple feature collection with 3 features and 3 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -80.06395 ymin: 37.78232 xmax: -79.96145 ymax: 37.81284 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## # A tibble: 3 x 4 ## StationID Latitude Longitude geometry ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;POINT [°]&gt; ## 1 Station_1 37.8 -80.1 (-80.06395 37.81284) ## 2 Station_2 37.8 -80.0 (-79.96145 37.78232) ## 3 Station_3 37.8 -80.0 (-79.96844 37.80164) You can operate on a spatial dataset created using this method just like any other tidy object and the sticky geometry will come along for the ride. justOneSite &lt;- filter(exampleSites_sf, StationID == &#39;Station_1&#39;) Lets plot the result to see what happened. CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addCircleMarkers(data = exampleSites_sf, color=&#39;yellow&#39;, fillColor=&#39;black&#39;, radius = 5, fillOpacity = 1, opacity=1,weight = 1,stroke=T, group=&quot;All Sites&quot;, label = ~StationID, popup=leafpop::popupTable(exampleSites_sf)) %&gt;% addCircleMarkers(data = justOneSite, color=&#39;orange&#39;, fillColor=&#39;orange&#39;, radius = 5, fillOpacity = 1, opacity=1,weight = 1,stroke=T, group=&quot;Just One Site&quot;, label = ~StationID, popup=leafpop::popupTable(justOneSite)) %&gt;% addLayersControl(baseGroups=c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), overlayGroups = c(&#39;Just One Site&#39;, &#39;All Sites&#39;), options=layersControlOptions(collapsed=T), position=&#39;topleft&#39;) 3.1.1 Removing Spatial Information Sometimes we need to remove spatial information in order to operate on a dataset (e.g. save just the tabular data as a csv). We can use sf to remove just the spatial information. exampleSites_noSpatial &lt;- exampleSites_sf %&gt;% st_drop_geometry() exampleSites_noSpatial ## # A tibble: 3 x 3 ## StationID Latitude Longitude ## * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Station_1 37.8 -80.1 ## 2 Station_2 37.8 -80.0 ## 3 Station_3 37.8 -80.0 "],["consumingGISRESTservices.html", "3.2 Consuming GIS REST Services in R", " 3.2 Consuming GIS REST Services in R Section Contact:Rex Robichaux (rex.robichaux@deq.virginia.gov) 3.2.1 GIS Rest Service Data DEQ Makes a wide variety of its geospatial data available in the form of published REST services (Esri Map Services). First- a little background information on GIS Services in general: Esri ArcGIS REST API documentation One advantage of learning to use Esri REST services over these other formats is that a user may filter their data prior to storing it locally, limiting download size and required data cleaning/filtering. Users may also go on to implement use of these APIs within programs and scripts and know that they are using the most current data available. Another advantage of utilizing Esri REST platforms is that they are available to non Esri clients allowing users to avoid costly subscription fees surrounding ArcGIS and other Esri software. 3.2.2 Primary DEQ REST Endpoints: Internal (Staff Only) Services: https://gis.deq.virginia.gov/arcgis/rest/services/staff External (Public Facing) Services: https://gis.deq.virginia.gov/arcgis/rest/services/public Its important to note that all GIS Services have an API access point to perform queries, which can greatly assist in scripting/query development. Once you have located a service, and a particular layer within a service (denoted by a /# format (ending in a number) such as https://gis.deq.virginia.gov/arcgis/rest/services/staff/DEQInternalDataViewer/MapServer/72 for TMDL Watersheds), you can find a Query operation at the bottom of the page under the Supported Operations section. Upon selecting the Query operation, ArcGIS Server will allow you to dynamically create/test and run custom queries, and provide you with variable output formats (HTML, json, KMZ, GeoJSON, and PBF). Due to GeoJSONs flexibility, and ease of use in R, we will focus on GeoJSON query output formats throughout this exercise. ArcGIS Server REST Query Interface 3.2.3 How do you know what fields/criteria to query on? There are a couple of easy ways to determine criteria upon which to build your queries around. The first, and most immediate way is to simply examine the layer information at the REST endpoint which will show all field names and aliases. Below is an example of the fields present in the TMDL Watersheds layer: REST layer fields for TMDL Watersheds The second way to really view the underlying data, is to leverage our web GIS apps to poke into the attribute and spatial data itself. Below are some examples of using the GIS Staff App to view and filter on specific attributes. We will start by opening the attribute table for the TMDL Watersheds layer in the DEQ Data Layers list: With the Attribute Table widget open, we can now build and apply validation-based filters to view the actual values in any field: TMDL Table Filtering To view the unique values found in any field, simply select Unique in the set input type drop down. After a second, you will be able to view all of the valid values in whichever field has been specified. Here, we are looking at the Pollutant Name (POL_NAME) field, and viewing valid categories for that field. TMDL unique values TMDL value filtering 3.2.4 Okaylets build a simple query and see it in R For this first example, we will query the DEQ REST service underlying the GIS Staff App. The REST url for this service is: https://gis.deq.virginia.gov/arcgis/rest/services/staff/DEQInternalDataViewer/MapServer Well start by querying the WQM Stations layer (layer/ID = 104). We can see there is a wide range of fields that we can query this data layer on. For this first example, we will simply query for a single Station, with a Station ID (or WQM_STA_ID) of 2-FLA028.98. So whats going on below? We parse the URL to create a list where we can add all the parameters of our query - where, outFields, returnGeometry and f - with their appropriate values. As soon as the list is populated we use the function build_url() to create a properly encoded request. This request is the passed to the function st_read() to populate Stream_Stations. This Stream_Stations becomes both a simple features object and a data frame, i.e. it does contain both the geometry and the attribute values of the Station (in this case, a singular station). The interactive inlmisc::CreateWebMap() has various basemaps, a ID/Popup (on hover) based on the Station ID (can be changed to other fields), as well as the ability to zoom in/out: #Read in GIS REST Data in geojson format library(tidyverse) library(httr) library(sf) library(leaflet) library(inlmisc) url &lt;- parse_url(&quot;https://gis.deq.virginia.gov/arcgis/rest/services&quot;) url$path &lt;- paste(url$path, &quot;staff/DEQInternalDataViewer/MapServer/104/query&quot;, sep = &quot;/&quot;) url$query &lt;- list(where = &quot;WQM_STA_ID = &#39;2-FLA028.98&#39;&quot;, outFields = &quot;*&quot;, returnGeometry = &quot;true&quot;, f = &quot;geojson&quot;) request &lt;- build_url(url) Stream_Station &lt;- st_read(request) CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addCircleMarkers(data = Stream_Stations, color = &#39;red&#39;, label = ~WQM_STA_ID ) As this data is now available for viewing, we can easily view/summarize/manipulate it further via summary, etc. 3.2.5 Querying multiple records: Lets look at the capability and performance of querying a series of records, based off of a more complex where statement. In this example, we are searching for all stations records attributed to the Appomattox River: #Read in GIS REST Data in geojson format url &lt;- parse_url(&quot;https://gis.deq.virginia.gov/arcgis/rest/services&quot;) url$path &lt;- paste(url$path, &quot;staff/DEQInternalDataViewer/MapServer/104/query&quot;, sep = &quot;/&quot;) url$query &lt;- list(where = &quot;WQM_STA_STREAM_NAME = &#39;Appomattox River&#39;&quot;, outFields = &quot;*&quot;, returnGeometry = &quot;true&quot;, f = &quot;geojson&quot;) request &lt;- build_url(url) Stream_Stations &lt;- st_read(request) CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addCircleMarkers(data = Stream_Stations, color = &#39;red&#39;, label = ~WQM_STA_ID ) 3.2.6 This works on lines and polygons as well!: We can apply this same logic and format to query and spatially display polygon based layer. For example, we will query the TMDL Watersheds layer (Layer ID 72), searching for all Benthic Impairment TMDLs. We can also convert this data and read it into a data frame for further interpretation: url &lt;- parse_url(&quot;https://gis.deq.virginia.gov/arcgis/rest/services&quot;) url$path &lt;- paste(url$path, &quot;staff/DEQInternalDataViewer/MapServer/72/query&quot;, sep = &quot;/&quot;) url$query &lt;- list(where = &quot;IMP_NAME = &#39;Benthic-Macroinvertebrate Bioassessments (Streams)&#39;&quot;, outFields = &quot;*&quot;, returnGeometry = &quot;true&quot;, f = &quot;geojson&quot;) request &lt;- build_url(url) Benthic_TMDLs &lt;- st_read(request) pal &lt;- colorFactor( palette = &quot;Set2&quot;, domain = unique(Benthic_TMDLs$WSHD_ID)) CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data = Benthic_TMDLs, color = ~pal(WSHD_ID), fillColor = ~pal(WSHD_ID), fillOpacity = 0.6, stroke=5, label = ~WSHD_ID ) 3.2.7 Building more advanced queries You can also add as many criteria as the input data allows. In this example, we are querying TMDL records with Sediment pollutants in the VRO region. When classified by status, we can quickly see that all qualifying TMDL records are approved. url &lt;- parse_url(&quot;https://gis.deq.virginia.gov/arcgis/rest/services&quot;) url$path &lt;- paste(url$path, &quot;staff/DEQInternalDataViewer/MapServer/72/query&quot;, sep = &quot;/&quot;) url$query &lt;- list(where = &quot;POL_NAME = &#39;Sediment&#39; AND REGION = &#39;VRO&#39;&quot;, outFields = &quot;*&quot;, returnGeometry = &quot;true&quot;, f = &quot;geojson&quot;) request &lt;- build_url(url) VRO_Sediment_TMDLs &lt;- st_read(request) pal &lt;- colorFactor( palette = &quot;Set2&quot;, domain = unique(VRO_Sediment_TMDLs$PROJECT_STATUS)) CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data = VRO_Sediment_TMDLs, color = ~pal(WSHD_ID), fillColor = ~pal(PROJECT_STATUS), fillOpacity = 0.6, stroke=5, label = ~WSHD_ID ) %&gt;% addLegend(data = VRO_Sediment_TMDLs, pal = pal, values = ~PROJECT_STATUS, title = &quot;Legend&quot;, position = &#39;topright&#39;) Armed with your new confidence and familiarity with querying and interacting with ArcGIS Server web services go out into the world and see what kinds of data you can query, summarize and analyze! "],["spatialOperationsGISRESTservices.html", "3.3 Spatial Operations on GIS REST Services in R", " 3.3 Spatial Operations on GIS REST Services in R Section Contact:Rex Robichaux (rex.robichaux@deq.virginia.gov) 3.3.1 GIS Rest Service Data DEQs GIS REST endpoints enable easy access to geospatial data for querying and analysis. For a primer on what these services offer and the basics of querying against these services, please see the Consuming GIS REST Services in R section. 3.3.2 Level Up Your Querying Game The final example of Consuming GIS REST Services in R highlights how to use more complicated queries to limit data from within a single spatial layer. What about using information from one layer to limit the information provided on a second layer? Yeah, thats where we are going. But first, why would we want to do this? Besides using the GIS REST services to retrieve the most up to date version of spatial data, we also like to use these services to limit the amount of redundant data that we maintain on our local machines. Instead of, say, bringing back an entire polygon layer and a point layer, spatially intersecting them to find commonalities, and then doing something with said intersected data, we can go straight to the doing something with intersected data step by forcing the GIS REST service to perform our spatial operations. This reduces the amount of data we need to bring back to our machine initially, and reduces the number of associated files that we create during analysis steps. Win win. 3.3.3 Set Up Your Environment Load in the necessary packages. library(sf) library(leaflet) library(inlmisc) library(geojsonsf) library(dplyr) library(urltools) library(rgdal) library(httr) 3.3.4 Query One Layer We will begin by querying the Internal (Staff Only) REST Services by parsing a query of the TMDL records for a specific TMDL project in the Willis River Watershed identified as POL0119. After querying the data we will plot it using and interactive map built with inlmisc::CreateWebMap(). url &lt;- parse_url(&quot;https://gis.deq.virginia.gov/arcgis/rest/services&quot;) url$path &lt;- paste(url$path, &quot;staff/DEQInternalDataViewer/MapServer/72/query&quot;, sep = &quot;/&quot;) url$query &lt;- list(where = &quot;TMDL_EQ_ID = &#39;POL0119&#39;&quot;, outFields = &quot;*&quot;, returnGeometry = &quot;true&quot;, f = &quot;geojson&quot;) request &lt;- build_url(url) tmdl &lt;- st_read(request) CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data = tmdl, color = &#39;black&#39;, fillColor = &#39;red&#39;, fillOpacity = 0.6, stroke=5, label = ~WSHD_ID ) Once we have our chosen watershed, we want to identify monitoring stations within the watershed to investigate further. This could be done one of two ways. The first way, which is much less efficient, is to compare the TMDL watershed against all possible monitoring stations available via the REST service. This would require querying all monitoring stations, bringing that information into your local memory, and spatially intersecting the TMDL watershed against the many thousands of monitoring stations. That is not a great idea for this use case. The preferred method is a two step process. Instead of querying all of the possible monitoring stations from the REST service, we could simply bring back stations that fall close by the TMDL watershed and then spatially intersect those against the watershed to find out which stations are actually within said watershed. That is a much better method as it only brings necessary data into your memory and is a much faster operation to perform on the REST service. To identify the stations that fall close by the TMDL watershed, we can use a piece of information embedded in that watershed known as a bounding box. These are the coordinates of an area that encompasses our watershed. Lets extract this information from the watershed and look at the results. bBox &lt;- st_bbox(tmdl) bBox ## xmin ymin xmax ymax ## -78.63319 37.36281 -78.10111 37.72192 The bBox object contains the latitude and longitude values of the outermost extent of our TMDL watershed. To actually make this useful, lets convert it to an sfc object so we can plot it on a map with our TMDL watershed. For a good resource on simple feature objects, see this article by Jesse Sadler. bBoxPolygon &lt;- bBox %&gt;% st_as_sfc() CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data = tmdl, color = &#39;black&#39;, fillColor = &#39;green&#39;, fillOpacity = 0.6, stroke=5, label = ~WSHD_ID ) %&gt;% addPolygons(data=bBoxPolygon, weight=2, fill=FALSE, color=&quot;blue&quot;) ### Query Layer Two Based On Information From Layer One We can tell the GIS REST service to only retrieve monitoring stations within this bounding box (i.e. query a layer based on information from another layer) by adding this bounding box information into our query of the WQM Stations layer. This forces the REST service to perform the more computationally expensive step of only returning stations within our area. We are also going to add the XY coordinates to the object to make the mapping steps easier using st_coordinates(). # baseURL for the WQM Stations layer baseURL &lt;- &#39;https://gis.deq.virginia.gov/arcgis/rest/services/staff/DEQInternalDataViewer/MapServer/104/query?&#39; # convert the bounding box from above to a character string to work in our query bbox &lt;- toString(bBox) # encode for use within URL bbox &lt;- urltools::url_encode(bbox) # EPSG code for coordinate reference system used by the TMDL polygon sf object epsg &lt;- st_crs(tmdl)$epsg # set parameters for query query &lt;- urltools::param_set(baseURL,key=&quot;geometry&quot;, value=bbox) %&gt;% param_set(key=&quot;inSR&quot;, value=epsg) %&gt;% param_set(key=&quot;resultRecordCount&quot;, value=500) %&gt;% param_set(key=&quot;f&quot;, value=&quot;geojson&quot;) %&gt;% param_set(key=&quot;outFields&quot;, value=&quot;*&quot;) # query the REST service wqmstations_inbox &lt;- geojson_sf(query) %&gt;% dplyr::mutate(Longitude = sf::st_coordinates(.)[,1], Latitude = sf::st_coordinates(.)[,2]) %&gt;% dplyr::select(STATION_ID, Latitude, Longitude, everything()) # print out the data using DT::datatable() datatable( wqmstations_inbox, rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) Lets plot these stations to make sure our query returned what we asked. CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data = tmdl, color = &#39;black&#39;, fillColor = &#39;green&#39;, fillOpacity = 0.6, stroke=5, label = ~WSHD_ID ) %&gt;% addPolygons(data=bBoxPolygon, weight=2, fill=FALSE, color=&quot;blue&quot;) %&gt;% addCircleMarkers(data = wqmstations_inbox, lng = ~Longitude, lat = ~Latitude, label = ~as.character(STATION_ID)) That is still a lot of stations! This is a good example of why it is a best practice to have the database do as much data limiting for you as possible. 3.3.5 Spatially Intersect Layers The final step of this workflow is to spatially intersect the monitoring stations within the watershed (i.e. find the points inside the polygon). We will do this with the st_intersection() function from the sf library. Once we intersect these layers, we will add the final point object to our map, colored in red, to verify the intersection only returned monitoring stations within the polygon. Like before, we will add the XY coordinates of these point to the object using st_coordinates() to enable easier plotting. insideWatershed &lt;- st_intersection(wqmstations_inbox, tmdl) %&gt;% dplyr::mutate(Longitude = sf::st_coordinates(.)[,1], Latitude = sf::st_coordinates(.)[,2]) datatable(insideWatershed) CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data = tmdl, color = &#39;black&#39;, fillColor = &#39;green&#39;, fillOpacity = 0.6, stroke=5, label = ~WSHD_ID ) %&gt;% addPolygons(data=bBoxPolygon, weight=2, fill=FALSE, color=&quot;blue&quot;) %&gt;% addCircleMarkers(data = wqmstations_inbox, lng = ~Longitude, lat = ~Latitude, label = ~as.character(STATION_ID)) %&gt;% addCircleMarkers(data = insideWatershed, lng = ~Longitude, lat = ~Latitude, color = &#39;red&#39;, label = ~as.character(STATION_ID) ) Mission accomplished. If we look closely at our data we can see there are 149 rows of information in the point file, but only 60 unique monitoring stations to investigate for the rest of our analysis. The map indicates this with the redder sites, indicating there is overplotting of points. We can clean this up in subsequent data manipulation steps to make a publication quality map. insideWatershedUniqueSites &lt;- insideWatershed %&gt;% dplyr::select(STATION_ID, WQM_STA_DESC, everything()) %&gt;% #rearrange columns distinct(STATION_ID, .keep_all = T) # keep only rows with unique STATION_ID information CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data = tmdl, color = &#39;black&#39;, fillColor = &#39;green&#39;, fillOpacity = 0.6, stroke=5, label = ~WSHD_ID ) %&gt;% addCircleMarkers(data = insideWatershedUniqueSites, lng = ~Longitude, lat = ~Latitude, color = &#39;red&#39;, label = ~as.character(STATION_ID) ) "],["usingSpatialDatasets.html", "3.4 Using Spatial Datasets", " 3.4 Using Spatial Datasets Coming Soon! "],["interactiveMapping.html", "3.5 Interactive Mapping", " 3.5 Interactive Mapping with leaflet. Coming soon! "],["watershed-delineation.html", "3.6 Watershed Delineation", " 3.6 Watershed Delineation Section Contact: Emma Jones (emma.jones@deq.virginia.gov) This module overviews the basics of delineating watersheds directly in your local R environment by scraping USGSs StreamStats API. General web scraping techniques are beyond the scope of this module, but the basics can be gleaned by unpacking the referenced functions. All watersheds delineated using this technique use USGSs StreamStats delineation techniques from a 1:24k NHD. You may manually explore the tool here. First, load in the necessary packages and functions to complete this task. library(tidyverse) # for tidy data manipulation library(sf) # for spatial analysis library(leaflet) # for interactive mapping library(inlmisc) # for interactive mapping on top of leaflet You will also need to use a custom function built for scraping the StreamStats API. This function is contained in the sourced script StreamStatsAutoDelineation.R which be downloaded for sourcing locally in your environment. source(&#39;StreamStatsAutoDelineation.R&#39;) # for custom web scraping tool to hit USGS StreamStats API Create an example dataset of stations that we want to delineate. Note: we are using the tibble::tribble() function to define a tibble row by row, which makes for easy visualization of test datasets. This is just one of many ways to create a tibble. exampleSites &lt;- tribble( ~StationID, ~Latitude, ~Longitude, &quot;Station_1&quot;, 37.812840, -80.063946, &quot;Station_2&quot;, 37.782322, -79.961449, &quot;Station_3&quot;, 37.801644, -79.968441) exampleSites ## # A tibble: 3 x 3 ## StationID Latitude Longitude ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Station_1 37.8 -80.1 ## 2 Station_2 37.8 -80.0 ## 3 Station_3 37.8 -80.0 The above dataset is simply tabular data. One could use a spatial dataset for this task by stripping out the coordinate information from the geometry listcolumn. See Spatial Data from Tabular Data for more information on this topic. 3.6.1 Single Station Delineation Next lets use USGSs automated delineation tools to delineate single station. We will first select only one site from our exampleSites object we created in the previous step and name it stationToDelineate. Then, we will feed our streamStats_Delineation() function (called into our environment when we sourced the StreamStatsAutoDelineation.R script) the necessary location information (state, longitude, and latitude arguments). The state argument tells StreamStats which state NHD we want to use for delineation, VA (Virginia) for our example. Lastly, the UID argument is the unique identifier we wish to associate with the spatial information we pull back from StreamStats. If we do not provide this information, we will not know which watersheds belong to which sites as we start to batch process these jobs. stationToDelineate &lt;- filter(exampleSites, StationID == &#39;Station_1&#39;) stationDelineation &lt;- streamStats_Delineation(state= &#39;VA&#39;, longitude = stationToDelineate$Longitude, latitude = stationToDelineate$Latitude, UID = stationToDelineate$StationID) The information returned from StreamStats is a list object containing point information (the location we supplied to delineate from) and polygon information (the resultant upstream shape returned from the pour point). We can easily unpack this information into easily used objects using the script below. stationDelineationWatershed &lt;- stationDelineation$polygon %&gt;% reduce(rbind) %&gt;% arrange(UID) stationDelineationPoint &lt;- stationDelineation$point %&gt;% reduce(rbind) %&gt;% arrange(UID) We can plot the results quickly to verify the desired watershed was returned. StreamStats returns a watershed for the input coordinates to the best of its ability; however, the accuracy of the coordinates, datum, projection, etc. can influence the accuracy of the returned watershed. It is best practice to always review the returned watershed. Below is a minimal example of how to do this in R with an interactive map. Note: you can switch basemaps to the USGS Hydrography layer for further information using the layers button in the top left corner. The following interactive map is created with the inlmisc package, see the package authors article for a detailed tutorial. The interactive mapping section covers some basics with leaflet. CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data= stationDelineationWatershed, color = &#39;black&#39;, weight = 1, fillColor=&#39;blue&#39;, fillOpacity = 0.4,stroke=0.1, group=&quot;Watershed&quot;, popup=leafpop::popupTable(stationDelineationWatershed, zcol=c(&#39;UID&#39;))) %&gt;% addCircleMarkers(data = stationDelineationPoint, color=&#39;orange&#39;, fillColor=&#39;black&#39;, radius = 5, fillOpacity = 1, opacity=1,weight = 1,stroke=T, group=&quot;Station&quot;, label = ~UID, popup=leafpop::popupTable(stationDelineationPoint)) %&gt;% addLayersControl(baseGroups=c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), overlayGroups = c(&#39;Station&#39;,&#39;Watershed&#39;), options=layersControlOptions(collapsed=T), position=&#39;topleft&#39;) 3.6.2 Multiple Station Delineation Using the dataset created above (exampleSites), we will now batch process the sites to StreamStats for an efficient data workflow. Remember, you are hitting the USGS API repeatedly, so there can be losses in connectivity resulting in missed watersheds. We will overview the QA process after we receive the watershed information back from USGS. multistationDelineation &lt;- streamStats_Delineation(state= &#39;VA&#39;, longitude = exampleSites$Longitude, latitude = exampleSites$Latitude, UID = exampleSites$StationID) The information returned from StreamStats is a list object containing point information (the location we supplied to delineate from) and polygon information (the resultant upstream shape returned from the pour point). We can easily unpack this information into easily used objects using the script below. watersheds &lt;- multistationDelineation$polygon %&gt;% reduce(rbind) %&gt;% arrange(UID) points &lt;- multistationDelineation$point %&gt;% reduce(rbind) %&gt;% arrange(UID) The next chunk overviews how to efficiently check to make sure all the desired sites were in fact delineated. If there are missing sites, the script will run back out to StreamStats to get anyone that is missing and smash that into the original dataset. # fix anything that is missing if(nrow(points) != nrow(watersheds) | nrow(exampleSites) != nrow(watersheds)){ missing &lt;- unique( c(as.character(points$UID[!(points$UID %in% watersheds$UID)]), as.character(exampleSites$StationID[!(exampleSites$StationID %in% watersheds$UID)]))) missingDat &lt;- filter(exampleSites, StationID %in% missing) #remove missing site from the paired dataset points &lt;- filter(points, ! UID %in% missing) watersheds &lt;- filter(watersheds, ! UID %in% missing) dat &lt;- streamStats_Delineation(state= &#39;VA&#39;, longitude = missingDat$Long, latitude = missingDat$Lat, UID = missingDat$StationID) watersheds_missing &lt;- dat$polygon %&gt;% reduce(rbind) points_missing &lt;- dat$point %&gt;% reduce(rbind) watersheds &lt;- rbind(watersheds, watersheds_missing) %&gt;% arrange(UID) points &lt;- rbind(points, points_missing) %&gt;% arrange(UID) rm(missingDat); rm(dat); rm(watersheds_missing); rm(points_missing) } Now lets map our results to ensure StreamStats delineated the correct watersheds. CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data= watersheds, color = &#39;black&#39;, weight = 1, fillColor=&#39;blue&#39;, fillOpacity = 0.4,stroke=0.1, group=&quot;Watershed&quot;, popup=leafpop::popupTable(watersheds, zcol=c(&#39;UID&#39;))) %&gt;% addCircleMarkers(data = points, color=&#39;orange&#39;, fillColor=&#39;black&#39;, radius = 5, fillOpacity = 1, opacity=1,weight = 1,stroke=T, group=&quot;Station&quot;, label = ~UID, popup=leafpop::popupTable(points)) %&gt;% addLayersControl(baseGroups=c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), overlayGroups = c(&#39;Station&#39;,&#39;Watershed&#39;), options=layersControlOptions(collapsed=T), position=&#39;topleft&#39;) 3.6.3 QA Should any of the returned watersheds prove incorrect based on visual analysis, you must remove that watershed from your dataset, manually delineate the watershed using StreamStats, and include that new watershed into your polygon dataset. See the Using Spatial Datasets section for example workflows. "],["landcover-analysis.html", "3.7 Landcover analysis", " 3.7 Landcover analysis Coming Soon "],["spatial-joins.html", "3.8 Spatial joins", " 3.8 Spatial joins Coming Soon "],["shinyAppHelp.html", "Chapter 4 Shiny App Pro Tips", " Chapter 4 Shiny App Pro Tips Shiny apps are interactive web based applications built in R to increase accessibility to analytical tools, workflows, and visualizations to non-R users. DEQ relies on these tools (hosted on the internal Connect platform) to extend data querying, manipulation, analysis, visualization, and reporting techniques to all staff, regardless of programming experience. Complicated workflows can be programmed in R using the shiny package to develop an easy to use front end interface, all in the R language. Shiny apps are easy to build. Many tutorials are available, but the best starting point is to follow the shiny tutorial by RStudio. More advanced shiny techniques employed regularly by DEQ staff to improve application responsiveness, user experience, or back end development are outlined below. Coming soon: Shiny file organization - Using functions and modules to improve code organization, how to integrate nested modules without namespace issues Shiny Tricks: Using HTML to help reset a complex UI How to integrate tool-tip HTML without calling new packages Escaping characters in user inputs Markdown and User-Friendly Output Files: readxlsx tips for formatting excel outputs Markdown files with \"child sub-markdown files Markdown tips in general for TinyTex and HTML "],["shiny-file-organization.html", "4.1 Shiny file organization", " 4.1 Shiny file organization Coming Soon! "],["shiny-tricks.html", "4.2 Shiny Tricks", " 4.2 Shiny Tricks Coming Soon! "],["markdown-and-user-friendly-output-files.html", "4.3 Markdown and User-Friendly Output Files", " 4.3 Markdown and User-Friendly Output Files Coming Soon! "],["miscTips.html", "Chapter 5 Miscellaeous Tips and Tricks ", " Chapter 5 Miscellaeous Tips and Tricks "],["regular-expression-and-pattern-matching.html", "5.1 Regular expression and pattern matching", " 5.1 Regular expression and pattern matching Section Contact: Connor Brogan (connor.brogan@deq.virginia.gov) Lets imagine a scenario where we have a series of water quality data that is filled with manually inputted character fields and notes. Although the notes are consistent in their structure, they arent easily read by R. For instance, we may have an entire column of field notes that look like Flow was 98 cfs. Temperature was 20 deg C. Recorded by LL. We need to get the flow, temperature, and field scientist from all 1200 records. Obviously, it would take a lot of time to break this apart manually. So how do we get R to find the data we need? The answer lies in regular expressions. Regular expression (often short-handed as Regex) is a method of matching patterns in string (or, in R, character) type data. Regular expression is powerful and mastering it can greatly enhance your applications and tools. You will be able to reliably find dates, flow values, station names, and more from long, poorly formatted data. However, regular expression is built on specific sequence structures that are different from typical R syntax. It is not a formal language, but is instead a series of key words and quantifiers. Because of this, R will not output any warnings or errors when working with regex. And in this way developing the right regular expression for your application can be extremely vexing. You will either create the perfect pattern to accomplish your goal, or you will not. So, if we get the pattern just right, we can extract 98 as the streamflow from the string Flow was 98 cfs. Temperature was 20 deg C. Recorded by LL. But if we mess it up, we may end up with Flow was 98 or 98 cfs. Temperature was 20 deg C. Recorded by LL. and R wont give us any information as to why. This section has been written to cover basic regular expression structure and key words. It concludes with a few examples of patterns that are commonly needed when parsing water quality data. Additional resources are listed at the end of the chapter. 5.1.1 Example Data Overview For this chapter, the following dataset will be used to demonstrate how we can use Regular Expression to extract data from long, complicated, or inconsistent character data. Lets read in the data and see examine what we are working with. A copy of this dataset is available here: library(tidyverse) library(DT) FieldData &lt;- read_csv(myDataPath,col_types = cols()) datatable(FieldData,rownames = F) This is a messy dataset. Notice that all three fields are character. The dates were recorded as full sentences. The FieldNotes column contains a lot of useful data, but it is all mashed together in one string. The streamflow is recorded as either MGD, cfs, or gpm. Some notes include the ambient air temperature, but others do not. The observations were taken by many different people. Finally, it appears that streamflow was adjusted by a constant scaling factor, but again the factor was recorded within a full sentence in the FlowAdjustment column. We want to extract the day of the week each sample was taken, the date, the flow of the stream, the water temperature, and the flow scaling factor. To do this, we are going to need to understand regex structure and how to apply it in R. We will first learn a couple of R commands that take regex as an input and then we will dive into writing our own regex statements to manipulate our dataset FieldData. 5.1.2 Common R Commands and How to Use Them We will cover what commands that find matching patterns, that extract or replace matching patterns, and those that return logical vectors for matching patterns. We will demonstrate both Tidyverse and Base R functions. 5.1.2.1 Tidyverse There are three main functions available in the stringr library that are extremely useful for parsing string data. Many other functions exist within stringr that can speed up more complicated analyses so it may be worth reviewing the basic documentation of stringr. The function str_detect will tell us if our entries contain a pattern or not. It will return a logical vector that we can use to assess which data entries match our pattern. It takes a vector of strings and a pattern to be matched as input. For now, lets use a basic pattern. We can search for all dates that happened on Friday by simply trying to match Friday. haveFridays &lt;- FieldData %&gt;% select(Date) %&gt;% unlist() %&gt;% str_detect(&quot;Friday&quot;) head(haveFridays) ## [1] FALSE FALSE FALSE FALSE FALSE TRUE FieldData %&gt;% filter(haveFridays) %&gt;% datatable(rownames=F,options=list(lengthMenu = list(c(3,5,10,-1),c(&quot;3&quot;,&quot;5&quot;,&quot;10&quot;,&quot;All&quot;)))) In the previous example we used str_detect to create a logical vector that was TRUE for all data within the Date column that contained the word Friday. What if we wanted to find out which entries contained the word Friday? In that case, we can use str_which. haveFridays &lt;- FieldData %&gt;% select(Date) %&gt;% unlist() %&gt;% str_which(&quot;Friday&quot;) head(haveFridays) ## [1] 6 17 27 30 31 50 datatable(FieldData[haveFridays,],options=list(lengthMenu = list(c(3,5,10,-1),c(&quot;3&quot;,&quot;5&quot;,&quot;10&quot;,&quot;All&quot;))),rownames=F) Here, the vector haveFridays contains the index of each entry that contains the word Friday. In some applications, this vector can be extremely useful. For instance, we can find every data point that immediately follows one containing Friday by doing: haveFridaysData &lt;- FieldData[(haveFridays+1),] head(haveFridaysData) %&gt;% datatable(rownames=F,options=list(lengthMenu = list(c(3,5,10,-1),c(&quot;3&quot;,&quot;5&quot;,&quot;10&quot;,&quot;All&quot;)))) Both str_detect and str_which perform similar tasks. They both match a pattern and return, in some form, a list of vectors matching the pattern. But, what if we want to extract or replace the matched pattern? In our example dataset, we know there was a computer error and all entries signed LWO are meant to read WLO. errataData &lt;- FieldData %&gt;% select(FieldNotes) %&gt;% unlist() %&gt;% str_which(&quot;LWO&quot;) datatable(FieldData[errataData,],options = list(dom = &#39;t&#39;),rownames=F) We can quickly find this pattern and replace it with the correct one using str_replace, which takes a vector of character data, a pattern to replace, and the data to replace it with: FieldData[errataData,] %&gt;% select(FieldNotes) %&gt;% unlist() %&gt;% str_replace(&quot;LWO&quot;,&quot;WLO&quot;) ## [1] &quot;Streamflow is 0.2 gpm. Ambient Air Temperature was 28 deg C. Temperature was 28.9 deg C. Recorded by WLO.&quot; ## [2] &quot;flow was 102.5 cfs. Ambient Air Temperature was 28 deg C. Temperature was 25.1 deg C. Recorded by WLO.&quot; ## [3] &quot;Stream flow at 0.2 gpm. Wadable conditions. Temperature was 29.8 deg C. Recorded by WLO.&quot; As we learn to detect patterns using Regex, we will continually return to these three commands. 5.1.2.2 Base R There are three main functions available in base R that are extremely useful for parsing string data. The function grepl will tell us if our entries contain a pattern or not. It will return a logical vector that we can use to assess which data entries match our pattern. It takes a vector of strings and a pattern to be matched as input. For now, lets use a basic pattern. We can search for all dates that happened on Friday by simply trying to match Friday. haveFridays &lt;- grepl(&quot;Friday&quot;,FieldData$Date) head(haveFridays) ## [1] FALSE FALSE FALSE FALSE FALSE TRUE datatable(FieldData[haveFridays,],options=list(lengthMenu = list(c(3,5,10,-1),c(&quot;3&quot;,&quot;5&quot;,&quot;10&quot;,&quot;All&quot;))),rownames=F) In the previous example we used grepl to create a logical vector that was TRUE for all data within the Date column that contained the word Friday. What if we wanted to find out which entries contained the word Friday? In that case, we can use grep. haveFridays &lt;- grep(&quot;Friday&quot;,FieldData$Date) head(haveFridays) ## [1] 6 17 27 30 31 50 datatable(FieldData[haveFridays,],options=list(lengthMenu = list(c(3,5,10,-1),c(&quot;3&quot;,&quot;5&quot;,&quot;10&quot;,&quot;All&quot;))),rownames=F) Here, the vector haveFridays contains the index of each entry that contains the word Friday. In some applications, this vector can be extremely useful. For instance, we can find every data point that immediately follows one containing Friday by doing: haveFridaysData &lt;- FieldData[(haveFridays+1),] datatable(head(haveFridaysData),rownames=F,options=list(lengthMenu = list(c(3,5,10,-1),c(&quot;3&quot;,&quot;5&quot;,&quot;10&quot;,&quot;All&quot;)))) Both grepl and grep perform similar tasks. They both match a pattern and return, in some form, a list of vectors matching the pattern. But, what if we want to extract or replace the matched pattern? In our example dataset, we know there was a computer error and all entries signed LWO are meant to read WLO. errataData &lt;- grep(&quot;LWO&quot;,FieldData$FieldNotes) datatable(FieldData[errataData,],options = list(dom = &#39;t&#39;),rownames=F) We can quickly find this pattern and replace it with the correct one using gsub, which takes a vector of character data, a pattern to replace, and the data to replace it with: gsub(&quot;LWO&quot;,&quot;WLO&quot;,FieldData$FieldNotes[errataData]) ## [1] &quot;Streamflow is 0.2 gpm. Ambient Air Temperature was 28 deg C. Temperature was 28.9 deg C. Recorded by WLO.&quot; ## [2] &quot;flow was 102.5 cfs. Ambient Air Temperature was 28 deg C. Temperature was 25.1 deg C. Recorded by WLO.&quot; ## [3] &quot;Stream flow at 0.2 gpm. Wadable conditions. Temperature was 29.8 deg C. Recorded by WLO.&quot; As we learn to detect patterns using Regex, we will continually return to these three commands. 5.1.3 Basic Regular Expressions As we saw above in our demonstrations of the core string matching commands, we can use words as the pattern to be matched. We can also use whole phrases. Below, note that grep() finds three entries containing Friday but only the one when we search for It is Friday. And it cant detect the last entry because it is searching for the whole pattern Friday and the last two are broken up by punctuation or capitalized. Regular expression is extremely case sensitive! notes &lt;- c( &quot;It is Friday today, hooray&quot;, &quot;It will never be Friday&quot;, &quot;Is today Friday or Frie-day?&quot;, &quot;I like mine with lettuce and tomato&quot;, &quot;F.r.i.d.a.y.&quot;, &quot;FRIDAY&quot; ) grep(&quot;Friday&quot;,notes) ## [1] 1 2 3 grep(&quot;It is Friday&quot;,notes) ## [1] 1 We can search for multiple phrases at the same time using the pipe symbol |: grep(&quot;Friday|FRIDAY&quot;,notes) ## [1] 1 2 3 6 grep(&quot;It is Friday|never be Friday&quot;,notes) ## [1] 1 2 5.1.4 Character Classes When it comes to crafting regex expressions, there are a couple of characters and character classes that are essential to matching complex character data. We can use character classes to quickly extract basic data from long strings. For simple data extraction, there are often a number of solutions to get at the data we want. Returning to our example, we can easily obtain the flow scaling factor from the FlowAdjustment column of FieldData by using character classes: #Here, we remove everything that is NOT a digit or a literal &quot;.&quot; gsub(&quot;[^\\\\.0-9]&quot;,&quot;&quot;,FieldData$FlowAdjustment[1:3]) ## [1] &quot;0.3&quot; &quot;0.9&quot; &quot;0.1&quot; #Here, we remove everything that is an alphabetic character or a blank space gsub(&quot;[[:alpha:]]|[[:space:]]&quot;,&quot;&quot;,FieldData$FlowAdjustment[1:3]) ## [1] &quot;0.3&quot; &quot;0.9&quot; &quot;0.1&quot; With our example data, we can use character classes to get the date if we are careful about what string we extract. #Here, we remove everything that is not the digits 0-9 as well as the &#39;dash&#39; symbol gsub(&quot;[^0-9\\\\-]&quot;,&quot;&quot;,FieldData$Date[1:3]) ## [1] &quot;2021-07-28&quot; &quot;2019-03-27&quot; &quot;2018-06-05&quot; #Here, we remove everything that is an alphabetic character or a blank space gsub(&quot;[[:alpha:]]|[[:space:]]|,&quot;,&quot;&quot;,FieldData$Date[1:3]) ## [1] &quot;2021-07-28&quot; &quot;2019-03-27&quot; &quot;2018-06-05&quot; So, we can easily use character classes to find the date and flow adjustment within our dataset. For simple data extraction, character classes provide an easy way to find data within long strings. #We can add the date and flow adjustment as a new columns to FieldData FieldData$ActualDate &lt;- as.Date(gsub(&quot;[^0-9\\\\-]&quot;,&quot;&quot;,FieldData$Date)) #Here, we remove everything that is NOT a digit or a literal &quot;.&quot; FieldData$AdjustmentRatio &lt;- as.numeric(gsub(&quot;[^\\\\.0-9]&quot;,&quot;&quot;,FieldData$FlowAdjustment)) datatable(FieldData,rownames=F,options=list(lengthMenu = list(c(5,10,-1),c(&quot;5&quot;,&quot;10&quot;,&quot;All&quot;)))) 5.1.5 Quantifiers As we saw above, character classes can be used in regex to easily extract data that stands out from the rest of the string. We can use them to find the date within a sentence or we can get the single number from a statement. However, they are not useful by themselves when there is mixed data within a single string.For instance, if we extract all numbers from field notes, we can a conglomerate that is not easy to understand. #We can&#39;t find the stream temperature just by extracting all digits: FieldData$FieldNotes[1:3] ## [1] &quot;Streamflow is 107.7 cfs. Wadable conditions. Temperature was 24.3 deg C. Recorded by EFA.&quot; ## [2] &quot;flow was 100 cfs. Wadable conditions. Temperature was 29 deg C. Recorded by MAB.&quot; ## [3] &quot;Streamflow is 135.8 MGD. Ambient Air Temperature was 28 deg C. Temperature was 28.4 deg C. Recorded by GVN.&quot; gsub(&quot;\\\\D&quot;,&quot;&quot;,FieldData$FieldNotes[1:3]) ## [1] &quot;1077243&quot; &quot;10029&quot; &quot;135828284&quot; To fix this problem, regex has a structure of quantifiers that allows us to search for particular patterns after some number of characters or previous matches. Regex always matches from left to right and takes the last possible match (or, in regex terms, it is greedy), so these quantifiers can help us find the second number in a sentence, for instance. We can use quantifiers on a character or a group of characters, as indicated by a set of parentheses. You will notice that in the table below, several of the quantifiers return the same value. Quantifiers are often used in tandem with anchors, which can be used to refine where in a string a pattern is expect. See Anchors for more information. dataToMatch&lt;-c(&#39;Q=4250 cfs&#39;,&#39;Q=210 cfs&#39;,&#39;Q=10 cfs&#39;,&#39;Q=45980 cfs&#39;,&#39;Q=0 cfs&#39;) Lets see if we can use quantifiers to address our example problem. We used character classes Character Classes to find the date and the flow adjustment ratio. We may be able to use quantifiers to find the stream flow, unit, water temperature, and field scientist. We will need to rely on groups of characters using parentheses. These groups will help us define a pattern and can also be called by gsub to return only the matched characters: #If we replace any number of characters that repeat any number of times and are followed #by &#39;Recorded by &#39; and replace all periods with &#39;&#39;, then we get the field scientist FieldData$FieldScientist&lt;- gsub(&quot;.*Recorded by |\\\\.&quot;,&quot;&quot;,FieldData$FieldNotes) head(FieldData$FieldScientist) ## [1] &quot;EFA&quot; &quot;MAB&quot; &quot;GVN&quot; &quot;LPX&quot; &quot;EHK&quot; &quot;NHL&quot; We cant find temperature or flow using this strategy because air temperature and the different flow units makes this challenging. We would instead need to write a long statement with a bunch of ors. This will work, but it is not ideal. The following statement looks for Stream flow, Streamflow, and flow by using the ? quantifier and also looks for is, was, or at. It also matches the streamflow units, followed by any number of any character. gsub replaces these with zero, leaving only the streamflow gsub(&quot;((Stream)? ?flow) (is|was|at) | (cfs|MGD|gpm).*&quot;,&quot;&quot;,FieldData$FieldNotes[1:10]) ## [1] &quot;107.7&quot; &quot;100&quot; &quot;135.8&quot; &quot;164.5&quot; &quot;0.2&quot; &quot;123.1&quot; &quot;0.2&quot; &quot;96.3&quot; &quot;91.4&quot; ## [10] &quot;134.1&quot; Similar to the example above, we can use groups to find the stream temperature but it will take a lot of or statements and you will need to be very familiar with the exact format of every string of data (e.g. in the above example, we knew in advance there were several units and several ways to write stream flow). We can get around this using anchors, in combination with numeric quantifiers. 5.1.6 Anchors Anchors are use to specify the location of a sequence of characters within a string. They are used to match patterns that occur at the beginning or end of a string or word. Used in combination with quantifiers and character classes, anchors can help match any kind of pattern within a string. dataToMatch &lt;- c(&quot;streamflow = 20 cfs in february&quot;,&quot;No streamflow recorded in january&quot;,&quot;No streamflow recorded in april by Eujane&quot;,&quot;flow = 26 cfs in jan&quot;) Now, we can set about finding the stream flow, water temperature, and the day of the week the sample was taken from our example data using anchors, character classes, quantifiers, and groups. Note that we can return a group by using gsub and enter \\\\1 as the replacement vector! This is useful because we can build the group into a pattern that will match the whole string and then replace it with just the group. Effectively, this will allow us to easily capture groups if we match the whole string! #WEEKDAY: #Here, we replace &#39;Date Assessed &#39;, the comma, and everything after the comma with the #group consisting of any character, any number of times. Because we are familiar with the #data string, we know this will capture the weekday! Note that we use &quot;\\\\1&quot; to return the group #in the parentheses. Our pattern &quot;Date Assessed (.*),.*&quot; matches the ENTIRE string so we are #using gsub to replace the whole string with just the group in the parentheses! FieldData$Weekday &lt;- gsub(&quot;Date Assessed (.*),.*&quot;,&quot;\\\\1&quot;,FieldData$Date) #We could alternatively replace everything that is not in the area we expect the weekday to be FieldData$Weekday &lt;- gsub(&quot;(Date Assessed )|(,.*)&quot;,&quot;&quot;,FieldData$Date) #Or we could just search for each individual day of the week and return it! #Here we list each weekday in the following form Monday|Tuesday|...|Sunday which allows us #to look for any day of the week. But we will need this to be in a group and to #remove any character any number of times before and after the day of the week WeekdaySearch&lt;-paste(weekdays((Sys.Date()+0:6)),collapse=&quot;|&quot;) FieldData$Weekday &lt;- gsub(paste0(&quot;.*(&quot;,WeekdaySearch,&quot;).*&quot;),&quot;\\\\1&quot;,FieldData$Date) #FLOW # This statement is a little complicated, so let&#39;s break it down into it&#39;s pieces. # The gsub command will look for the pattern and replace it with the first group (as indicated # by the &quot;\\\\1&quot;!) If our pattern matches the whole string, we can replace it all with our group! #The pattern consists of: #&quot;.* &quot; = look for any character any number of times until a literal space #&quot;(\\\\d*\\\\.*\\\\d* \\\\w*)\\\\.&quot; = look any number of digits, followed by zero or more periods, #followed by any number of digits, a space, adn then any number of word characters that are #followed by a period. This will match our streamflow with unit! #&quot;.*(Temperature.*\\\\d*\\\\.*\\\\d*)?&quot; = look for any number of any character follow by zero or one #instances of &#39;Temperature&#39; followed by some characters and another digit, decimal, digit #combination. #&quot;.*Temperature.*\\\\d+(\\\\.)?\\\\d* deg C.* = Look for the temperature followed by some number of any #character followed by a digit, a potential decimal, another 0 or more digits, and deg C, followed #by any character any number of times FieldData$Flow &lt;- gsub(&quot;.* (\\\\d*\\\\.*\\\\d* \\\\w*)\\\\..*(Temperature.*\\\\d*\\\\.*\\\\d*)?.*Temperature.*\\\\d+(\\\\.)?\\\\d* deg C.*&quot;,&quot;\\\\1&quot;,FieldData$FieldNotes) #Stream Temperature: #The gsub command below will find the pattern and replace it with the first group (as indicated by #the &quot;\\\\1&quot;!) The pattern begins with any character for any number of times followed by a literal #space. The group (to be returned by &quot;\\\\1&quot;) is any digit one or more times, potentially followed #by a period and any number of digits. After the group, the pattern will match any character any #number of times.This pattern exactly matches the entire Field notes column and replaces it with #group 1, returned by gsub FieldData$WaterTemperature &lt;- as.numeric(gsub(&quot;.* (\\\\d+\\\\.*\\\\d*).*&quot;,&quot;\\\\1&quot;,FieldData$FieldNotes)) FieldData_clean &lt;- FieldData[,c(4,7,6,8,9,5)] datatable(FieldData_clean,rownames=F,caption=&quot;Field Data Modifed with Regular Expression&quot;) 5.1.7 Common or Useful Patterns Below is a common set of regrex commands to use for day to day tasks: The following command escapes all special regex characters from a string. This can be built into a function and used to validate user-input strings in a Shiny app! inputString&lt;-&quot;Now we have learned regex\\\\Regular Expression. What&#39;s Next? Application$&quot; gsub(&quot;([.|()\\\\^{}+$*?]|\\\\[|\\\\])&quot;, &quot;\\\\\\\\\\\\1&quot;,inputString) ## [1] &quot;Now we have learned regex\\\\\\\\Regular Expression\\\\. What&#39;s Next\\\\? Application\\\\$&quot; #this command can be useful as a function to check Shiny input fields for illegal characters: escapeCharacters&lt;-function(inputString){ outputString&lt;-gsub(&quot;([.|()\\\\^{}+$*?]|\\\\[|\\\\])&quot;, &quot;\\\\\\\\\\\\1&quot;,inputString) } To get the day, month, or year from a standard date string, we can use a simple gsub command! inputString&lt;-&quot;04/05/2063&quot; gsub(&quot;\\\\d+/(\\\\d+)/\\\\d+&quot;,&quot;\\\\1&quot;,inputString)#Day = Replace the whole pattern with the month ## [1] &quot;05&quot; gsub(&quot;/.*&quot;,&quot;&quot;,inputString)#Month = Replace everything after the first &quot;/&quot; ## [1] &quot;04&quot; gsub(&quot;^.*/&quot;,&quot;&quot;,inputString)#Year = Replace everything up until the last &quot;/&quot; ## [1] &quot;2063&quot; To get a file type from a given directory or to check the file type against an expected value: inputString&lt;-&quot;C:/Users/JTKirk/Desktop/SpO_CK.csv&quot; #Is the input file path of the correct file type? Check everything after last &quot;.&quot; gsub(&quot;.*(\\\\..*)$&quot;,&quot;\\\\1&quot;,inputString) ## [1] &quot;.csv&quot; grepl(&quot;.csv$&quot;,inputString) ## [1] TRUE We can use the . character class to check for empty strings: inputString&lt;-&quot;&quot; #Is the input string empty? Search to see if string contains any character grepl(&quot;.&quot;,inputString) ## [1] FALSE We can use gsub to find whole numbers and decimal numbers from a large numeric data string: inputString&lt;-&quot;23.952 25 26.859 98 -101 58.582&quot; #Get whole numbers or decimal numbers by checking for digits surrounding a literal &quot;.&quot; gsub(&quot;(-?\\\\d*\\\\.\\\\d* ?)&quot;,&quot;&quot;,inputString) ## [1] &quot;25 98 -101 &quot; #Decimal numbers are trickier, and its best to find them by breaking the string #into its component numbers: AllNumbers&lt;-strsplit(inputString,&quot; &quot;) AllNumbers&lt;-unlist(AllNumbers) #After splitting the string, we can find those with/without a decimal point AllNumbers[grepl(&quot;\\\\.&quot;,AllNumbers)]#Find all numbers containing a literal &quot;.&quot; ## [1] &quot;23.952&quot; &quot;26.859&quot; &quot;58.582&quot; wholeNumbers&lt;-AllNumbers[!grepl(&quot;\\\\.&quot;,AllNumbers)] 5.1.8 Additional Resources R help menu, e.g. ?regex https://ryanstutorials.net/regular-expressions-tutorial/regular-expressions-cheat-sheet.php https://www.tutorialspoint.com/vb.net/vb.net_regular_expressions.htm https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285 "],["non-standard-evaluation.html", "5.2 Non standard evaluation", " 5.2 Non standard evaluation "],["accessAPIfromR.html", "5.3 Accessing APIs from R", " 5.3 Accessing APIs from R Section Contact: Emma Jones (emma.jones@deq.virginia.gov) 5.3.1 What is an API Whether or not you realize it, you have been using application programming interfaces, or APIs, for years. APIs are a set of rules for building and integrating software so computer services and servers can communicate and share information efficiently. APIs are where users go to pull data from various entities. That said, some APIs are well documented and easily explain how users might access open data (e.g. the USGS StreamStats Service Documentation). However, many entities that make data available via an API dont have the resources to maintain robust documentation, but valuable data can still be acquired with a bit of thought and data exploration (and politely emailing the web service maintainer if all else fails!). 5.3.2 CMC API Use Case This example will overview the process of using R to scrape or acquire data from (at the time of writing) a less documented API. The Chesapeake Monitoring Cooperative (CMC) coordinates with citizen monitoring groups to share data resources collected from the Chesapeake Bay watershed. Thanks to a grant, DEQ and the CMC are expanding data resources to incorporate non-Bay citizen data into the database such that DEQ may point citizen groups throughout Virginia to a single data entry portal. This expansion will allow DEQ to pull all citizen monitoring data conducted for biannual Integrated Reports from one location in one data format. By automating the process of scraping the CMC API, DEQ staff will benefit from significant time savings acquiring and organizing data, as well as tagging data a different citizen monitoring data levels or tiers. 5.3.3 Required Packages In order to scrape APIs, you will need the following R packages installed and loaded into your environment. library(tidyverse) library(rmarkdown) # just for pretty tables in this document library(httr) library(jsonlite) 5.3.4 Explore the endpoint in a browser Like all good data analysis tasks, we must first understand what data are available and how they are formated before operating on the data. The CMC API is available off the odata endpoint. We can view the data available by pasting this URL into a browser window: https://cmc.vims.edu/odata/ . This tells us about the structure of the database and the names of the different datasets available from the API. It is helpful to explore a few datasets to understand what data are contained where and how to connect datasets. To demonstrate how to explore a dataset, we will first show the browser method and then how to bring back the data into R. Lets investigate the first dataset called Groups. To do so in a browser, we would simply augment our original URL to include Groups endpoint, e.g. https://cmc.vims.edu/odata/Groups . This returns a json dataset that is somewhat easy to understand, but lets pull it back into our R environment to make data exploration even easier. 5.3.5 Query API with R To use R to perform the same query, we need to feed the same URL from above into httr::GET(). If you were to print the results of the CMCgroupsQuery object you will see metadata on when the operation was executed, the status (200 if it is successful), data returned (json format), and data size. This isnt super helpful for actually working with the data, so lets convert this response to an R object. The nested functions fromJSON(rawToChar(CMCgroupsQuery$content))$value unpack the json data returned from the API endpoint into a data format you may be more familiar with. You can explore this step on your own and dig into more articles that explain the ins and outs of APIs and webscraping with R. CMCgroupsQuery &lt;- GET(&quot;https://cmc.vims.edu/odata/Groups&quot;) CMCgroups &lt;- fromJSON(rawToChar(CMCgroupsQuery$content))$value CMCgroups %&gt;% datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) Thats much better. We can now see this dataset contains information about the individual citizen groups that provide data to the CMC database. The Id field will be important when we want to query data from groups from the Samples dataset (see above). But first, lets build our first real query to the database. As with all databases, it is best practice to ask the database itself to perform the data simplification instead of bringing back unnecessary data. Lets identify all groups that are based in Virginia. To do so, we will add a filter statement to our URL and limit the data pulled back to just rows with State = VA. You can test this in a browser with the following URL: https://cmc.vims.edu/odata/Groups?$filter=State eq Va . Now lets try that same call with R. You will immediately notice a lot of wonky characters in this URL. When you send spaces or  to the browser, these will be converted to the appropriate character string for you. Note \"\" will return an error, so use . VAgroupsQuery &lt;- GET(&quot;https://cmc.vims.edu/odata/Groups?$filter=State%20eq%20%27Va%27&quot;) VAgroups &lt;- fromJSON(rawToChar(VAgroupsQuery$content))$value paged_table(VAgroups) Lets practice querying one more time on a different endpoint: Stations. This dataset contains all of the stations within the CMC database. For our task, we dont need any stations outside of Virginia, so lets only bring back stations within Virginia in this next query. The query will look like so: https://cmc.vims.edu/odata/Stations?$filter=State eq Virginia . VAstationsQuery &lt;- GET(&quot;https://cmc.vims.edu/odata/Stations?$filter=State%20eq%20%27Virginia%27&quot;) VAstations &lt;- fromJSON(rawToChar(VAstationsQuery$content))$value paged_table(VAstations) How did we know that in this dataset we needed to type out Virginia instead of Va like the above example? We first explored the Stations dataset in our browser to see how that dataset was structured, noticed the State field, and identified that each state name was spelled out completely. 5.3.6 Example: Query CMC API for Assessment Uses The problem outlined above will be completed using R in the following steps. We will use the previously created VAgroups object to identify which groups provided data in Virginia. Then we will query each of those GroupIds to query sample data within the assessment window of interest. Lastly, we will double check that all stations returned are in Virginia and pass necessary QA flags. First, lets make sure we can query data from the Sample endpoint. We can perform a very basic query by pulling back all the sample events collected by a certain group (e.g. 64) in the CMC database like so. Note the expand statement. This links the Samples dataset to the Parameter, Qualifier, Problem, and Event datasets in the database like a join would. By adding this expand step, we can avoid pulling each individual dataset into our environment and joining it to the relevant sample information. It is always best to ask the database to perform operations to minimize the amount of data returned into your environment. Note the filter statement is performed on the GroupId field from the joined in Event dataset. This field is represented in the dataset as Event.GroupId, but we need to communicate the . as a / in our URL. If you copy/paste the URL below into a browser, you will see the json dataset below returned. https://cmc.vims.edu/odata/Samples?$expand=Parameter,Qualifier,Problem,Event($expand=Station,Group)&amp;$filter=Event/GroupId eq 64 json sampleEventQuery &lt;- GET(&quot;https://cmc.vims.edu/odata/Samples?$expand=Parameter,Qualifier,Problem,Event($expand=Station,Group)&amp;$filter=Event/GroupId%20eq%2064&quot;) sampleEvent &lt;- fromJSON(rawToChar(sampleEventQuery$content))$value paged_table(sampleEvent) Now that we can query sample data by for one GroupId, lets only return data within a given temporal window. From this point on, we will parse URLs into more manageable pieces to make queries more understandable. See the Consuming GIS REST Services in R section for more information on parsing URL statements. Note that any date needs to be communicated in json datetime format. We are querying all samples collected by GroupId 64 from January 1, 2022 to March 15, 2022. # base URL for endpoint sampleParmeterQualifierEventGroupFilter &lt;- &quot;https://cmc.vims.edu/odata/Samples?$expand=Parameter,Qualifier,Event($expand=Station,Group)&amp;$filter=Event/GroupId%20eq%20&quot; # time window jsonFormatDateWindow &lt;- c(&quot;2022-01-01T00:00:00Z&quot;,&quot;2022-03-15T23:59:59Z&quot;) # One group groupId &lt;- 64 # just a helper and &lt;- &quot;%20and%20&quot; # Date filter statement inDateWindowFilter &lt;- paste0(&quot;Event/DateTime%20ge%20&quot;,jsonFormatDateWindow[1],and, &quot;Event/DateTime%20lt%20&quot;,jsonFormatDateWindow[2]) stationByDateFilter &lt;- fromJSON( rawToChar( GET( paste0(sampleParmeterQualifierEventGroupFilter, groupId, and, inDateWindowFilter) )$content))$value paged_table(stationByDateFilter) Now lets scale this solution to iterate through the GroupIds in Virginia (VAgroups) to compile a dataset of sample data within the 2022 Integrated Report data window (January 1, 2015 to December 31, 2020). We know that there are 37 unique GroupIds for Virginia, thats a lot of data to pull back at once. For this minimal example, we will only iterate through the first two groups, but the real assessment process would include all potential Virginia groups. We will demonstrate this using a for loop for clarity, but we offer that loops are slow and not best practices for iterating through analyses in a functional programming language like R. The better solution would be to build the operation into a function, but that is beyond the scope of this article. ### Establish all your important building blocks outside your loop # assessment period in a R format (for QA later) assessmentPeriod &lt;- as.POSIXct(c(&quot;2015-01-01 00:00:00 UTC&quot;,&quot;2020-12-31 23:59:59 UTC&quot;),tz=&#39;UTC&#39;) # assessment period in json format (for URL) jsonFormatAssessmentPeriod &lt;- c(&quot;2015-01-01T00:00:00Z&quot;,&quot;2020-12-31T23:59:59Z&quot;)# base URL for endpoint # base URL for endpoint sampleParmeterQualifierEventGroupFilter &lt;- &quot;https://cmc.vims.edu/odata/Samples?$expand=Parameter,Qualifier,Event($expand=Station,Group)&amp;$filter=Event/GroupId%20eq%20&quot; # just a helper and &lt;- &quot;%20and%20&quot; # Date filter statement inDateWindowFilter &lt;- paste0(&quot;Event/DateTime%20ge%20&quot;,jsonFormatAssessmentPeriod[1],and, &quot;Event/DateTime%20lt%20&quot;,jsonFormatAssessmentPeriod[2]) ## And provide a place to store output information dataOut &lt;- tibble() ## Now for the loop for (i in unique(VAgroups$Id)[1:2]){ stationByDateFilter &lt;- fromJSON( rawToChar( GET( paste0(sampleParmeterQualifierEventGroupFilter, i, and, inDateWindowFilter) )$content))$value dataOut &lt;- bind_rows(dataOut, stationByDateFilter) } # What was returned? glimpse(dataOut) ## Observations: 20,146 ## Variables: 17 ## $ Id &lt;int&gt; 21042, 21043, 21044, 21045, 21046, 21047, 21048, 21049... ## $ Value &lt;dbl&gt; 23.00, 8.60, 8.80, 130.00, 0.30, 120.00, 24.00, 25.20,... ## $ Depth &lt;dbl&gt; 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, NA, NA, NA, NA, NA,... ## $ SampleId &lt;int&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, ... ## $ Comments &lt;chr&gt; &quot;It is not letting me save due to \\&quot;errors\\&quot; from sect... ## $ EventId &lt;int&gt; 3272, 3272, 3272, 3272, 3272, 3272, 3272, 3272, 3272, ... ## $ ParameterId &lt;int&gt; 217, 227, 227, 229, 253, 264, 266, 268, 269, 269, 287,... ## $ ProblemId &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA... ## $ QualifierId &lt;int&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ QaFlagId &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ... ## $ CreatedBy &lt;chr&gt; &quot;c390249e-a157-4927-b0ee-e796f57cd173&quot;, &quot;c390249e-a157... ## $ CreatedDate &lt;chr&gt; &quot;2017-11-21T21:42:48.44Z&quot;, &quot;2017-11-21T21:42:48.44Z&quot;, ... ## $ ModifiedBy &lt;chr&gt; &quot;2473b5dc-f75b-47ea-a902-869bb804c21a&quot;, &quot;2473b5dc-f75b... ## $ ModifiedDate &lt;chr&gt; &quot;2018-01-09T20:58:08.577Z&quot;, &quot;2018-01-09T20:58:08.577Z&quot;... ## $ Parameter &lt;df[,31]&gt; &lt;data.frame[26 x 31]&gt; ## $ Qualifier &lt;df[,3]&gt; &lt;data.frame[26 x 3]&gt; ## $ Event &lt;df[,12]&gt; &lt;data.frame[26 x 12]&gt; Not bad! Within a minute or so (depending on your network speed) we have returned 20146 records for citizen data in Virginia. If you are an astute observer, you will notice that the Parameter, Qualifier, and Event columns are in fact list-columns used to efficiently store data. A full explanation of list-columns is beyond the scope of this article, but this snippet of code will unpack the data in a way we can use it for other purposes. unnested_dataOut &lt;- dataOut %&gt;% tidyr::unnest(Parameter, names_sep = &quot;.&quot;, keep_empty = TRUE) %&gt;% tidyr::unnest(Qualifier, names_sep = &quot;.&quot;, keep_empty = TRUE) %&gt;% tidyr::unnest(Event, names_sep = &quot;.&quot;, keep_empty = TRUE) names(unnested_dataOut) ## [1] &quot;Id&quot; ## [2] &quot;Value&quot; ## [3] &quot;Depth&quot; ## [4] &quot;SampleId&quot; ## [5] &quot;Comments&quot; ## [6] &quot;EventId&quot; ## [7] &quot;ParameterId&quot; ## [8] &quot;ProblemId&quot; ## [9] &quot;QualifierId&quot; ## [10] &quot;QaFlagId&quot; ## [11] &quot;CreatedBy&quot; ## [12] &quot;CreatedDate&quot; ## [13] &quot;ModifiedBy&quot; ## [14] &quot;ModifiedDate&quot; ## [15] &quot;Parameter.Id&quot; ## [16] &quot;Parameter.Code&quot; ## [17] &quot;Parameter.Name&quot; ## [18] &quot;Parameter.Units&quot; ## [19] &quot;Parameter.Method&quot; ## [20] &quot;Parameter.Tier&quot; ## [21] &quot;Parameter.DeqLevel&quot; ## [22] &quot;Parameter.Matrix&quot; ## [23] &quot;Parameter.Tidal&quot; ## [24] &quot;Parameter.NonTidal&quot; ## [25] &quot;Parameter.AnalyticalMethod&quot; ## [26] &quot;Parameter.ApprovedProcedure&quot; ## [27] &quot;Parameter.Equipment&quot; ## [28] &quot;Parameter.Precision&quot; ## [29] &quot;Parameter.Accuracy&quot; ## [30] &quot;Parameter.Range&quot; ## [31] &quot;Parameter.QcCriteria&quot; ## [32] &quot;Parameter.InspectionFreq&quot; ## [33] &quot;Parameter.InspectionType&quot; ## [34] &quot;Parameter.CalibrationFrequency&quot; ## [35] &quot;Parameter.StandardOrCalInstrumentUsed&quot; ## [36] &quot;Parameter.TierIIAdditionalReqs&quot; ## [37] &quot;Parameter.HoldingTime&quot; ## [38] &quot;Parameter.SamplePreservation&quot; ## [39] &quot;Parameter.requiresSampleDepth&quot; ## [40] &quot;Parameter.requiresDuplicate&quot; ## [41] &quot;Parameter.isCalibrationParameter&quot; ## [42] &quot;Parameter.Status&quot; ## [43] &quot;Parameter.Description&quot; ## [44] &quot;Parameter.NonfatalUpperRange&quot; ## [45] &quot;Parameter.NonfatalLowerRange&quot; ## [46] &quot;Qualifier.Id&quot; ## [47] &quot;Qualifier.Code&quot; ## [48] &quot;Qualifier.Description&quot; ## [49] &quot;Event.Id&quot; ## [50] &quot;Event.DateTime&quot; ## [51] &quot;Event.Project&quot; ## [52] &quot;Event.Comments&quot; ## [53] &quot;Event.StationId&quot; ## [54] &quot;Event.GroupId&quot; ## [55] &quot;Event.CreatedBy&quot; ## [56] &quot;Event.CreatedDate&quot; ## [57] &quot;Event.ModifiedBy&quot; ## [58] &quot;Event.ModifiedDate&quot; ## [59] &quot;Event.Station&quot; ## [60] &quot;Event.Group&quot; Lets do a little QA quickly to make sure that our data is in fact from Virginia stations (VAstations object from above) unnested_dataOut$Event.StationId[!unnested_dataOut$Event.StationId %in% VAstations$Id] ## [1] 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 ## [20] 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 ## [39] 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 ## [58] 252 252 252 252 252 252 252 our data is within the appropriate date window (assessmentPeriod object from above) unnested_dataOut %&gt;% # fix Event.DateTime (character) field to actual date time for QA mutate(`Event.DateTime2` = as.POSIXct( str_replace_all(Event.DateTime,c(&#39;T&#39;= &#39; &#39;, &#39;Z&#39; = &#39;&#39;)), format = &quot;%Y-%m-%d %H:%M:%S&quot;) ) %&gt;% summarise(min(Event.DateTime2), max(Event.DateTime2)) ## # A tibble: 1 x 2 ## `min(Event.DateTime2)` `max(Event.DateTime2)` ## &lt;dttm&gt; &lt;dttm&gt; ## 1 2015-01-02 14:00:00 2020-12-29 13:45:00 Interesting results! this is why it is always prudent to check and clean your data! First a note on the QA methods, the examples show both a base R and tidy method for QAing your results. One method is not more correct than the other. They each have utility depending on your use case and your personal syntax preferences. So we found that there was one station pulled back that wasnt in fact in Virginia. This tells us that querying on GroupId may not be the best method to get back only the data we want. The lesson learned is that some groups sample across state boundaries and we should use the VAstations object as a better starting point for this specific task. The second QA check demonstrated that we pulled back data from just the date range we wanted, great! This is something to check again when we change our Query to use unique StationIds as a starting point. 5.3.7 Parting Challenge Can you adjust the method above to answer the original question? Can you replace the loop with a function to complete the task? "]]
