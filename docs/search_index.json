[["index.html", "DEQ Data Science Primer Key Chapter 1 Background", " DEQ Data Science Primer Key Joe Famularo 7/27/2023 Chapter 1 Background The purpose of this book is to describe common tasks undertaken by DEQ staff and clearly outlines how to accomplish these objectives using the R programming language in order to promote learning opportunities for various skill levels of R users. This project is an example of a bookdown report built using R and RStudios R Markdown. Many authors have contributed to this effort: Emma Jones (emma.jones@deq.virginia.gov) Connor Brogan (connor.brogan@deq.virginia.gov) Rex Robichaux (rex.robichaux@deq.virginia.gov) Joe Famularo (joseph.famularo@deq.virginia.gov) Feel free to explore different chapters to learn about specific tasks and how to complete them efficiently and transparently in R. "],["whyR.html", "1.1 Why R?", " 1.1 Why R? Section Contact: (joseph.famularo@deq.virginia.gov) Why should you consider using R and why do we use R here at DEQ? Well address these questions in this chapter and well make the case by illustrating the following: R is accessible. R allows users to automate processes: saving time, building consistency, and reducing human-error. R gives users the ability to create outstanding data-driven products. 1.1.1 Accessible, Open Source Software R is an open source programming language, which fundamentally means that users contribute to the development of the software. RStudio is also free and is the integrated development environment (IDE) of choice for most users. These factors make R highly accessible, collaborative, and dynamic. Anyone can download R and RStudio onto their computer and get started. 1.1.1.1 Packages R benefits from the development of packages by its users. These packages enhance R by providing additional functions to the original software (i.e., base R). The availability of these packages gives users options when approaching questions and often makes arriving at a solution easier. Here are some examples to demonstrate the range of packages available to users. RColorBrewer - This package is used to simplify the creation of color palettes that are applied to figures. lubridate - A package that provides functions for handling various date-time formats. beepr - Allows the user to add audible notifications at the end of their scripts. mailR - Allows the user to send an email from R. rayshader - A package that can be used to create 3D figures. leaflet - A javascript library that allows R to create interactive maps. As of 5/23/2022, there are 18,586 packages hosted on the Comprehensive R Archive Network (CRAN). If theres something youre interested in doing with data, even if its oddly specific, its very likely that there is package that can help you. 1.1.1.2 Online Support R users contribute to the community in ways other than package development. There is a wealth of content online that has been created to help users learn new R skills and solve R related questions. Stack overflow is a public forum where users ask and answer coding questions about multiple coding languages. The R community is active here, which makes it a great resource when youre stuck. Odds are someone else has run into the problem youre having, or at least something similar. There are a number of other resources online developed by the R user community. In my experience, the best way to find answers is to google a question using thoughtful keywords, work through the google results, and then rephrase the search as necessary. The abundance of online resources and R contributors makes R very accessible. Its entirely possible to learn R through google because of these resources. The accessibility, availability of materials, and other contributions by the community are some of the important reasons why organizations and individuals choose to use R. 1.1.2 Automation and Reproducibility Making a business case for R is straightforward: the software is free, it can reduce human-error, and it can lead to significant time savings by allowing users to automate repetitive processes. R also provides a platform for making these processes easily reproducible. Taking the time to translate processes into R code can preserve institutional knowledge, facilitate QA/QC, and it promotes transparency. As an example of process automation, imagine that you have a weekly database query that is used to track the results of sampling at a specific site. You may currently query the data, export these data to excel, create a figure in excel, and then copy and paste the figure into a Word document. This process requires a time commitment every week and introduces the potential for error as you select data to be included in the figure. After the initial investment of time to write the code, this routine can be automated using R Markdown (more on that later) and executed as needed. An R-based approach would limit human error in this scenario because there would be no manual selection of data. The simple example below illustrates this principle. Imagine you have a data set with NA values that youd like to remove. The first line of code specifies the individual rows where there are NA values that youd like to remove from the dataset. This is analogous to excluding cells from a selection in excel. The second line of code uses the function drop_na() to remove rows with NA values from the iris dataset. Using the second approach minimizes the potential for errors because there is no manual selection of rows, which is especially important as the dataset grows and new NA values may be introduced. iris[-c(1,5,6,18,20),] #Excel analogue iris %&gt;% drop_na() #Using a function to make code dynamic in R 1.1.3 R Products The term products is used here to describe figures, tables, markdown documents, and shiny applications. These are among the most common products that users create with R, and their utility is outlined below. 1.1.3.1 Graphics Libraries There are a number of packages that allow users to create compelling data visualizations. Two of the more commonly used packages are ggplot2 and plotly .Ggplot2 offers over 40 different geometries (e.g., bar, line, point, histogram, text, etc.) that can be mapped to a figure. There are numerous aesthetic configurations such as color, fill, weight, transparency. There are even extension packages that expand ggplot2. Plotly is another highly configurable package used for creating figures, and it allows the user to make interactive data visualizations. Below are examples of figures using ggplot2, a ggplot2 extension package (ggridges), and plotly. These examples only hint at the degree to which users can configure their visualizations. Take a look at the R Graph Gallery or Plotlys graphing library to get a better sense of what is possible. library(ggplot2) Plot&lt;-ggplot(data=iris)+ geom_point(aes(x=Sepal.Length, y=Sepal.Width, color=Species))+ scale_color_manual(values=c(&#39;#00CC96&#39;,&#39;#EF553B&#39;, &#39;#636EFA&#39;))+ labs(x=&quot;Sepal Length (cm)&quot;, y=&quot;Sepal Width (cm)&quot;)+ theme_bw() Plot+facet_grid(rows=vars(Species))+theme(strip.text = element_blank()) library(ggplot2) library(ggridges) ggplot(data=iris, aes(x=Sepal.Length,y=Species, fill=Species))+ labs(x=&quot;Sepal Length (cm)&quot;, y=&quot;Species&quot;)+geom_density_ridges(alpha=0.7)+ scale_fill_manual(values=c(&#39;#00CC96&#39;,&#39;#EF553B&#39;, &#39;#636EFA&#39;), guide=&#39;none&#39;)+ theme_classic() library(plotly) plot_ly(data=iris, x=~Sepal.Length, y=~Sepal.Width, color=~Species)%&gt;% layout(xaxis = list(title = &#39;Sepal Length (cm)&#39;), yaxis = list(title = &#39;Sepal Width (cm)&#39;)) ## Warning: `arrange_()` is deprecated as of dplyr 0.7.0. ## Please use `arrange()` instead. ## See vignette(&#39;programming&#39;) for more help ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. Figure 1.1: Plotly figure of the sepal width ~ sepal length relationship used to ilustrate the interactive functionality of the plotly package. Note how the plotly figure is interactive. You can zoom, hover, select, and pan within the figure and you can also toggle layers on and off. Interactive figures are frequently used in the creation of other R products such as shiny applications and markdown documents, which are discussed below. The degree of control that these packages give in the configuration of data visualizations is yet another reason why users choose R. 1.1.3.1.1 Tables in R Like figures, there are a number of packages that provide different approaches to creating tables. These packages have functional overlaps, but youll find that each package has its own general aesthetic, strengths, and weaknesses. Below there are examples from DT, reactable, and kableExtra. This small exhibit of tables hints at the range of possibilities, from a classic report table to the interactive tables seen online. For more examples, take a look at the RStudio table gallery. library(DT) ## Warning: package &#39;DT&#39; was built under R version 3.6.3 sample_n(iris, 10)%&gt;% arrange(Species)%&gt;% datatable(rownames = FALSE) Figure 1.2: Table created using the DT package. library(reactable) library(reactablefmtr) data&lt;-sample_n(iris, 10) reactable( data, defaultSorted = &#39;Sepal.Length&#39;, defaultSortOrder = &#39;desc&#39;, columns = list( Sepal.Length = colDef( style = color_scales(data)), Petal.Width= colDef(cell = data_bars(data, text_position = &quot;above&quot;, align_bars = &#39;right&#39;, round_edges = TRUE)))) library(kableExtra) sample_n(iris, 10)%&gt;% arrange(Species)%&gt;% kbl()%&gt;% kable_classic(full_width = F, html_font = &quot;Cambria&quot;) 1.1.3.2 Markdown Documents R offers users the ability to integrate their code within documents (Word, PDF, HTML) and presentations (Powerpoint, PDF, HTML) using R Markdown. The bookdown package allows users to compile markdown files into a book format, with specific chapters (like this encyclopedia!). The ability to integrate code into your report or presentation is a major advantage of R Markdown over traditional tools. Youve seen examples of this code integration earlier in the chapter. Youve also seen the code associated with those figures and tables (which can be hidden in the output if desired). After writing the code and supporting text in a markdown document, all that you need to do to produce the document or presentation is compile the code, which is as simple as clicking a button. One of the greatest benefits of using R Markdown is that every time you create your report, your figures and tables will update to include any new data from the input dataset(s).This approach also limits the need for manually reformatting documents when you add or remove figures and tables. Check out the RStudio R Markdown gallery for examples of these documents. 1.1.3.3 Shiny Applications Shiny is a framework that allows R users to develop custom interactive web applications. This platform has made the creation of applications accessible, without the need to know HTML, CSS, Java, etc. The defining feature of Shiny is reactivity, which allows the developer to create user interface (UI) input elements that communicate with figures and tables in the application. These figures and tables have the ability to react based on the user inputs. As an example, the developer could add a date range input, which then subsets the data included in a time-series figure that is displayed in the UI. The use cases for shiny applications are seemingly endless, and this is illustrated by the RStudio Shiny application gallery. DEQ hosts a shiny server and has developed a number of applications and markdown documents that support water quality and biological monitoring, water quality assessment, and permitting. These tools have streamlined and standardized processes, saving staff time and improving our ability to communicate results. 1.1.4 Summary R is a free, and open source tool that has been made accessible by the R community through the creation of packages, engagement in problem-solving forums, and sharing of learning materials. R gives users the opportunity to work efficiently on tasks ranging from data analysis to report writing and application development. These R products are easily configurable and provide a broad range of options for customization. Leveraging these tools can lead to time savings, greater consistency, reduction of errors, knowledge transfer, and transparency - among other benefits. Hopefully weve convinced you that using R may be worth your time. If so, please proceed to the next chapter, where well show you how to get started! "],["gettingStartedWithR.html", "1.2 Getting Started With R", " 1.2 Getting Started With R Section Contact: Joe Famularo (joseph.famularo@deq.virginia.gov) Welcome! Were glad youre interested in using R in your role at DEQ. Below there are a series of instructions that will help guide you through the process for downloading and configuring R and RStudio. Feel free to reach out to Joe with any questions (joseph.famularo@deq.virginia.gov). 1.2.1 Downloading R and RStudio Download Base R. This is the software that contains the R programming language. Once you have Base R installed, download RStudio. Executing the installation for RStudio will require elevation (see step 3). RStudio is the standard integrated development environment (IDE) that is used for writing R code, but you can use it to write in different languages (e.g., python, SQL). RStudio also allows you to connect to databases (e.g., ODS), visualize your data and working environment, and publish R products (e.g., excel files, plots, markdown documents, shiny applications, etc.). After downloading the RStudio executable, submit a VCCC ticket. This will get the ball rolling on resolving the elevation requirement for the installation. 1.2.2 Configuring RStudio After installing R and RStudio, its a best practice to configure your settings. Begin by verifying the version of RStudio that you are using. Open RStudio. Navigate to Tools &gt; Global Options &gt; General &gt; Change Select [64-bit] option located in Documents sub-directory. Click OK until you are out of the Global Options menu. Restart R. Set the CRAN repository. CRAN is the repository that stores R packages and documentation. There are CRAN mirrors around the world. We will be selecting the one located at Oak Ridge National Lab. Open RStudio. Navigate to Tools &gt; Global Options &gt; Packages &gt; Change Select USA (TN) [https] - National Institute for Computational Sciences, Oak Ridge, TN. Select Apply. Click OK. Set your IDE appearance. This gives you the option to personalize RStudio, which can be helpful if youre spending hours writing or reviewing code: Navigate to Tools &gt; Global Options &gt; Appearance &gt; Editor theme. 1.2.3 Downloading Packages Like most operations in R, there are multiple approaches to downloading packages. Call the function install.packages()in your console. Packages can be installed using the Packages tab. Click on the Packages tab. Select Install. Type package name(s). Click Install. We recommend downloading the following packages to get started. Tidyverse tmap sp sf dataRetrevial 1.2.4 How to Learn (or Continue Learning) R? Simply typing free beginner R tutorial into Google produces an overwhelming number of results in the form of online courses, blogs, university lecture notes, and so much more. DEQ periodically offers an Introduction to R Programming Course (email Kevin Vaughan for information on the next course offering). We also host course materials from previous course offerings on the R server if you want to dive in right now. There are a number of additional free online resources we can recommend when you are just getting started. Datacamp Introduction to R Datacarpentry Intro to R Udemy R Basics - R Programming Language Introduction R for Data Science RStudio Primers Once you are comfortable with the basics, the best thing you can do is to try to get involved in your own project involving R. Maybe that is reprogramming an Excel spreadsheet process with R. Maybe it is taking one step out of a much larger process and trying to complete that with R. Maybe it is just trying to make a pretty plot in R. The point is, if you dont use it you lose it. Repeatedly practicing your skills is the key to further development and asking the question could I do this in R? is the gateway to strengthening your abilities. You can always consult chapters of this book for ideas on how other DEQ staff are using R in their roles for inspiration on how to incorporate it into your workflow. 1.2.4.1 But what if I get stuck? Great! That means you are trying. First, always first, see the Online Support section and reframe your question to Google (remember to always include R in your search term to specify solutions in this language). Then, ask a friend or colleague for help by building a reproducible example or reprex. Simply working through the process of trying to explain your problem often leads to finding your error. If it doesnt, then you have a tidy example to provide a friend when asking for assistance. As a last solution, you can post on a blog like Stack Overflow. This is not recommended unless you have exhausted your own research, exhausted your friends patience, and exhausted your search terms in Google. Chances are, someone else has already solved this or a very similar problem to yours, so the R community doesnt like multiple posts about the same questions. Always remember to never reveal sensitive information or data in any online spaces. "],["wantToContribute.html", "1.3 Want to Contribute?", " 1.3 Want to Contribute? We are always looking for collaborators. All reprex (reproducible examples) must be published in R markdown using the bookdown package and bookdown output style. Contributions are not limited to R as R markdown natively utilizes many languages including python and SQL. To get started with bookdown, see Yihui Xies technical references book bookdown: Authoring Books and Technical Documents with R Markdown or watch his webinar introducing bookdown. "],["queryInternalDataSources.html", "Chapter 2 Query Internal Data Sources", " Chapter 2 Query Internal Data Sources Accessing internal DEQ data sources has been a longtime goal of many R programmers. Connecting a local R instance to raw data allows for increased automation and reduces redundant data copies on local drives. However, querying data directly from internal resources into ones local environment is a privilege and requires a number of R skills and approval from OIS. Database operations rely on the SQL language. Users must first familiarize themselves with the basics of database operations with SQL. The DataCamp Introduction to SQL course is a good starting point. Additionally, the DataCarpentry SQL databases and R course provides is a good practice before seeking a direct, read-only connection to ODS (the SQLServer back end of CEDS). After you have demonstrated proficiency manipulating smaller, local data sources in R, you may submit a Special Network Access Form to OIS seeking access to ODS data view using your Microsoft credentials. After OIS approval, you may access ODS from R following the connection instructions in the Connect to ODS module. Alternatively, data pinned on the R server are available to all staff (on the internal network or VPN) without OIS authorization. Many pre-analyzed data sources are available to be pulled into your local R environment to assist with projects. See the Connecting to R Connect (for Pinned Data) module for more information on connecting your local environment to the R server to query these internal data resources. "],["connectToConnectPins.html", "2.1 Connecting to R Connect (for Pinned Data)", " 2.1 Connecting to R Connect (for Pinned Data) Section Contact: Emma Jones (emma.jones@deq.virginia.gov) Pinned data are powerful internal data sources that are developed by R programmers that expedite analyses by offering pre-analyzed data for others to use. The data products are often the end result of multiple analytical steps that one would need to repeat each time certain datasets are needed for analyses. Because these common datasets could prove useful for many end users, these datasets are pinned to the R server to expedite (and standardize) the acquisition of common data needs. Examples of pinned data are VSCI/VCPMI scores, station level geospatial data, station level Water Quality Standards (WQS) information, and many more. To access this data, you must first link your local R environment to the R Connect server. YOU MUST BE ON THE DEQ NETWORK OR VPN IN ORDER TO ACCESS ANY INTERNAL DATA RESOURCES Additionally, you must access pinned data using the pins library version 0.4.3. More recent versions of the pins package will not successfully connect to the version of pins on the R server. #install.packages(&quot;https://cran.r-project.org/src/contrib/Archive/pins/pins_0.4.3.tar.gz&quot;, repos=NULL, type=&quot;source&quot;) #install.packages(&quot;https://cran.r-project.org/src/contrib/filelock_1.0.2.tar.gz&quot;, repos=NULL, type=&quot;source&quot;) library(tidyverse) library(pins) library(config) Each time you wish to access pinned data, you must connect to the R server like you might connect to a database. To gain access to the R server, you need to have the API key information. Obtain this information by emailing Emma Jones (emma.jones@deq.virginia.gov) and specify you want access to pinned data on the R server. 2.1.1 API Keys To connect to the R server, you must use the appropriate API key. NEVER HARD CODE ACCESS CODES INTO YOUR SCRIPTS This means you should NEVER place the actual API key into your code ANYWHERE. You must source this information from a secret DEQconfig.yml file (obtained from Emma Jones and stored locally on your computer) in order to access data from the R server in your local R environment. Read more about config files. 2.1.2 Connect to the R server Use the following script to source the server API key and connect to the R server. # Server connection things conn &lt;- config::get(file = &quot;PINSconfig.yml&quot;, &quot;connectionSettings&quot;) # get configuration settings board_register_rsconnect(key = conn$CONNECT_API_KEY, server = conn$CONNECT_SERVER) 2.1.3 Browse Available Pins Once you are connected to the R server, you can view available pins and metadata. as_tibble(pin_find(board = &#39;rsconnect&#39;)) 2.1.4 Access a Pin To bring a particular pin into your local R environment, simply create an object and call the pin from the R server. totalHabitat &lt;- pin_get(&#39;ejones/totalHabitatScore&#39;, board = &#39;rsconnect&#39;) head(totalHabitat) ## # A tibble: 6 x 8 ## StationID `Collection Date` HabSampID `Field Team` `HabSample Comm~ Gradient ## &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2-LIJ003~ 2021-10-18 10:00:00 2-LIJ165~ bvw, rtt, j~ &lt;NA&gt; High ## 2 2-XUL000~ 2013-04-09 09:30:00 2-XUL GJD &lt;NA&gt; High ## 3 6BTHC000~ 2020-05-12 12:30:00 6BTHC159~ LLS &lt;NA&gt; High ## 4 6CLAE001~ 2020-06-04 12:30:00 6CLAE159~ LLS &lt;NA&gt; High ## 5 1AABR000~ 2012-10-23 14:30:00 ABR1577 rtt &lt;NA&gt; High ## 6 6AABR000~ 2021-04-07 14:45:00 ABR16496 LLS &lt;NA&gt; High ## # ... with 2 more variables: Season &lt;chr&gt;, `Total Habitat Score` &lt;dbl&gt; 2.1.5 Query (some of) a Pin Sometimes you do not want to bring an entire dataset into your environment. You can query just the information you want to bring back from a pinned data source by using simple dplyr verbs. pin_get(&#39;ejones/VSCIresults&#39;, board = &#39;rsconnect&#39;) %&gt;% # Query one station between a set date range filter(StationID == &#39;2-JKS023.61&#39; &amp; between(as.Date(`Collection Date`), as.Date(&#39;2015-01-01&#39;), as.Date(&#39;2020-12-31&#39;))) %&gt;% # only bring back rarified Samples filter(`Target Count` == 110) %&gt;% dplyr::select(StationID, `Collection Date`, everything()) %&gt;% datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) "],["connectToODS.html", "2.2 Connect to ODS", " 2.2 Connect to ODS Section Contact: Emma Jones (emma.jones@deq.virginia.gov) The ODS environment allows read-only access from your local R instance to the overnight copy of data stored in CEDS. The Oracle based CEDS environment is transferred to a SQLServer database each evening, ensuring data entered into CEDS is available for querying from ODS the next morning. 2.2.1 Credentials (Local vs Remote) Your Microsoft (MS) credentials are used for verifying you have access rights to this database. Details on acquiring access to this data source are available in the Query Internal Data Sources chapter. If you are building a report or application that relies on querying data from ODS at prescribed intervals, your personal MS credentials will not work when your data product is deployed to the R server. Instead, you must perform all data product testing locally with your personal MS credentials. Upon pushing your data product to the R server, you must switch the ODS credentials to the R servers unique credentials for the program to work on the remote server. To obtain the R servers ODS credentials, please contact Emma Jones (emma.jones@deq.virginia.gov) with specifics on exactly which areas of ODS your data product require access, how frequently your product hits ODS, example data retrieved from the environment, and general application purpose, use case, and audience information. 2.2.2 Required Packages To access ODS, you will need to use specific R packages that enable database access in addition to the recommended tidyverse package for general data management. The chunk below specifies which packages are required for the two methods for connecting to ODS overviewed in this report. The pool method requires the R packages pool and dbplyr while the DBI method requires the odbc and DBI R packages. Like all tasks completed in R, it is good be be aware that there are multiple methods to perform various operations, but in time users tend to prefer certain methods over others for their regular business practices. library(tidyverse) # &quot;pool method&quot; required packages library(pool) library(dbplyr) # &quot;DBI method&quot; required packages library(odbc) library(DBI) 2.2.2.1 ODSprod vs ODStest There are two ODS environments for you to be aware of: ODS (production or prod, DEQ-SQLODS-PROD) and ODStest (DEQ-SQLODS-TEST). The ODS environment is the production environment that offers data views as they are stored in CEDS. The ODStest environment is a testing environment (sandbox) that is used for testing data architecture and database functions and may not always contain data meant for querying for business applications. Which should you connect to? If you are querying data for business applications, reports, etc. you must ensure you are connected to the ODS production environment. If you are learning how to use databases, building test queries, or generally exploring, use the ODStest environment. It is the responsibility of each data analyst to know which data source they are connected to while querying data and the implications of data sources on resultant data. 2.2.3 DBI Method for ODS Connection The most basic way of connecting to a database uses the odbc and DBI R packages. library(odbc) library(DBI) We connect to a database by establishing a connection object with a very specific connection string. There are a few things to note in this connection string: The driver method used is the ODBC driver from the odbc R package. 2.Examples of drivers that work with ODS are SQL Server, ODBC Driver 18 for SQL Server, ODBC Driver 17 for SQL Server, ODBC Driver 11 for SQL Server, and SQL Server Native Client 11.0 see below for more information on drivers; however, we do not recommend using the SQL Server driver. The server argument specifies the server name (DEQ-SQLODS-PROD) and port (50000) that you are using for the connection The database argument specifies which database you wish to connect to (ODS) The trusted_connection argument must always be yes con &lt;- dbConnect(odbc::odbc(), .connection_string = &quot;driver={ODBC Driver 11 for SQL Server};server={DEQ-SQLODS-PROD,50000};database={ODS};trusted_connection=yes&quot;) The above script establishes a database connection named con in your environment. This connection will be passed into other query strings to query the database, requiring an open database connection each time you query the database. To test the connection, we will query an entire data view that is small from the WQM area of ODS. We use SQL to tell ODS to select everything from the Edas_Benthic_Master_Taxa_View data view. Note that we must provide the querying function the con connection string we established above. masterTaxaGenus &lt;- dbGetQuery(con, &quot;SELECT * FROM wqm.Edas_Benthic_Master_Taxa_View&quot;) head(masterTaxaGenus) ## # A tibble: 6 x 19 ## Phylum Class Subclass Order Suborder Superfamily Family Subfamily Tribe Genus ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Arthr~ Inse~ &lt;NA&gt; Dipt~ &lt;NA&gt; &lt;NA&gt; Culic~ &lt;NA&gt; &lt;NA&gt; Aedes ## 2 Arthr~ Inse~ &lt;NA&gt; Odon~ Anisopt~ &lt;NA&gt; Aeshn~ &lt;NA&gt; &lt;NA&gt; Aesh~ ## 3 Arthr~ Inse~ &lt;NA&gt; Cole~ &lt;NA&gt; &lt;NA&gt; Dytis~ Agabinae Agab~ Agab~ ## 4 Arthr~ Inse~ &lt;NA&gt; Tric~ &lt;NA&gt; &lt;NA&gt; Gloss~ Agapetin~ &lt;NA&gt; Agap~ ## 5 Arthr~ Inse~ &lt;NA&gt; Tric~ &lt;NA&gt; &lt;NA&gt; Seric~ &lt;NA&gt; &lt;NA&gt; Agar~ ## 6 Arthr~ Inse~ &lt;NA&gt; Plec~ &lt;NA&gt; &lt;NA&gt; Perli~ Perlinae Perl~ Agne~ ## # ... with 9 more variables: Species &lt;chr&gt;, `Final VA Family ID` &lt;chr&gt;, ## # FinalID &lt;chr&gt;, TolVal &lt;dbl&gt;, FFG &lt;chr&gt;, Habit &lt;chr&gt;, FamFFG &lt;chr&gt;, ## # FamTolVal &lt;dbl&gt;, FamHabit &lt;chr&gt; We can use this DBI method to send SQL statements directly to the database. SQL will not be covered in this section, please see DataCamp Introduction to SQL course for a basic primer on the SQL language. dbGetQuery(con, &quot;SELECT TOP 10 * FROM wqm.WQM_Stations_View&quot;) ## # A tibble: 10 x 103 ## Sta_Id Sta_Desc Sta_Cbp_Name Sta_Fic_County Sta_Fic_State Sta_Rec_Code ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2-WPK~ Winterp~ &lt;NA&gt; 041 51 Piedmont ## 2 2-WPK~ Winterp~ &lt;NA&gt; 041 51 Piedmont ## 3 2-XVE~ UT to W~ &lt;NA&gt; 041 51 Piedmont ## 4 2-WPK~ Winterp~ &lt;NA&gt; 041 51 Piedmont ## 5 2-WPK~ Winterp~ &lt;NA&gt; 041 51 Piedmont ## 6 2-XVF~ UT to W~ &lt;NA&gt; 041 51 Piedmont ## 7 2-WPK~ UT to W~ &lt;NA&gt; 041 51 Piedmont ## 8 2-XVG~ UT to W~ &lt;NA&gt; 041 51 Piedmont ## 9 2-WPK~ Winterp~ &lt;NA&gt; 041 51 Piedmont ## 10 2-XVJ~ UT to W~ &lt;NA&gt; 041 51 Piedmont ## # ... with 97 more variables: Admin_Region &lt;chr&gt;, Sta_Lat_Deg &lt;dbl&gt;, ## # Sta_Lat_Min &lt;dbl&gt;, Sta_Lat_Sec &lt;dbl&gt;, Sta_Long_Deg &lt;dbl&gt;, ## # Sta_Long_Min &lt;dbl&gt;, Sta_Long_Sec &lt;dbl&gt;, Sta_Lv1_Code &lt;chr&gt;, ## # Lv1_Description &lt;chr&gt;, Sta_Lv2_Code &lt;chr&gt;, Lv2_Description &lt;chr&gt;, ## # Sta_Lv3_Code &lt;chr&gt;, Lv3_Description &lt;chr&gt;, Sta_Lv4_Code &lt;chr&gt;, ## # Lv4_Description &lt;chr&gt;, Sta_Lv5_Code &lt;chr&gt;, Lv5_Description &lt;chr&gt;, ## # Sta_Stream_Section &lt;chr&gt;, Sta_Stream_Name &lt;chr&gt;, Sta_Huc_Code &lt;chr&gt;, ## # Sta_Huc_Sbc_Bsc_Code &lt;chr&gt;, Sta_Huc_Sbc_Code &lt;chr&gt;, Sta_Wsh_Code &lt;chr&gt;, ## # Sta_Toc_Map_Num &lt;chr&gt;, Sta_Sampling_Frequency &lt;chr&gt;, Sfc_Desc &lt;chr&gt;, ## # Sta_First_Sample_Date &lt;dttm&gt;, Sta_Last_Sample_Date &lt;dttm&gt;, ## # Sta_Size_Sampling_Area &lt;chr&gt;, Sta_Size_Sampling_Area_Unit &lt;chr&gt;, ## # Sau_Desc &lt;chr&gt;, Sta_Straher_Order &lt;chr&gt;, Sta_Shreve_Order &lt;chr&gt;, ## # Sta_Wadable &lt;chr&gt;, Sta_Tod_Code &lt;chr&gt;, Sta_Tbc_Code &lt;chr&gt;, ## # Sta_305B_Code &lt;chr&gt;, Sta_Water_Quality_Standards &lt;chr&gt;, Wqs_Desc &lt;chr&gt;, ## # Sta_Salinity_Area_Type &lt;chr&gt;, Sat_Desc &lt;chr&gt;, ## # Sta_Habitat_Descriptors &lt;chr&gt;, Sta_Comment &lt;chr&gt;, Sta_Storet_Bound &lt;chr&gt;, ## # Sta_Stored_Storet_Date &lt;dttm&gt;, Sta_Inserted_Date &lt;dttm&gt;, ## # Sta_Inserted_By &lt;chr&gt;, Sta_Changed_Date &lt;dttm&gt;, Sta_Changed_By &lt;chr&gt;, ## # Rep_Timestamp &lt;dttm&gt;, Sta_Str_Stream_Code &lt;chr&gt;, ## # Sta_Wqm_Wat_Shed_Code &lt;chr&gt;, Sta_Num_Of_Visits &lt;dbl&gt;, Sta_Type &lt;chr&gt;, ## # Sta_Old_Id &lt;chr&gt;, Sta_Fi_Permit_Id &lt;chr&gt;, Sta_Fi_Outfall_No &lt;chr&gt;, ## # Sta_Fi_Type &lt;chr&gt;, Sta_River_Mile &lt;chr&gt;, Sta_Milepoint_On &lt;chr&gt;, ## # Sta_His_Data_Revw_Flag &lt;chr&gt;, Sta_His_Data_Revw_Note &lt;chr&gt;, ## # Sta_Spa_Dist_Flag &lt;chr&gt;, Sta_Spa_Dist_Note &lt;chr&gt;, Sta_Spec_Prob_Flag &lt;chr&gt;, ## # Sta_Spec_Prob_Note &lt;chr&gt;, Sta_Maj_Tributary_Flag &lt;chr&gt;, ## # Sta_Maj_Tributary_Note &lt;chr&gt;, Sta_Maj_Fish_Flag &lt;chr&gt;, ## # Sta_Maj_Fish_Note &lt;chr&gt;, Sta_Ext_Recomm_Flag &lt;chr&gt;, ## # Sta_Ext_Recomm_Note &lt;chr&gt;, Sta_Source_Scale &lt;dbl&gt;, Sta_Hmv &lt;chr&gt;, ## # Sta_Hmuc &lt;chr&gt;, Sta_Hcm &lt;chr&gt;, Hcm_Short_Desc &lt;chr&gt;, Hcm_Full_Desc &lt;chr&gt;, ## # Sta_Hcrs &lt;chr&gt;, Hcrs_Short_Desc &lt;chr&gt;, Hcrs_Full_Desc &lt;chr&gt;, Sta_Vmv &lt;chr&gt;, ## # Sta_Vmuc &lt;chr&gt;, Sta_Vcm &lt;chr&gt;, Vcm_Desc &lt;chr&gt;, Sta_Vcrs &lt;chr&gt;, ## # Vcrs_Short_Desc &lt;chr&gt;, Vcrs_Full_Desc &lt;chr&gt;, Sta_Country &lt;chr&gt;, ## # Country_Desc &lt;chr&gt;, Sta_Mlt &lt;chr&gt;, Mlt_Description &lt;chr&gt;, ## # Sd_First_Sample_Date &lt;dttm&gt;, Sd_Last_Sample_Date &lt;dttm&gt;, ## # Sd_Num_Visits &lt;dbl&gt;, Loaded_Date &lt;dttm&gt;, Loaded_By &lt;chr&gt; 2.2.4 Pool Method for ODS Connection Generally speaking, the pool method is preferred for most business practices, especially any dynamic content (e.g. shiny apps). The pool package creates a connection to a specified database, even if the database connection is temporarily lost or idle for a period of time. The instability of connecting to ODS of VPN makes the pool package especially useful for DEQ business practices. The concepts of open database connections are important when interacting with databases, read more on this topic here. The pool method requires the pool and dbplyr R packages to be loaded into the user environment. library(pool) library(dbplyr) To connect to a database, you must establish a pool object. This object stores your connection information and may easily be piped into query requests. You may name your pool object anything you want, but most programmers name it pool by convention to ease the transfer of scripts from one project/programmer to the next. There are a few things to note in this connection string: The driver method used is the ODBC driver from the odbc R package. The specific Driver version is unique to your computer. You must input the exact name of the driver software and version you have installed on your machine. Examples of drivers that work with ODS are SQL Server, ODBC Driver 18 for SQL Server, ODBC Driver 17 for SQL Server, ODBC Driver 11 for SQL Server, and SQL Server Native Client 11.0 see below for more information on drivers; however, we do not recommend the using SQL Server driver. The Server argument specifies the server name and port (50000) you are using for the connection The dbname argument specifies which database you wish to connect to (ODS) The trusted_connection argument must always be yes pool &lt;- dbPool( drv = odbc::odbc(), # (1) Driver = &quot;ODBC Driver 11 for SQL Server&quot;, # (2) Server= &quot;DEQ-SQLODS-PROD,50000&quot;, # (3) dbname = &quot;ODS&quot;, # (4) #database = &quot;ODS&quot;, # (4 alternative if dbname does not work on your system) trusted_connection = &quot;yes&quot; # (5) ) After running the above chunk, you should have a list object named pool in your environment. We will use this object to build a minimal query to test our local connection to the ODS production environment. 2.2.4.1 More Information on Drivers You may or may not have database drivers already installed on your machine. To check, open the ODBC Data Source Administrator (64-bit) software. Under the Drivers tab, you should see all available drivers installed locally. We highly recommend you connect with an ODBC driver for optimum performance. Thus, if you do not have an ODBC driver already installed, you will need to install it. Please contact VCCC for assistance installing drivers. 2.2.4.1.1 Note on using the SQL Server Driver Users may successfully connect to ODS using the SQL Server driver and build elementary queries. However, more complicated queries that rely on externally scoped variables are not possible using the SQL Server driver. Thus, we highly recommend users only connect to ODS using an ODBC driver. To date, the following ODBC drivers allow users to connect to ODS successfully and perform more complicated queries: ODBC Driver 18 for SQL Server ODBC Driver 17 for SQL Server ODBC Driver 11 for SQL Server 2.2.4.2 Pool: dbname vs database Certain system configurations will not connect using the dbname argument inside the dbPool() function. If you have ensured you have access to the ODS environment you are trying to connect to, all the appropriate packages are installed and in your environment, and you still receive error messages citing SQL errors, try changing the dbname argument to database in your pool connection string like the 4 alternate solution above. 2.2.5 Pool Method: Query Data Once a pool object is created, you may test it by querying the database. The below example is a good test of querying an entire data view in ODS and bringing that information back into your local environment because the view is relatively small. If you successfully bring back the benthic macroinvertebrate master taxa list, then you have successfully connected to ODS. masterTaxaGenus &lt;- pool %&gt;% tbl(in_schema(&quot;wqm&quot;, &quot;Edas_Benthic_Master_Taxa_View&quot;)) %&gt;% as_tibble() head(masterTaxaGenus) ## # A tibble: 6 x 19 ## Phylum Class Subclass Order Suborder Superfamily Family Subfamily Tribe Genus ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Arthr~ Inse~ &lt;NA&gt; Dipt~ &lt;NA&gt; &lt;NA&gt; Culic~ &lt;NA&gt; &lt;NA&gt; Aedes ## 2 Arthr~ Inse~ &lt;NA&gt; Odon~ Anisopt~ &lt;NA&gt; Aeshn~ &lt;NA&gt; &lt;NA&gt; Aesh~ ## 3 Arthr~ Inse~ &lt;NA&gt; Cole~ &lt;NA&gt; &lt;NA&gt; Dytis~ Agabinae Agab~ Agab~ ## 4 Arthr~ Inse~ &lt;NA&gt; Tric~ &lt;NA&gt; &lt;NA&gt; Gloss~ Agapetin~ &lt;NA&gt; Agap~ ## 5 Arthr~ Inse~ &lt;NA&gt; Tric~ &lt;NA&gt; &lt;NA&gt; Seric~ &lt;NA&gt; &lt;NA&gt; Agar~ ## 6 Arthr~ Inse~ &lt;NA&gt; Plec~ &lt;NA&gt; &lt;NA&gt; Perli~ Perlinae Perl~ Agne~ ## # ... with 9 more variables: Species &lt;chr&gt;, `Final VA Family ID` &lt;chr&gt;, ## # FinalID &lt;chr&gt;, TolVal &lt;dbl&gt;, FFG &lt;chr&gt;, Habit &lt;chr&gt;, FamFFG &lt;chr&gt;, ## # FamTolVal &lt;dbl&gt;, FamHabit &lt;chr&gt; A few notes on querying using the pool method. The data schema are very important for ensuring the query runs successfully. In the above example, we are querying the data view entitled Edas_Benthic_Master_Taxa_View that lies inside the wqm area of ODS. Depending on your data needs (and information specified in your Special Network Access Form), you will be granted access to specific areas on the ODS environment. Water Quality Monitoring information is stored in the wqm area, Water Quality Assessment data is stored in the wqa area, TMDL data is stored in the TMDL area, etc. If you do not have access to the above area in ODS, then the above script will not work for you. The pool method allows for more complex queries to be piped into a single call. This is very powerful because it allows you to use simple dplyr verbs (through the dbplyr package) to force the database to perform these operations instead of bringing a lot of data into your environment for local filtering and manipulation operations. More on this concept will be covered in Querying Data from ODS but the importance of this cannot be understated. It is always more efficient to have the database perform querying operations to minimize the amount of data sent to your local R environment. "],["queryDataFromODS.html", "2.3 Querying Data from ODS", " 2.3 Querying Data from ODS Section Contact: Emma Jones (emma.jones@deq.virginia.gov) There are a few general concepts to be aware of when querying data from ODS. These include connecting and disconnecting from a database, understanding the underlying data schema, how data Views interact with one another, and data update frequencies. This module deals exclusively with data from the ODS production environment. Please see the Connect to ODS section for information about the test vs production environments. In an ideal world, one would practice querying data against a test environment and only query the production environment for official data retrievals. This is especially true when building automated reports or applications that could repeatedly tax the database with frequent large queries during the development process. However, due to frequency of data updates to the ODStest environment and potential schema differences, we do not encourage users to rely on data in this environment. 2.3.1 Connecting and Disconnecting from a Database It is imperative to understand that each time a user connects to a database it is that users responsibility to disconnect from the database once they have finished their database operations. Forgotten open database connections (or leaked connections) unnecessarily tax a database and are poor practice as programmers. Below are example connection and disconnection statements based on how a user may connect to a database. # Connect using the DBI library con &lt;- dbConnect(odbc::odbc(), .connection_string = &quot;driver={ODBC Driver 11 for SQL Server};server={DEQ-SQLODS-PROD,50000};database={ODS};trusted_connection=yes&quot;) # Disconnect using the DBI library dbDisconnect(con) The Connect to ODS article emphasizes the benefits of using the pool package to manage database connections. Besides being useful for piping and dplyr/dbplyr functionality, pool handles the closing of unused connections without requiring user input. This is very important. For more information on pooled connections, see the R pool package and read more generally about pooled database connections. 2.3.1.1 Standalone Reports/Applications that Query ODS When building any shiny application or report that queries data from ODS, it is best to use the pool package instead of opening and closing individual database connections with each query. It is also important to include a the following script to immediately close any pooled connections immediately when a user closes an application/report. Place the connection/disconnection scripts inside the global.R file of a multifile shiny app. library(pool) # open pool connection, best stored in global.R file pool &lt;- dbPool( drv = odbc::odbc(), Driver = &quot;ODBC Driver 11 for SQL Server&quot;,#&quot;SQL Server Native Client 11.0&quot;, Server= &quot;DEQ-SQLODS-PROD,50000&quot;, dbname = &quot;ODS&quot;, trusted_connection = &quot;yes&quot; ) # close pool connection, best stored in global.R file onStop(function() { poolClose(pool) }) 2.3.2 Data Schema ODS is divided into multiple program areas. These areas are turned on for certain users depending on their data needs. To request access to certain areas of ODS, you need to submit a Special Network Access Form to OIS seeking access to specific areas of ODS. Once access to one or more areas of ODS are granted, you may explore the available areas using code ( e.g. using the DBI package ) or interactively with a GUI ( e.g. using the built in GUI functionality in the RStudio IDE ). 2.3.2.1 Data Exporation: DBI After connecting to ODS using the config information (see Connect to ODS), the DBI::dbListTables() allows users to see what data views are available in certain areas of ODS. Depending on your data needs, you may have an extensive list of available tables. For presentation purposes, the dbListTables() results were piped into a head() call with an n argument of 20 to only show the top 20 results. library(DBI) con &lt;- dbConnect(odbc::odbc(), .connection_string = &quot;driver={ODBC Driver 11 for SQL Server};server={DEQ-SQLODS-PROD,50000};database={ODS};trusted_connection=yes&quot;) dbListTables(con) %&gt;% head(20) ## [1] &quot;WP_WATER_PERMITS_VIEW&quot; &quot;trace_xe_action_map&quot; ## [3] &quot;trace_xe_event_map&quot; &quot;Enf_Admin_Proceed_View&quot; ## [5] &quot;Enf_ECM_Docs_View&quot; &quot;Enf_enforcement_Cases_View&quot; ## [7] &quot;Enf_Facilities_View&quot; &quot;Enf_Respon_Parties_View&quot; ## [9] &quot;Enf_Settle_Details_View&quot; &quot;CHECK_CONSTRAINTS&quot; ## [11] &quot;COLUMN_DOMAIN_USAGE&quot; &quot;COLUMN_PRIVILEGES&quot; ## [13] &quot;COLUMNS&quot; &quot;CONSTRAINT_COLUMN_USAGE&quot; ## [15] &quot;CONSTRAINT_TABLE_USAGE&quot; &quot;DOMAIN_CONSTRAINTS&quot; ## [17] &quot;DOMAINS&quot; &quot;KEY_COLUMN_USAGE&quot; ## [19] &quot;PARAMETERS&quot; &quot;REFERENTIAL_CONSTRAINTS&quot; This method is good for identifying all data views available to a user with their current permissions, but it is not efficient for understanding the overall database structure, e.g. where the views referenced above exist in the ODS schema as a whole. 2.3.2.2 Data Exporation: RStudio Connections Pane The built in Connections pane is a better way to understand how the pieces of the database fit together and quickly view the top 1000 rows of a selected data view. To access this feature of RStudio, click the Connections tab in the Environment pane in your IDE. If you have already connected to the ODS environment using the DBI package, you will automatically see this as an available option. If not, see Connect to ODS for more information on making this connection for the first time. Click on the ODS database connection to show the connection information. Use the Connect drop down to choose how you want to connect to the database. For this example we will choose R Console but other options are helpful for different use cases. The available environments are displayed once the connection is made. This happens almost instantaneously. Click on the drop down arrow next to ODS to show the database structure and what areas you have access to. Your connection may not look identical to the example. Click on any of the areas to preview what data views are available in each area. You can click the drop down arrow next to each data view for details on the variables within the data view as well as each data format. Or you can click the name of the data view to open a preview of the data view (up to first 1,000 rows) in a tabular form in the Viewer Pane. Below we are looking at the first 1,000 rows of the Edas_Benthic_Master_Taxa_View. It is important to tell R in your data query not only which data view you are querying but also the area of ODS the data view lives in. Using the pool and dbplyr packages this is very straightforward. Note the in_schema() nested function inside the tbl() function in the example below. library(tidyverse) library(pool) library(dbplyr) ## Connect to ODS production pool &lt;- dbPool( drv = odbc::odbc(), Driver = &quot;ODBC Driver 11 for SQL Server&quot;,#&quot;SQL Server Native Client 11.0&quot;, Server= &quot;DEQ-SQLODS-PROD,50000&quot;, dbname = &quot;ODS&quot;, trusted_connection = &quot;yes&quot; ) # Query the entire Edas_Benthic_Master_Taxa_View data view from the wqm area of ODS masterTaxaGenus &lt;- pool %&gt;% tbl(in_schema(&quot;wqm&quot;, &quot;Edas_Benthic_Master_Taxa_View&quot;)) %&gt;% as_tibble() # preview the top 6 rows of the data pulled back from ODS head(masterTaxaGenus) ## # A tibble: 6 x 19 ## Phylum Class Subclass Order Suborder Superfamily Family Subfamily Tribe Genus ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Arthr~ Inse~ &lt;NA&gt; Dipt~ &lt;NA&gt; &lt;NA&gt; Culic~ &lt;NA&gt; &lt;NA&gt; Aedes ## 2 Arthr~ Inse~ &lt;NA&gt; Odon~ Anisopt~ &lt;NA&gt; Aeshn~ &lt;NA&gt; &lt;NA&gt; Aesh~ ## 3 Arthr~ Inse~ &lt;NA&gt; Cole~ &lt;NA&gt; &lt;NA&gt; Dytis~ Agabinae Agab~ Agab~ ## 4 Arthr~ Inse~ &lt;NA&gt; Tric~ &lt;NA&gt; &lt;NA&gt; Gloss~ Agapetin~ &lt;NA&gt; Agap~ ## 5 Arthr~ Inse~ &lt;NA&gt; Tric~ &lt;NA&gt; &lt;NA&gt; Seric~ &lt;NA&gt; &lt;NA&gt; Agar~ ## 6 Arthr~ Inse~ &lt;NA&gt; Plec~ &lt;NA&gt; &lt;NA&gt; Perli~ Perlinae Perl~ Agne~ ## # ... with 9 more variables: Species &lt;chr&gt;, `Final VA Family ID` &lt;chr&gt;, ## # FinalID &lt;chr&gt;, TolVal &lt;dbl&gt;, FFG &lt;chr&gt;, Habit &lt;chr&gt;, FamFFG &lt;chr&gt;, ## # FamTolVal &lt;dbl&gt;, FamHabit &lt;chr&gt; As emphasized before, it is prudent to close all database connections established using DBI (the connection method the RStudio Connections pane utilizes). To do this using the GUI, click the Disconnect From A Connection button highlighted in red in the picture below. 2.3.3 Data Views Interaction Another component of any database schema involves how data can be efficiently combined from the various areas of the database. Generally speaking, the way we use data locally in spreadsheets is not how a database stores said data. It is the responsibility of the data query-er to link the datasets they need to make it usable. A full tour of each area of ODS is beyond the scope of these articles, but short overviews of specific ODS areas are discussed in subsequent articles. These areas include: Common WQM queries Common Benthic queries Common WQA queries Common TMDL queries 2.3.4 Data Update Frequency ODS production is update nightly reflecting any data entered into CEDS the evening before. If you need data immediately after it is entered into CEDS, then CEDS is the only location users can view the data. ODStest is updated infrequently and should not be used for official data queries. "],["commonWQMqueries.html", "2.4 Common WQM queries", " 2.4 Common WQM queries Section Contact: Emma Jones (emma.jones@deq.virginia.gov) This section will highlight a few common queries one might want to perform to acquire water quality monitoring data from CEDS data. This article assumes the user had local rights to query data from the WQM area of ODS and pinned data. See the Connect to ODS and Connecting to R Connect (for Pinned Data) modules for more information on connecting your local environment to the these data repositories. 2.4.1 Connect to Internal Data Resources Before we can query data from any of the below examples, we need to set up our connections to ODS and the pinned data resources on the R Connect service. Your connection strings may differ from the example below due to local system differences. library(tidyverse) # libraries for ODS library(pool) library(dbplyr) # libraries for pins library(pins) library(config) # set up connection to ODS production pool &lt;- dbPool( drv = odbc::odbc(), Driver = &quot;ODBC Driver 11 for SQL Server&quot;, Server= &quot;DEQ-SQLODS-PROD,50000&quot;, dbname = &quot;ODS&quot;, trusted_connection = &quot;yes&quot; ) # connect to R Connect for pinned data conn &lt;- config::get(file = &quot;PINSconfig.yml&quot;, &quot;connectionSettings&quot;) # get configuration settings board_register_rsconnect(key = conn$CONNECT_API_KEY, server = conn$CONNECT_SERVER) 2.4.2 Query All Water Monitoring Data from a Provided Watershed We can use a combination of geospatial and querying skills to retrieve all the water monitoring information from a given watershed. Detailed instructions on automated watershed delineation is available in the Watershed Delineation section. We will sample that method for this tutorial. For this example, we will look for all water monitoring data from the watershed that drains to the Occoquan Reservoir. First, we need to scrape USGS StreamStats for this watershed. The point we will use as our pour point is the dam (38.694394085503646, -77.27644004865934). First, we need to bring in the packages and scripts to perform the spatial and web scraping operations of to delineate our watershed. library(sf) # for spatial analysis library(leaflet) # for interactive mapping library(inlmisc) # for interactive mapping on top of leaflet source(&#39;StreamStatsAutoDelineation.R&#39;) # for custom web scraping tool to hit USGS StreamStats API # delineate watershed stationDelineation &lt;- streamStats_Delineation(state= &#39;VA&#39;, longitude = -77.27644004865934, latitude = 38.694394085503646, UID = &#39;Occoquan&#39;) # convert to a more useable format stationDelineationWatershed &lt;- stationDelineation$polygon %&gt;% reduce(rbind) %&gt;% arrange(UID) stationDelineationPoint &lt;- stationDelineation$point %&gt;% reduce(rbind) %&gt;% arrange(UID) Next lets make sure the watershed retrieved makes sense for our goals. CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data= stationDelineationWatershed, color = &#39;black&#39;, weight = 1, fillColor=&#39;blue&#39;, fillOpacity = 0.4,stroke=0.1, group=&quot;Watershed&quot;, popup=leafpop::popupTable(stationDelineationWatershed, zcol=c(&#39;UID&#39;))) %&gt;% addCircleMarkers(data = stationDelineationPoint, color=&#39;orange&#39;, fillColor=&#39;black&#39;, radius = 5, fillOpacity = 1, opacity=1,weight = 1,stroke=T, group=&quot;Station&quot;, label = ~UID, popup=leafpop::popupTable(stationDelineationPoint)) %&gt;% addLayersControl(baseGroups=c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), overlayGroups = c(&#39;Station&#39;,&#39;Watershed&#39;), options=layersControlOptions(collapsed=T), position=&#39;topleft&#39;) Now lets leverage the spatial WQM station information on the R server to identify all the DEQ stations that fall in this polygon. There are multiple ways to spatially intersect polygon and point features. This example uses the sf::st_join() function to spatially join all overlapping geometries and then filters on the successfully joined rows. We will also plot the points to make sure the spatial join performed as we wanted. wqmStations &lt;- pin_get(&#39;ejones/WQM-Stations-Spatial&#39;, board = &#39;rsconnect&#39;) %&gt;% # Immediately transform this tabular data into a spatial object st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) # Identify which stations fall into the watershed with a spatial join occoquanStations &lt;- st_join(wqmStations, stationDelineationWatershed) %&gt;% filter(UID == &#39;Occoquan&#39;) # Plot the points to make sure we retrieved what we wanted CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data= stationDelineationWatershed, color = &#39;black&#39;, weight = 1, fillColor=&#39;blue&#39;, fillOpacity = 0.4,stroke=0.1, group=&quot;Watershed&quot;, popup=leafpop::popupTable(stationDelineationWatershed, zcol=c(&#39;UID&#39;))) %&gt;% addCircleMarkers(data = occoquanStations, color=&#39;orange&#39;, fillColor=&#39;black&#39;, radius = 5, fillOpacity = 1, opacity=1,weight = 1,stroke=T, group=&quot;Station&quot;, label = ~StationID, popup=leafpop::popupTable(occoquanStations)) %&gt;% addLayersControl(baseGroups=c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), overlayGroups = c(&#39;Station&#39;,&#39;Watershed&#39;), options=layersControlOptions(collapsed=T), position=&#39;topleft&#39;) So there are 231 stations in CEDS that have been sampled in the desired area. Lets go pull the monitoring data from these sites now. We are interested in all available sampling data, but one could limit the sampling events to a certain period of time if desired. The query below will only return field data from the selected sites. Note we need to use the !! operator (!! is pronounced bang bang) in order to send the desired StationIDs from the occoquanStations object to the ODS query. fieldData &lt;- pool %&gt;% tbl(in_schema(&quot;wqm&quot;, &quot;Wqm_Field_Data_View&quot;)) %&gt;% filter(Fdt_Sta_Id %in% !! occoquanStations$StationID) %&gt;% as_tibble() # preview data retrieved head(fieldData) %&gt;% DT::datatable( rownames = F, options = list(scrollX = T)) To acquire analyte data from DCLS, we need to use the unique Ana_Sam_Fdt_Id code from the fieldData object. We will join the StationID and sample date information to this object so we can better understand the data returned. Again, note the use of the !! to call the Fdt_Id field from the fieldData object for our query. analyteData &lt;- pool %&gt;% tbl(in_schema(&quot;wqm&quot;, &quot;Wqm_Analytes_View&quot;)) %&gt;% filter(Ana_Sam_Fdt_Id %in% !! fieldData$Fdt_Id) %&gt;% as_tibble() %&gt;% left_join(dplyr::select(fieldData, Fdt_Id, Fdt_Sta_Id, Fdt_Date_Time), by = c(&quot;Ana_Sam_Fdt_Id&quot; = &quot;Fdt_Id&quot;)) # preview data retrieved head(analyteData) %&gt;% DT::datatable( rownames = F, options = list(scrollX = T)) So we have the data for the desired sites, but the data formats dont exactly match how we typically work with monitoring data at DEQ. The field data are in a wide format and the analyte data are in a long format. Lets use a standardized data wrangling process to return the data in a combined, wide format with one row per sampling event. To do so, we will use the conventionals data format and a custom script that does all the data manipulation and cleaning for us. We will source that script locally (you can download and review the script here). We also need a few more pieces of data to make this script work, so we will pull that data from ODS before calling the function that smashes all the data together, known as conventionalsSummary(). source(&#39;conventionalsFunction02032022.R&#39;) # Basic station info from CEDS required for function stationsInfo &lt;- pool %&gt;% tbl(in_schema(&quot;wqm&quot;, &quot;Wqm_Stations_View&quot;)) %&gt;% filter(Sta_Id %in% !! occoquanStations$StationID) %&gt;% as_tibble() stationsGIS_View &lt;- pool %&gt;% tbl(in_schema(&quot;wqm&quot;, &quot;Wqm_Sta_GIS_View&quot;)) %&gt;% filter(Station_Id %in% !! occoquanStations$StationID) %&gt;% as_tibble() # conventionals-like dataset for standardized data consolidation organizedData &lt;- conventionalsSummary(conventionals = pin_get(&quot;conventionals2022IRfinalWithSecchi&quot;, board = &quot;rsconnect&quot;)[0,], stationFieldDataUserFilter = fieldData, stationAnalyteDataUserFilter = analyteData, stationInfo = stationsInfo, stationGIS_View = stationsGIS_View, dropCodes = c(&#39;QF&#39;), # lab codes we want to drop assessmentUse = F, overwriteUncensoredZeros= T) Once the conventionalsSummary() function completes the data manipulation, we can investigate the list object it returns. Should you only want parameters consistent with the Water Quality Assessment conventionals dataset, use the organizedData$Conventionals object. Should you want to explore all parameters sampled, use the organizedData$More object. head(organizedData$Conventionals) %&gt;% DT::datatable( rownames = F, options = list(scrollX = T)) head(organizedData$More) %&gt;% DT::datatable( rownames = F, options = list(scrollX = T)) We can also use this information to pull all the benthic macroinvertebrate data from this watershed as well. Using the StationID information from the occoquanStations object, we can pull pre-analyzed Stream Condition Index (SCI) scores from the R Connect server pinned data. Benthic data are calculated using all three SCI methods for application rendering purposes, but users should only use the appropriate SCI method based on the correct Level III Ecoregion, Basin, and/or speaking with their regional biologist. We will pull all benthic information statewide for each SCI method, filter these results by the desired StationIDs and ecoregion/basin information, and then combine them all into a single result for sharing. Again, it is highly recommended that you speak with your Regional Biologist prior to doing further analyses with the default SCI metrics and results to ensure the suggested SCI method is appropriate for each site. # pull in all SCI info for all stations VSCIresults &lt;- pin_get(&quot;ejones/VSCIresults&quot;, board = &quot;rsconnect&quot;) VCPMI63results &lt;- pin_get(&quot;ejones/VCPMI63results&quot;, board = &quot;rsconnect&quot;) VCPMI65results &lt;- pin_get(&quot;ejones/VCPMI65results&quot;, board = &quot;rsconnect&quot;) # identify which SCI to use SCI_filter &lt;- filter(VSCIresults, StationID %in% filter(occoquanStations, ! US_L3CODE %in% c(63,65))$StationID) %&gt;% bind_rows( filter(VCPMI63results, StationID %in% filter(occoquanStations, US_L3CODE %in% c(63) | str_detect(Basin, &quot;Chowan&quot;))$StationID) ) %&gt;% bind_rows( filter(VCPMI65results, StationID %in% filter(occoquanStations, US_L3CODE %in% c(65) &amp; !str_detect(Basin, &quot;Chowan&quot;))$StationID) ) %&gt;% left_join(dplyr::select(occoquanStations, StationID , Sta_Desc), by = c(&#39;StationID&#39;)) %&gt;% dplyr::select(StationID, Sta_Desc, BenSampID, `Collection Date`, RepNum, SCI, `SCI Score`, `SCI Threshold`,`Sample Comments`:Season, everything()) %&gt;% filter(`Target Count` == 110) # only bring back rarified samples head(SCI_filter) %&gt;% DT::datatable( rownames = F, options = list(scrollX = T)) Lastly, one can pull the associated raw benthic results as well as the qualitative Rapid Biological Protocol (RBP) habitat results from pinned data on the R server for the desired stations within the watershed of interest. benthics &lt;- pin_get(&quot;ejones/benthics&quot;, board = &quot;rsconnect&quot;) %&gt;% filter(StationID %in% occoquanStations$StationID) habSamps &lt;- pin_get(&quot;ejones/habSamps&quot;, board = &quot;rsconnect&quot;) %&gt;% filter(StationID %in% occoquanStations$StationID) habValues &lt;- pin_get(&quot;ejones/habValues&quot;, board = &quot;rsconnect&quot;)%&gt;% filter(HabSampID %in% habSamps$HabSampID) habObs &lt;- pin_get(&quot;ejones/habObs&quot;, board = &quot;rsconnect&quot;)%&gt;% filter(HabSampID %in% habSamps$HabSampID) Please see the Common Benthic Queries section for more detailed information on querying and manipulating benthic macroinvertebrate data. "],["commonBenthicqueries.html", "2.5 Common Benthic queries", " 2.5 Common Benthic queries Coming Soon! "],["commonWQAqueries.html", "2.6 Common WQA queries", " 2.6 Common WQA queries Coming Soon! "],["commonTMDLqueries.html", "2.7 Common TMDL queries", " 2.7 Common TMDL queries Coming Soon! "],["test-your-knowledge-data-science-learning-activity.html", "2.8 Test Your Knowledge: Data Science Learning Activity", " 2.8 Test Your Knowledge: Data Science Learning Activity Section Contact: Joe Famularo (joe.famularo@deq.virginia.gov) Often the best way to learn a coding language or database environment (or both!) is to just dig in and try to answer some questions. It can be difficult at first to create new questions to ask of a dataset if you are unfamiliar with what common questions might need to be asked. The R Team has pulled together some questions to challenge your data science problem solving abilities and allow you to explore multiple data resources using your R environment. In order to complete some of these activities, you will need access to ODS and pinned data on the R server. Please see these articles prior to attempting the following exercises: Connecting to R Connect (for Pinned Data) Connect to ODS One method to solve each activity is provided in the key. There can often be multiple ways to solve a problem, so dont be discouraged if your method does not follow the key methodology. Use the key as a learning tool to familiarize yourself with efficient problem solving skills. 2.8.1 Data Science Activity 2.8.1.1 DEQ Data Science Primer The goal of this chapter is to provide some exercises to help new users get oriented with the ODS, CEDS, and pinned data on the Connect server. For a refresher (or introduction) on how to connect to, and query, the ODS or the Connect server check out [Chapter 2] of the encyclopedia. 2.8.1.1.1 Challenge 1 Lets start simple. How many field data records (e.g., individual rows) are there in the ODS? The field data table houses quantitative and qualitative data that are captured in the field. These field data can be joined with associated analytical results (nutrients, metals, TSS, etc.), which are contained in WQM_Analytes_View. Whats the earliest record in the field data view? Head over to CEDS and see if you can find this field data record in the wqm field data module. Where was the station located? 2.8.1.1.2 Challenge 2 Next, lets make a figure displaying all of the temperature records for 2022. After you get done with that, lets take it a step further and refine the figure to include only the stations within the James River Basin. Hint: look for a table that includes the field Basin, find the key between the field data table and this other table, and join them by that key. 2.8.1.1.3 Challenge 3 Lets make a map to visualize the station-specific median concentration of arsenic samples collected in the James River basin since 2000. Hint: youll need to find the view that identifies samples by Analyte (not parameter group, which provides a code for groups of multiple analytes), find the date that those samples were collected, and then join the spatial data for those stations to allow you to map the median concentration. Try changing the parameter or basin youre visualizing. You could also use number of samples as an additional aesthetic to control point size. 2.8.1.1.4 Challenge 4 Weve spent our time in the wqm (water quality monitoring) ODS module so far. Lets take a look at the wqa (water quality assessment) module. Note that there are 2 groups of views in the wqa module, Tableau views and Logi views. The Tableau views include spaces in their names, and the Logi views do not (e.g., WQA AU History ~ Tableau ; WQA_AU_History_View ~ Logi). The tableau views are more condensed, meaning in some cases they have list fields that are contained in seperate tables in the Logi views. The tableau views are expected to fully replace the Logi views, but for now both exist to support the assessment as this transition occurs. The same transition is expected to occur with WQM, but there are not currently Tableau views in the wqm module. Now, lets go ahead and pull the stations associated with assessment units associated with the Lynnhaven River. Well want to make a map of enterococci exceedances that occurred at these stations during the 2022 assessment cycle. Hint: youll want to search for Lynnhaven within a field containing water name strings to narrow down the assessment units that are returned to you. Using stringr, a tidyverse package, is not possible before you convert your query into a tibble(). This is a limitation of dbplyrs ability to translate tidyverse syntax into SQL statements. You could return the whole table and filter after converting the query results to a tibble, but this has an outsized impact on performance since you are requesting an entire table from the ODS. Alternatively, you can use the SQL term %like% %string% in a filter statement before converting to tibble and dbplyr will be able to handle this. As an example: filter(colname %like% %Lynnhaven%). Once you wrap up with the figure above, take a look at the assessment module in CEDS and see how closely you can replicate this query. 2.8.1.1.5 Challenge 5 Sometimes DEQ collects data that do not yet have a home within the ODS. The PFAS data collected within the past several years is a good example of this. This condition is also true with fish metals and fish PCB data. The connect server offers a solution to store these data until the ODS environment can house them. Of equal or greater importance, we use the Connect server to store curated data views from the ODS to support applications and end users. For this challenge, make a table that displays the number of PFAS surface water samples and the median Total PFAS concentration of those samples within each HUC8 watershed. Change this up and look at number of samples and the median concentration by sampling program. 2.8.1.1.6 Challenge 6 Pinned data on the R server is useful to incorporate into your workflow as pre analyzed data that can expedite analyses. Benthic data fall into this category. Raw benthic macroinvertebrate data (what bugs where collected where, when, and how many) are stored in CEDS, but to understand the ecological implications of this data, DEQ evaluates the raw data using stream condition indicies (SCI). Pulling the raw benthic data, running the appropriate SCI analysis, and then applying these results to answer a larger question can be burdensome, so the R Biological Team makes the analyzed SCI scores available to anyone on the DEQ network. Lets work with high gradient sites in the Southwest office and see if there are any relationships between benthic SCI scores (using the VSCI) and Specific Conductivity collected at these sites. Pinned benthic VSCI scores can be found on the R server as the pin named ejones/VSCIresults. 2.8.2 Data Science Activity Key library(tidyverse) library(pins) library(pool) library(dbplyr) library(leaflet) con&lt;-config::get(file=&quot;config.yml&quot;, &quot;connectionSettings&quot;) pins::board_register_rsconnect(key = con$CONNECT_API_KEY, server = con$CONNECT_SERVER) pool &lt;- dbPool( drv = odbc::odbc(), # (1) Driver = &quot;ODBC Driver 11 for SQL Server&quot;, # (2) Server= &quot;DEQ-SQLODS-PROD,50000&quot;, # (3) database = &quot;ODS&quot;, # (4) trusted_connection = &quot;yes&quot; # (5) ) 2.8.2.1 Challenge 1 #Total number of records in the Field Data view pool%&gt;% tbl(in_schema(&quot;wqm&quot;, &quot;Wqm_Field_Data_View&quot;))%&gt;% as_tibble()%&gt;% nrow() #Oldest record in the Field Data view pool%&gt;% tbl(in_schema(&quot;wqm&quot;, &quot;Wqm_Field_Data_View&quot;))%&gt;% filter(Fdt_Date_Time == min(Fdt_Date_Time))%&gt;% as_tibble()%&gt;% pull(Fdt_Date_Time) 2.8.2.2 Challenge 2 #Pull temp data for 2022 Temp_2022&lt;-pool%&gt;% tbl(in_schema(&quot;wqm&quot;, &quot;Wqm_Field_Data_View&quot;))%&gt;% filter(year(Fdt_Date_Time) == &quot;2022&quot;)%&gt;% mutate(Year = year(Fdt_Date_Time))%&gt;% select(Fdt_Date_Time, Fdt_Sta_Id, Fdt_Temp_Celcius, Year)%&gt;% as_tibble() #Plot 2022 water temperatures Temp_2022%&gt;% ggplot()+ geom_point(aes(Fdt_Date_Time, Fdt_Temp_Celcius))+ theme_classic() #Refine figure to include only stations within the James River Basin tempJamesRiver2022&lt;-Temp_2022%&gt;% left_join(., pool%&gt;%tbl(in_schema(&quot;wqm&quot;, &quot;WQM_Sta_GIS_View&quot;))%&gt;%as_tibble(), by= c(&quot;Fdt_Sta_Id&quot; = &quot;Station_Id&quot;))%&gt;% filter(Basin == &quot;James River Basin&quot;) tempJamesRiver2022%&gt;% ggplot()+ geom_point(aes(Fdt_Date_Time, Fdt_Temp_Celcius))+ theme_classic() 2.8.2.3 Challenge 3 arsenicSamples&lt;-pool%&gt;% tbl(in_schema(&quot;wqm&quot;, &quot;Wqm_Analytes_View&quot;))%&gt;% filter(Pg_Parm_Short_Name == &quot;AS&quot;)%&gt;% left_join(., pool%&gt;%tbl(in_schema(&quot;wqm&quot;, &quot;Wqm_Field_Data_View&quot;)), by = c(&quot;Ana_Sam_Fdt_Id&quot; = &quot;Fdt_Id&quot;))%&gt;% #Join with field data view to get station IDs left_join(., pool%&gt;%tbl(in_schema(&quot;wqm&quot;, &quot;WQM_Sta_GIS_View&quot;)), by= c(&quot;Fdt_Sta_Id&quot; = &quot;Station_Id&quot;))%&gt;% #capture the lat long for mapping filter(Basin == &quot;James River Basin&quot;)%&gt;% as_tibble()%&gt;% drop_na(Ana_Value)%&gt;% mutate(Year = year(Fdt_Date_Time))%&gt;% filter(Year &gt;= &quot;2000&quot;)%&gt;% group_by(Fdt_Sta_Id, Latitude, Longitude)%&gt;% summarise(Median = median(Ana_Value), n_samples = n()) #use radius = ~n_samples below to use point size as indicator of n pal&lt;-colorNumeric(palette = c(&quot;#D1DDE2FF&quot;, &quot;#96ABC6FF&quot;, &quot;#204D88FF&quot;, &quot;#112040FF&quot;), domain = arsenicSamples$Median) arsenicSamples%&gt;% leaflet()%&gt;% addCircleMarkers(fillColor = ~pal(Median), fillOpacity = 1, radius = 7, stroke = FALSE)%&gt;% addProviderTiles(providers$Esri.WorldTopoMap)%&gt;% addLegend(pal = pal, values = ~Median) 2.8.2.4 Challenge 4 LynnhavenAU_Stations2022&lt;- pool %&gt;% tbl(in_schema(&quot;wqa&quot;, &quot;WQA Assessment Unit Details&quot;))%&gt;% filter(`Water Name` %like% &quot;%Lynnhaven%&quot; &amp; `Assessment Cycle` == &quot;2022&quot;)%&gt;% left_join(., pool%&gt;%tbl(in_schema(&quot;wqa&quot;, &quot;WQA Station Details&quot;)), &quot;Assessment Unit Detail Id&quot;)%&gt;% # join to add lat long and exceedance values select(Latitude = `GIS Latitude`, Longitude = `GIS Longitude`, `Station Id`, `Enterococci Exceedances`)%&gt;% as_tibble()%&gt;% mutate(`Enterococci Exceedances` = factor(`Enterococci Exceedances`)) pal&lt;-colorFactor(&quot;Reds&quot;, LynnhavenAU_Stations2022$`Enterococci Exceedances`) LynnhavenAU_Stations2022%&gt;% leaflet()%&gt;% addCircleMarkers(color = ~pal(`Enterococci Exceedances`))%&gt;% addProviderTiles(providers$Esri.WorldStreetMap)%&gt;% addLegend(pal = pal, values = ~`Enterococci Exceedances`) 2.8.2.5 Challenge 5 #Determine count and concentration by HUC8 pin_get(&quot;jfamularo/PFAS-Data&quot;)%&gt;% filter(ANALYTE == &quot;Total PFAS&quot; &amp; MEDIUM_DESC == &quot;Surface Water Sample&quot;)%&gt;% group_by(HUC8_NAME)%&gt;% summarise(`Number of Samples` = n(), `Median Concentration (ppt)` = median(ANA_VALUE)) #Determine count and concentration by Program pin_get(&quot;jfamularo/PFAS-Data&quot;)%&gt;% filter(ANALYTE == &quot;Total PFAS&quot; &amp; MEDIUM_DESC == &quot;Surface Water Sample&quot;)%&gt;% group_by(PROGRAM)%&gt;% summarise(`Number of Samples` = n(), `Median Concentration (ppt)` = median(ANA_VALUE)) 2.8.2.6 Challenge 6 # First, bring in the assessment regions as a spatial object and keep just the SWRO polygon library(sf) SWRO &lt;- st_as_sf(pin_get(&quot;ejones/AssessmentRegions_simple&quot;, board = &quot;rsconnect&quot;)) %&gt;% filter(ASSESS_REG == &#39;SWRO&#39;) # pull all VSCI scores for the SWRO sites SWRO_VSCI &lt;- pin_get(&#39;ejones/VSCIresults&#39;, board = &#39;rsconnect&#39;) %&gt;% filter(`Target Count` == 110) %&gt;% # only keep rarified samples left_join(pin_get(&#39;ejones/WQM-Stations-Spatial&#39;, board = &#39;rsconnect&#39;), by = &#39;StationID&#39;) %&gt;% filter(ASSESS_REG == &#39;SWRO&#39;) # pull Sp Cond field data from each of these sites SpCond &lt;- pool%&gt;% tbl(in_schema(&quot;wqm&quot;, &quot;Wqm_Field_Data_View&quot;))%&gt;% filter(Fdt_Sta_Id %in% !! SWRO_VSCI$StationID) %&gt;% filter(Fdt_Spg_Code != &#39;QA&#39;) %&gt;% as_tibble() # Keep only field data associated with each benthic collection event SWRO_VSCI_SpCond &lt;- left_join(SWRO_VSCI, dplyr::select(SpCond, StationID = Fdt_Sta_Id, `Collection Date` = Fdt_Date_Time, SpCond = Fdt_Specific_Conductance), by = c(&#39;StationID&#39;, &#39;Collection Date&#39;)) # But wait, there are more records in SWRO_VSCI_SpCond than SWRO_VSCI using a left join. That means even with removing QA records, there are still records with &gt;1 Sp Cond result per `Collection Date`. We will average those results and then try to join again. # How to verify this theory: # SpCond %&gt;% # dplyr::select(StationID = Fdt_Sta_Id, `Collection Date` = Fdt_Date_Time, SpCond = Fdt_Specific_Conductance, Fdt_Spg_Code) %&gt;% # group_by(StationID, `Collection Date`) %&gt;% # mutate(n = n()) %&gt;% # arrange(desc(n)) %&gt;% # View() SpCond &lt;- SpCond %&gt;% dplyr::select(StationID = Fdt_Sta_Id, `Collection Date` = Fdt_Date_Time, SpCond = Fdt_Specific_Conductance) %&gt;% # don&#39;t need Fdt_Spg_Code anymore group_by(StationID, `Collection Date`) %&gt;% summarise(SpCond = mean(SpCond, na.rm = T)) # Try again to keep only field data associated with each benthic collection event SWRO_VSCI_SpCond &lt;- left_join(SWRO_VSCI, SpCond, by = c(&#39;StationID&#39;, &#39;Collection Date&#39;)) # much better because nrow(SWRO_VSCI_SpCond) == nrow(SWRO_VSCI) # Now make a basic scatter plot of VSCI vs SpCond SWRO_VSCI_SpCond %&gt;% ggplot(aes(x = `SCI Score`, SpCond))+ geom_point()+ geom_smooth(method = &#39;loess&#39;)+ # add a simple loess model to ease visualization theme_classic() # There certainly are some outliers to dig into, but broadly speaking, VSCI scores increase where sites measure lower Specific Conductance "],["spatialAnalysis.html", "Chapter 3 Spatial Analysis", " Chapter 3 Spatial Analysis R is a powerful tool for spatial analysis. Among the many benefits to using R as a GIS are the reproducibility of codified methods, which make sharing procedures and updating results after underlying data updates easy and efficient. There are many resources for learning how to use R for geospatial operations. A favorite is the Geocomputation with R online book (free) by Robin Lovelace. To familiarize yourself with common geospatial procedures in R, the following chapter has been developed by DEQ staff to help colleagues with typical geospatial tasks. This is by no means a comprehensive introduction to geospatial techniques in R. Please see other resources for more background information on underlying geospatial principles and methods in R. "],["spatialDataFromTabularData.html", "3.1 Spatial Data from Tabular Data", " 3.1 Spatial Data from Tabular Data Section Contact: Emma Jones (emma.jones@deq.virginia.gov) Location information can be critical to efficiently analyzing a dataset. Often, spatial data is included into spreadsheets as Latitude and Longitude fields, which can quickly be turned into spatial objects in R for further spatial analysis. This short example demonstrates a method of turning tabular data into spatial data, but this is not the only way to do so. First, load in the necessary packages. library(tidyverse) # for tidy data manipulation library(sf) # for spatial analysis library(leaflet) # for interactive mapping library(inlmisc) # for interactive mapping on top of leaflet Create an example tabular dataset of stations that we want to make into a spatial dataset. Note: we are using the tibble::tribble() function to define a tibble row by row, which makes for easy visualization of test datasets. This is just one of many ways to create a tibble. exampleSites &lt;- tribble( ~StationID, ~Latitude, ~Longitude, &quot;Station_1&quot;, 37.812840, -80.063946, &quot;Station_2&quot;, 37.782322, -79.961449, &quot;Station_3&quot;, 37.801644, -79.968441) exampleSites ## # A tibble: 3 x 3 ## StationID Latitude Longitude ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Station_1 37.8 -80.1 ## 2 Station_2 37.8 -80.0 ## 3 Station_3 37.8 -80.0 Using the sf package, we can use the Latitude and Longitude fields to convert this object into a spatial object. Note we are using EPSG 4326 for our coordinate reference system. We chose that CRS exampleSites_sf &lt;- exampleSites %&gt;% st_as_sf(coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), # make spatial layer using these columns remove = F, # don&#39;t remove these lat/lon cols from df crs = 4326) # Now look at the difference in the object. The geometry listcolumn is where all the spatial magic is stored. exampleSites_sf ## Simple feature collection with 3 features and 3 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -80.06395 ymin: 37.78232 xmax: -79.96145 ymax: 37.81284 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## # A tibble: 3 x 4 ## StationID Latitude Longitude geometry ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;POINT []&gt; ## 1 Station_1 37.8 -80.1 (-80.06395 37.81284) ## 2 Station_2 37.8 -80.0 (-79.96145 37.78232) ## 3 Station_3 37.8 -80.0 (-79.96844 37.80164) You can operate on a spatial dataset created using this method just like any other tidy object and the sticky geometry will come along for the ride. justOneSite &lt;- filter(exampleSites_sf, StationID == &#39;Station_1&#39;) Lets plot the result to see what happened. CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addCircleMarkers(data = exampleSites_sf, color=&#39;yellow&#39;, fillColor=&#39;black&#39;, radius = 5, fillOpacity = 1, opacity=1,weight = 1,stroke=T, group=&quot;All Sites&quot;, label = ~StationID, popup=leafpop::popupTable(exampleSites_sf)) %&gt;% addCircleMarkers(data = justOneSite, color=&#39;orange&#39;, fillColor=&#39;orange&#39;, radius = 5, fillOpacity = 1, opacity=1,weight = 1,stroke=T, group=&quot;Just One Site&quot;, label = ~StationID, popup=leafpop::popupTable(justOneSite)) %&gt;% addLayersControl(baseGroups=c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), overlayGroups = c(&#39;Just One Site&#39;, &#39;All Sites&#39;), options=layersControlOptions(collapsed=T), position=&#39;topleft&#39;) 3.1.1 Removing Spatial Information Sometimes we need to remove spatial information in order to operate on a dataset (e.g. save just the tabular data as a csv). We can use sf to remove just the spatial information. exampleSites_noSpatial &lt;- exampleSites_sf %&gt;% st_drop_geometry() exampleSites_noSpatial ## # A tibble: 3 x 3 ## StationID Latitude Longitude ## * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Station_1 37.8 -80.1 ## 2 Station_2 37.8 -80.0 ## 3 Station_3 37.8 -80.0 "],["consumingGISRESTservices.html", "3.2 Consuming GIS REST Services in R", " 3.2 Consuming GIS REST Services in R Section Contact:Rex Robichaux (rex.robichaux@deq.virginia.gov) 3.2.1 GIS Rest Service Data DEQ Makes a wide variety of its geospatial data available in the form of published REST services (Esri Map Services). First- a little background information on GIS Services in general: Esri ArcGIS REST API documentation One advantage of learning to use Esri REST services over these other formats is that a user may filter their data prior to storing it locally, limiting download size and required data cleaning/filtering. Users may also go on to implement use of these APIs within programs and scripts and know that they are using the most current data available. Another advantage of utilizing Esri REST platforms is that they are available to non Esri clients allowing users to avoid costly subscription fees surrounding ArcGIS and other Esri software. 3.2.2 Primary DEQ REST Endpoints: Internal (Staff Only) Services: https://gis.deq.virginia.gov/arcgis/rest/services/staff External (Public Facing) Services: https://gis.deq.virginia.gov/arcgis/rest/services/public Its important to note that all GIS Services have an API access point to perform queries, which can greatly assist in scripting/query development. Once you have located a service, and a particular layer within a service (denoted by a /# format (ending in a number) such as https://gis.deq.virginia.gov/arcgis/rest/services/staff/DEQInternalDataViewer/MapServer/72 for TMDL Watersheds), you can find a Query operation at the bottom of the page under the Supported Operations section. Upon selecting the Query operation, ArcGIS Server will allow you to dynamically create/test and run custom queries, and provide you with variable output formats (HTML, json, KMZ, GeoJSON, and PBF). Due to GeoJSONs flexibility, and ease of use in R, we will focus on GeoJSON query output formats throughout this exercise. ArcGIS Server REST Query Interface 3.2.3 How do you know what fields/criteria to query on? There are a couple of easy ways to determine criteria upon which to build your queries around. The first, and most immediate way is to simply examine the layer information at the REST endpoint which will show all field names and aliases. Below is an example of the fields present in the TMDL Watersheds layer: REST layer fields for TMDL Watersheds The second way to really view the underlying data, is to leverage our web GIS apps to poke into the attribute and spatial data itself. Below are some examples of using the GIS Staff App to view and filter on specific attributes. We will start by opening the attribute table for the TMDL Watersheds layer in the DEQ Data Layers list: With the Attribute Table widget open, we can now build and apply validation-based filters to view the actual values in any field: TMDL Table Filtering To view the unique values found in any field, simply select Unique in the set input type drop down. After a second, you will be able to view all of the valid values in whichever field has been specified. Here, we are looking at the Pollutant Name (POL_NAME) field, and viewing valid categories for that field. TMDL unique values TMDL value filtering 3.2.4 Okaylets build a simple query and see it in R For this first example, we will query the DEQ REST service underlying the GIS Staff App. The REST url for this service is: https://gis.deq.virginia.gov/arcgis/rest/services/staff/DEQInternalDataViewer/MapServer Well start by querying the WQM Stations layer (layer/ID = 104). We can see there is a wide range of fields that we can query this data layer on. For this first example, we will simply query for a single Station, with a Station ID (or WQM_STA_ID) of 2-FLA028.98. So whats going on below? We parse the URL to create a list where we can add all the parameters of our query - where, outFields, returnGeometry and f - with their appropriate values. As soon as the list is populated we use the function build_url() to create a properly encoded request. This request is the passed to the function st_read() to populate Stream_Stations. This Stream_Stations becomes both a simple features object and a data frame, i.e. it does contain both the geometry and the attribute values of the Station (in this case, a singular station). The interactive inlmisc::CreateWebMap() has various basemaps, a ID/Popup (on hover) based on the Station ID (can be changed to other fields), as well as the ability to zoom in/out: #Read in GIS REST Data in geojson format library(tidyverse) library(httr) library(sf) library(leaflet) library(inlmisc) url &lt;- parse_url(&quot;https://gis.deq.virginia.gov/arcgis/rest/services&quot;) url$path &lt;- paste(url$path, &quot;staff/DEQInternalDataViewer/MapServer/104/query&quot;, sep = &quot;/&quot;) url$query &lt;- list(where = &quot;WQM_STA_ID = &#39;2-FLA028.98&#39;&quot;, outFields = &quot;*&quot;, returnGeometry = &quot;true&quot;, f = &quot;geojson&quot;) request &lt;- build_url(url) Stream_Station &lt;- st_read(request) CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addCircleMarkers(data = Stream_Stations, color = &#39;red&#39;, label = ~WQM_STA_ID ) As this data is now available for viewing, we can easily view/summarize/manipulate it further via summary, etc. 3.2.5 Querying multiple records: Lets look at the capability and performance of querying a series of records, based off of a more complex where statement. In this example, we are searching for all stations records attributed to the Appomattox River: #Read in GIS REST Data in geojson format url &lt;- parse_url(&quot;https://gis.deq.virginia.gov/arcgis/rest/services&quot;) url$path &lt;- paste(url$path, &quot;staff/DEQInternalDataViewer/MapServer/104/query&quot;, sep = &quot;/&quot;) url$query &lt;- list(where = &quot;WQM_STA_STREAM_NAME = &#39;Appomattox River&#39;&quot;, outFields = &quot;*&quot;, returnGeometry = &quot;true&quot;, f = &quot;geojson&quot;) request &lt;- build_url(url) Stream_Stations &lt;- st_read(request) CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addCircleMarkers(data = Stream_Stations, color = &#39;red&#39;, label = ~WQM_STA_ID ) 3.2.6 This works on lines and polygons as well!: We can apply this same logic and format to query and spatially display polygon based layer. For example, we will query the TMDL Watersheds layer (Layer ID 72), searching for all Benthic Impairment TMDLs. We can also convert this data and read it into a data frame for further interpretation: url &lt;- parse_url(&quot;https://gis.deq.virginia.gov/arcgis/rest/services&quot;) url$path &lt;- paste(url$path, &quot;staff/DEQInternalDataViewer/MapServer/72/query&quot;, sep = &quot;/&quot;) url$query &lt;- list(where = &quot;IMP_NAME = &#39;Benthic-Macroinvertebrate Bioassessments (Streams)&#39;&quot;, outFields = &quot;*&quot;, returnGeometry = &quot;true&quot;, f = &quot;geojson&quot;) request &lt;- build_url(url) Benthic_TMDLs &lt;- st_read(request) pal &lt;- colorFactor( palette = &quot;Set2&quot;, domain = unique(Benthic_TMDLs$WSHD_ID)) CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data = Benthic_TMDLs, color = ~pal(WSHD_ID), fillColor = ~pal(WSHD_ID), fillOpacity = 0.6, stroke=5, label = ~WSHD_ID ) 3.2.7 Building more advanced queries You can also add as many criteria as the input data allows. In this example, we are querying TMDL records with Sediment pollutants in the VRO region. When classified by status, we can quickly see that all qualifying TMDL records are approved. url &lt;- parse_url(&quot;https://gis.deq.virginia.gov/arcgis/rest/services&quot;) url$path &lt;- paste(url$path, &quot;staff/DEQInternalDataViewer/MapServer/72/query&quot;, sep = &quot;/&quot;) url$query &lt;- list(where = &quot;POL_NAME = &#39;Sediment&#39; AND REGION = &#39;VRO&#39;&quot;, outFields = &quot;*&quot;, returnGeometry = &quot;true&quot;, f = &quot;geojson&quot;) request &lt;- build_url(url) VRO_Sediment_TMDLs &lt;- st_read(request) pal &lt;- colorFactor( palette = &quot;Set2&quot;, domain = unique(VRO_Sediment_TMDLs$PROJECT_STATUS)) CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data = VRO_Sediment_TMDLs, color = ~pal(WSHD_ID), fillColor = ~pal(PROJECT_STATUS), fillOpacity = 0.6, stroke=5, label = ~WSHD_ID ) %&gt;% addLegend(data = VRO_Sediment_TMDLs, pal = pal, values = ~PROJECT_STATUS, title = &quot;Legend&quot;, position = &#39;topright&#39;) Armed with your new confidence and familiarity with querying and interacting with ArcGIS Server web services go out into the world and see what kinds of data you can query, summarize and analyze! "],["spatialOperationsGISRESTservices.html", "3.3 Spatial Operations on GIS REST Services in R", " 3.3 Spatial Operations on GIS REST Services in R Section Contact:Rex Robichaux (rex.robichaux@deq.virginia.gov) 3.3.1 GIS Rest Service Data DEQs GIS REST endpoints enable easy access to geospatial data for querying and analysis. For a primer on what these services offer and the basics of querying against these services, please see the Consuming GIS REST Services in R section. 3.3.2 Level Up Your Querying Game The final example of Consuming GIS REST Services in R highlights how to use more complicated queries to limit data from within a single spatial layer. What about using information from one layer to limit the information provided on a second layer? Yeah, thats where we are going. But first, why would we want to do this? Besides using the GIS REST services to retrieve the most up to date version of spatial data, we also like to use these services to limit the amount of redundant data that we maintain on our local machines. Instead of, say, bringing back an entire polygon layer and a point layer, spatially intersecting them to find commonalities, and then doing something with said intersected data, we can go straight to the doing something with intersected data step by forcing the GIS REST service to perform our spatial operations. This reduces the amount of data we need to bring back to our machine initially, and reduces the number of associated files that we create during analysis steps. Win win. 3.3.3 Set Up Your Environment Load in the necessary packages. library(sf) library(leaflet) library(inlmisc) library(geojsonsf) library(dplyr) library(urltools) library(rgdal) library(httr) 3.3.4 Query One Layer We will begin by querying the Internal (Staff Only) REST Services by parsing a query of the TMDL records for a specific TMDL project in the Willis River Watershed identified as POL0119. After querying the data we will plot it using and interactive map built with inlmisc::CreateWebMap(). url &lt;- parse_url(&quot;https://gis.deq.virginia.gov/arcgis/rest/services&quot;) url$path &lt;- paste(url$path, &quot;staff/DEQInternalDataViewer/MapServer/72/query&quot;, sep = &quot;/&quot;) url$query &lt;- list(where = &quot;TMDL_EQ_ID = &#39;POL0119&#39;&quot;, outFields = &quot;*&quot;, returnGeometry = &quot;true&quot;, f = &quot;geojson&quot;) request &lt;- build_url(url) tmdl &lt;- st_read(request) CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data = tmdl, color = &#39;black&#39;, fillColor = &#39;red&#39;, fillOpacity = 0.6, stroke=5, label = ~WSHD_ID ) Once we have our chosen watershed, we want to identify monitoring stations within the watershed to investigate further. This could be done one of two ways. The first way, which is much less efficient, is to compare the TMDL watershed against all possible monitoring stations available via the REST service. This would require querying all monitoring stations, bringing that information into your local memory, and spatially intersecting the TMDL watershed against the many thousands of monitoring stations. That is not a great idea for this use case. The preferred method is a two step process. Instead of querying all of the possible monitoring stations from the REST service, we could simply bring back stations that fall close by the TMDL watershed and then spatially intersect those against the watershed to find out which stations are actually within said watershed. That is a much better method as it only brings necessary data into your memory and is a much faster operation to perform on the REST service. To identify the stations that fall close by the TMDL watershed, we can use a piece of information embedded in that watershed known as a bounding box. These are the coordinates of an area that encompasses our watershed. Lets extract this information from the watershed and look at the results. bBox &lt;- st_bbox(tmdl) bBox ## xmin ymin xmax ymax ## -78.63319 37.36281 -78.10111 37.72192 The bBox object contains the latitude and longitude values of the outermost extent of our TMDL watershed. To actually make this useful, lets convert it to an sfc object so we can plot it on a map with our TMDL watershed. For a good resource on simple feature objects, see this article by Jesse Sadler. bBoxPolygon &lt;- bBox %&gt;% st_as_sfc() CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data = tmdl, color = &#39;black&#39;, fillColor = &#39;green&#39;, fillOpacity = 0.6, stroke=5, label = ~WSHD_ID ) %&gt;% addPolygons(data=bBoxPolygon, weight=2, fill=FALSE, color=&quot;blue&quot;) 3.3.5 Query Layer Two Based On Information From Layer One We can tell the GIS REST service to only retrieve monitoring stations within this bounding box (i.e. query a layer based on information from another layer) by adding this bounding box information into our query of the WQM Stations layer. This forces the REST service to perform the more computationally expensive step of only returning stations within our area. We are also going to add the XY coordinates to the object to make the mapping steps easier using st_coordinates(). # baseURL for the WQM Stations layer baseURL &lt;- &#39;https://gis.deq.virginia.gov/arcgis/rest/services/staff/DEQInternalDataViewer/MapServer/104/query?&#39; # convert the bounding box from above to a character string to work in our query bbox &lt;- toString(bBox) # encode for use within URL bbox &lt;- urltools::url_encode(bbox) # EPSG code for coordinate reference system used by the TMDL polygon sf object epsg &lt;- st_crs(tmdl)$epsg # set parameters for query query &lt;- urltools::param_set(baseURL,key=&quot;geometry&quot;, value=bbox) %&gt;% param_set(key=&quot;inSR&quot;, value=epsg) %&gt;% param_set(key=&quot;resultRecordCount&quot;, value=500) %&gt;% param_set(key=&quot;f&quot;, value=&quot;geojson&quot;) %&gt;% param_set(key=&quot;outFields&quot;, value=&quot;*&quot;) # query the REST service wqmstations_inbox &lt;- geojson_sf(query) %&gt;% dplyr::mutate(Longitude = sf::st_coordinates(.)[,1], Latitude = sf::st_coordinates(.)[,2]) %&gt;% dplyr::select(STATION_ID, Latitude, Longitude, everything()) # print out the data using DT::datatable() datatable( wqmstations_inbox, rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) Lets plot these stations to make sure our query returned what we asked. CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data = tmdl, color = &#39;black&#39;, fillColor = &#39;green&#39;, fillOpacity = 0.6, stroke=5, label = ~WSHD_ID ) %&gt;% addPolygons(data=bBoxPolygon, weight=2, fill=FALSE, color=&quot;blue&quot;) %&gt;% addCircleMarkers(data = wqmstations_inbox, lng = ~Longitude, lat = ~Latitude, label = ~as.character(STATION_ID)) That is still a lot of stations! This is a good example of why it is a best practice to have the database do as much data limiting for you as possible. 3.3.6 Spatially Intersect Layers The final step of this workflow is to spatially intersect the monitoring stations within the watershed (i.e. find the points inside the polygon). We will do this with the st_intersection() function from the sf library. Once we intersect these layers, we will add the final point object to our map, colored in red, to verify the intersection only returned monitoring stations within the polygon. Like before, we will add the XY coordinates of these point to the object using st_coordinates() to enable easier plotting. insideWatershed &lt;- st_intersection(wqmstations_inbox, tmdl) %&gt;% dplyr::mutate(Longitude = sf::st_coordinates(.)[,1], Latitude = sf::st_coordinates(.)[,2]) datatable(insideWatershed) CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data = tmdl, color = &#39;black&#39;, fillColor = &#39;green&#39;, fillOpacity = 0.6, stroke=5, label = ~WSHD_ID ) %&gt;% addPolygons(data=bBoxPolygon, weight=2, fill=FALSE, color=&quot;blue&quot;) %&gt;% addCircleMarkers(data = wqmstations_inbox, lng = ~Longitude, lat = ~Latitude, label = ~as.character(STATION_ID)) %&gt;% addCircleMarkers(data = insideWatershed, lng = ~Longitude, lat = ~Latitude, color = &#39;red&#39;, label = ~as.character(STATION_ID) ) Mission accomplished. If we look closely at our data we can see there are 149 rows of information in the point file, but only 60 unique monitoring stations to investigate for the rest of our analysis. The map indicates this with the redder sites, indicating there is overplotting of points. We can clean this up in subsequent data manipulation steps to make a publication quality map. insideWatershedUniqueSites &lt;- insideWatershed %&gt;% dplyr::select(STATION_ID, WQM_STA_DESC, everything()) %&gt;% #rearrange columns distinct(STATION_ID, .keep_all = T) # keep only rows with unique STATION_ID information CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data = tmdl, color = &#39;black&#39;, fillColor = &#39;green&#39;, fillOpacity = 0.6, stroke=5, label = ~WSHD_ID ) %&gt;% addCircleMarkers(data = insideWatershedUniqueSites, lng = ~Longitude, lat = ~Latitude, color = &#39;red&#39;, label = ~as.character(STATION_ID) ) "],["usingSpatialDatasets.html", "3.4 Using Spatial Datasets", " 3.4 Using Spatial Datasets Coming Soon "],["interactiveMapping.html", "3.5 Interactive Mapping", " 3.5 Interactive Mapping with leaflet. Coming soon! "],["watershed-delineation.html", "3.6 Watershed Delineation", " 3.6 Watershed Delineation Section Contact: Emma Jones (emma.jones@deq.virginia.gov) This module overviews the basics of delineating watersheds directly in your local R environment by scraping USGSs StreamStats API. General web scraping techniques are beyond the scope of this module, but the basics can be gleaned by unpacking the referenced functions. All watersheds delineated using this technique use USGSs StreamStats delineation techniques from a 1:24k NHD. You may manually explore the tool here. First, load in the necessary packages and functions to complete this task. library(tidyverse) # for tidy data manipulation library(sf) # for spatial analysis library(leaflet) # for interactive mapping library(inlmisc) # for interactive mapping on top of leaflet You will also need to use a custom function built for scraping the StreamStats API. This function is contained in the sourced script StreamStatsAutoDelineation.R which be downloaded for sourcing locally in your environment. source(&#39;StreamStatsAutoDelineation.R&#39;) # for custom web scraping tool to hit USGS StreamStats API Create an example dataset of stations that we want to delineate. Note: we are using the tibble::tribble() function to define a tibble row by row, which makes for easy visualization of test datasets. This is just one of many ways to create a tibble. exampleSites &lt;- tribble( ~StationID, ~Latitude, ~Longitude, &quot;Station_1&quot;, 37.812840, -80.063946, &quot;Station_2&quot;, 37.782322, -79.961449, &quot;Station_3&quot;, 37.801644, -79.968441) exampleSites ## # A tibble: 3 x 3 ## StationID Latitude Longitude ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Station_1 37.8 -80.1 ## 2 Station_2 37.8 -80.0 ## 3 Station_3 37.8 -80.0 The above dataset is simply tabular data. One could use a spatial dataset for this task by stripping out the coordinate information from the geometry listcolumn. See Spatial Data from Tabular Data for more information on this topic. 3.6.1 Single Station Delineation Next lets use USGSs automated delineation tools to delineate single station. We will first select only one site from our exampleSites object we created in the previous step and name it stationToDelineate. Then, we will feed our streamStats_Delineation() function (called into our environment when we sourced the StreamStatsAutoDelineation.R script) the necessary location information (state, longitude, and latitude arguments). The state argument tells StreamStats which state NHD we want to use for delineation, VA (Virginia) for our example. Lastly, the UID argument is the unique identifier we wish to associate with the spatial information we pull back from StreamStats. If we do not provide this information, we will not know which watersheds belong to which sites as we start to batch process these jobs. stationToDelineate &lt;- filter(exampleSites, StationID == &#39;Station_1&#39;) stationDelineation &lt;- streamStats_Delineation(state= &#39;VA&#39;, longitude = stationToDelineate$Longitude, latitude = stationToDelineate$Latitude, UID = stationToDelineate$StationID) The information returned from StreamStats is a list object containing point information (the location we supplied to delineate from) and polygon information (the resultant upstream shape returned from the pour point). We can easily unpack this information into easily used objects using the script below. stationDelineationWatershed &lt;- stationDelineation$polygon %&gt;% reduce(rbind) %&gt;% arrange(UID) stationDelineationPoint &lt;- stationDelineation$point %&gt;% reduce(rbind) %&gt;% arrange(UID) We can plot the results quickly to verify the desired watershed was returned. StreamStats returns a watershed for the input coordinates to the best of its ability; however, the accuracy of the coordinates, datum, projection, etc. can influence the accuracy of the returned watershed. It is best practice to always review the returned watershed. Below is a minimal example of how to do this in R with an interactive map. Note: you can switch basemaps to the USGS Hydrography layer for further information using the layers button in the top left corner. The following interactive map is created with the inlmisc package, see the package authors article for a detailed tutorial. The interactive mapping section covers some basics with leaflet. CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data= stationDelineationWatershed, color = &#39;black&#39;, weight = 1, fillColor=&#39;blue&#39;, fillOpacity = 0.4,stroke=0.1, group=&quot;Watershed&quot;, popup=leafpop::popupTable(stationDelineationWatershed, zcol=c(&#39;UID&#39;))) %&gt;% addCircleMarkers(data = stationDelineationPoint, color=&#39;orange&#39;, fillColor=&#39;black&#39;, radius = 5, fillOpacity = 1, opacity=1,weight = 1,stroke=T, group=&quot;Station&quot;, label = ~UID, popup=leafpop::popupTable(stationDelineationPoint)) %&gt;% addLayersControl(baseGroups=c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), overlayGroups = c(&#39;Station&#39;,&#39;Watershed&#39;), options=layersControlOptions(collapsed=T), position=&#39;topleft&#39;) 3.6.2 Multiple Station Delineation Using the dataset created above (exampleSites), we will now batch process the sites to StreamStats for an efficient data workflow. Remember, you are hitting the USGS API repeatedly, so there can be losses in connectivity resulting in missed watersheds. We will overview the QA process after we receive the watershed information back from USGS. multistationDelineation &lt;- streamStats_Delineation(state= &#39;VA&#39;, longitude = exampleSites$Longitude, latitude = exampleSites$Latitude, UID = exampleSites$StationID) The information returned from StreamStats is a list object containing point information (the location we supplied to delineate from) and polygon information (the resultant upstream shape returned from the pour point). We can easily unpack this information into easily used objects using the script below. watersheds &lt;- multistationDelineation$polygon %&gt;% reduce(rbind) %&gt;% arrange(UID) points &lt;- multistationDelineation$point %&gt;% reduce(rbind) %&gt;% arrange(UID) The next chunk overviews how to efficiently check to make sure all the desired sites were in fact delineated. If there are missing sites, the script will run back out to StreamStats to get anyone that is missing and smash that into the original dataset. # fix anything that is missing if(nrow(points) != nrow(watersheds) | nrow(exampleSites) != nrow(watersheds)){ missing &lt;- unique( c(as.character(points$UID[!(points$UID %in% watersheds$UID)]), as.character(exampleSites$StationID[!(exampleSites$StationID %in% watersheds$UID)]))) missingDat &lt;- filter(exampleSites, StationID %in% missing) #remove missing site from the paired dataset points &lt;- filter(points, ! UID %in% missing) watersheds &lt;- filter(watersheds, ! UID %in% missing) dat &lt;- streamStats_Delineation(state= &#39;VA&#39;, longitude = missingDat$Long, latitude = missingDat$Lat, UID = missingDat$StationID) watersheds_missing &lt;- dat$polygon %&gt;% reduce(rbind) points_missing &lt;- dat$point %&gt;% reduce(rbind) watersheds &lt;- rbind(watersheds, watersheds_missing) %&gt;% arrange(UID) points &lt;- rbind(points, points_missing) %&gt;% arrange(UID) rm(missingDat); rm(dat); rm(watersheds_missing); rm(points_missing) } Now lets map our results to ensure StreamStats delineated the correct watersheds. CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data= watersheds, color = &#39;black&#39;, weight = 1, fillColor=&#39;blue&#39;, fillOpacity = 0.4,stroke=0.1, group=&quot;Watershed&quot;, popup=leafpop::popupTable(watersheds, zcol=c(&#39;UID&#39;))) %&gt;% addCircleMarkers(data = points, color=&#39;orange&#39;, fillColor=&#39;black&#39;, radius = 5, fillOpacity = 1, opacity=1,weight = 1,stroke=T, group=&quot;Station&quot;, label = ~UID, popup=leafpop::popupTable(points)) %&gt;% addLayersControl(baseGroups=c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), overlayGroups = c(&#39;Station&#39;,&#39;Watershed&#39;), options=layersControlOptions(collapsed=T), position=&#39;topleft&#39;) 3.6.3 QA Should any of the returned watersheds prove incorrect based on visual analysis, you must remove that watershed from your dataset, manually delineate the watershed using StreamStats, and include that new watershed into your polygon dataset. See the Using Spatial Datasets section for example workflows. "],["landcover-analysis.html", "3.7 Landcover analysis", " 3.7 Landcover analysis Coming Soon "],["spatial-joins.html", "3.8 Spatial joins", " 3.8 Spatial joins Section Contact: Emma Jones (emma.jones@deq.virginia.gov) Spatial joins are incredibly useful operations that allow users to link different spatial features together. If you have ever wondered if something falls inside something else, thats a spatial join (or intersection, if we want to be completely honest with ourselves). If you have ever wanted to know information about feature y based on feature x, thats a spatial join. 3.8.1 Case Study: Water Permit Review A use case for using spatial joins in R is demonstrated through the process of reviewing permits in the Water Programs. During the permit review process, Water Planning staff should identify whether or not permits fall into TMDL watersheds. The following workflow documents how to identify whether or not a permitted outfall falls into a TMDL watershed. 3.8.1.1 Point Locations First, we must create a dataset of all the point locations we want to compare against polygons (TMDL watersheds). An easy way to visually create a dataset in R (for demonstration purposes) is by using the tibble::tribble() function. We will create a tibble of 17 sites we want to investigate further. This might not be the most efficient way to bring data into your R environment for real data examples, so we advise reading in datasets from local files when possible. library(tidyverse) outfalls &lt;- tribble( ~SiteID, ~Latitude, ~Longitude, &quot;Site 1&quot;, 37.48516, -79.166643, &quot;Site 2&quot;, 37.489919, -79.228329, &quot;Site 3&quot;, 37.048334, -79.886112, &quot;Site 4&quot;, 37.377499, -79.558333, &quot;Site 5&quot;, 37.211113, -79.299446, &quot;Site 6&quot;, 37.146388, -79.619167, &quot;Site 7&quot;, 37.363862, -79.955802, &quot;Site 8&quot;, 37.365318, -79.956827, &quot;Site 9&quot;, 37.24955, -79.943387, &quot;Site 10&quot;, 37.066667, -78.958333, &quot;Site 11&quot;, 36.627502, -80.288611, &quot;Site 12&quot;, 36.57995, -79.428251, &quot;Site 13&quot;, 36.579799, -79.427712, &quot;Site 14&quot;, 36.783053, -80.003705, &quot;Site 15&quot;, 36.951112, -79.351669, &quot;Site 16&quot;, 36.99861, -80.723612, &quot;Site 17&quot;, 37.335081, -80.757731) Now convert the outfalls object into a spatial dataset using the sf package. library(sf) outfalls &lt;- st_as_sf(outfalls, coords = c(&#39;Longitude&#39;, &#39;Latitude&#39;), remove = F, # don&#39;t remove these lat/lon cols from dataset crs = 4326) # EPSG 4326 = WGS84 coordinate reference system Lets look at these points before proceeding to see if there is anything special about these points? library(leaflet) library(inlmisc) CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addCircleMarkers(data = outfalls, color = &#39;black&#39;, fillColor = &#39;blue&#39;, fillOpacity = 0.6, stroke=5, label = ~SiteID ) These stations all appear to be in the Blue Ridge Regional Office region, so we will use this to dial further data queries. 3.8.2 Polygons of Interest We know we need to compare these points to TMDL watersheds, so lets use the latest published version of this dataset that is available on the internal GIS REST service. See the Consuming GIS REST Services in R section to learn more about querying the GIS REST service. The below chunk creates a spatial object of all the TMDL watersheds in the state and creates a map of all the returned watersheds overlayed by the outfall points from above. library(httr) url &lt;- parse_url(&quot;https://gis.deq.virginia.gov/arcgis/rest/services&quot;) url$path &lt;- paste(url$path, &quot;staff/DEQInternalDataViewer/MapServer/72/query&quot;, sep = &quot;/&quot;) url$query &lt;- list(where = &quot;REGION = &#39;WCRO&#39;&quot;, #&quot;OBJECTID_1 &gt; 0&quot;, outFields = &quot;*&quot;, returnGeometry = &quot;true&quot;, f = &quot;geojson&quot;) request &lt;- build_url(url) # create a spatial dataset based on the REST query TMDLwatersheds &lt;- st_read(request) # map returned watersheds and outfall points CreateWebMap(maps = c(&quot;Topo&quot;,&quot;Imagery&quot;,&quot;Hydrography&quot;), collapsed = TRUE) %&gt;% addPolygons(data = TMDLwatersheds, color = &#39;gray&#39;, fillColor = &#39;gray&#39;, fillOpacity = 0.6, stroke=5, label = ~PJ_NAME ) %&gt;% addCircleMarkers(data = outfalls, color = &#39;black&#39;, fillColor = &#39;blue&#39;, fillOpacity = 0.6, stroke=5, label = ~SiteID ) 3.8.2.1 Spatial Join So far we have identified that the outfall point do fall within our TMDL watersheds, but how do we know which outfalls are contained within specific TMDL watersheds? Enter the spatial join. We will use the outfalls as our left hand join argument to join to our TMDL watershed layer in order to extract the TMDL watershed information relative to each outfall. Said in a more general way, we will use the points as our left hand join argument to join to our polygon layer in order to extract the polygon information relative to each point. outfallsInTMDL &lt;- st_join(outfalls, TMDLwatersheds) Wait, what? It looks like the joining step ran into an error. Time to figure out what that error message means in order to solve the problem. So after a bit of poking around the internet, it seems this error is generated when there are geometry issues with one of the two spatial datasets involved in the join. Polygon features are usually to blame for geometry issues. Lets start there with our troubleshooting. Lets try and repair the geometry issues in the local version of the TMDLwatersheds data using sfs built in geometry repair function st_make_valid(). TMDLwatersheds_fixedGeometry &lt;- st_make_valid(TMDLwatersheds) Now lets try the join again and see if we can get join to work properly. outfallsInTMDL &lt;- st_join(outfalls, TMDLwatersheds_fixedGeometry) Huzzah! It worked. Lets see what the returned data look like in tabular form. library(DT) datatable(outfallsInTMDL %&gt;% st_drop_geometry()) # drop spatial component for the HTML table Looks good! We have each point that fell into a polygon attached to the metadata (or attribute information) associated with each polygon. Note: The sites with multiple rows indicate that the point falls into multiple overlapping polygons. Lets do a quick summary to understand this data: outfallsInTMDL %&gt;% st_drop_geometry() %&gt;% #drop spatial part for this analysis group_by(SiteID) %&gt;% summarise(`n Projects` = n(), `Project Names` = paste(unique(PJ_NAME), collapse = &#39;, &#39;), `Project Types` = paste(unique(IMP_NAME), collapse = &#39;, &#39;)) ## # A tibble: 17 x 4 ## SiteID `n Projects` `Project Names` `Project Types` ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Site 1 1 James River - Lynchburg Escherichia coli ## 2 Site 10 1 Falling River Watershed Escherichia coli ## 3 Site 11 2 South Mayo River Watershed,~ Escherichia coli ## 4 Site 12 1 Dan River Watershed Escherichia coli ## 5 Site 13 1 Dan River Watershed Escherichia coli ## 6 Site 14 2 Dan River Watershed, Smith ~ Escherichia coli, Benthic-~ ## 7 Site 15 1 NA NA ## 8 Site 16 1 NA NA ## 9 Site 17 1 New River PCB PCB in Fish Tissue ## 10 Site 2 1 James River - Lynchburg Escherichia coli ## 11 Site 3 3 Roanoke (Staunton) River Wa~ Polychlorinated biphenyls,~ ## 12 Site 4 4 Little Otter River, Johns C~ Benthic-Macroinvertebrate ~ ## 13 Site 5 2 Big Otter River Watershed, ~ Total Fecal Coliform, Poly~ ## 14 Site 6 2 Roanoke (Staunton) River Wa~ Polychlorinated biphenyls,~ ## 15 Site 7 3 Tinker Creek Watershed, Upp~ Escherichia coli, Benthic-~ ## 16 Site 8 3 Tinker Creek Watershed, Upp~ Escherichia coli, Benthic-~ ## 17 Site 9 3 Roanoke (Staunton) River Wa~ Polychlorinated biphenyls,~ Depending on how the user needs to share the results of the analysis, the data can be saved in tabular or spatial data formats. The chunk below shows how to save the results in two different formats. # save as a csv write.csv(outfallsInTMDL %&gt;% st_drop_geometry(), &#39;outfallsInTMDL.csv&#39;, na = &#39;&#39;, row.names = F) # don&#39;t include geometry in tabular output # save as a shapefile st_write(outfallsInTMDL, &#39;outfallsInTMDL.shp&#39;) "],["shinyAppHelp.html", "Chapter 4 Shiny App Pro Tips", " Chapter 4 Shiny App Pro Tips Shiny apps are interactive web based applications built in R to increase accessibility to analytical tools, workflows, and visualizations to non-R users. DEQ relies on these tools (hosted on the internal Connect platform) to extend data querying, manipulation, analysis, visualization, and reporting techniques to all staff, regardless of programming experience. Complicated workflows can be programmed in R using the shiny package to develop an easy to use front end interface, all in the R language. Shiny apps are easy to build. Many tutorials are available, but the best starting point is to follow the shiny tutorial by RStudio. More advanced shiny techniques employed regularly by DEQ staff to improve application responsiveness, user experience, or back end development are outlined below. Coming soon: Shiny file organization - Using functions and modules to improve code organization, how to integrate nested modules without namespace issues Shiny Tricks: Using HTML to help reset a complex UI How to integrate tool-tip HTML without calling new packages Escaping characters in user inputs Markdown and User-Friendly Output Files: readxlsx tips for formatting excel outputs Markdown files with \"child sub-markdown files Markdown tips in general for TinyTex and HTML "],["shiny-file-organization.html", "4.1 Shiny file organization", " 4.1 Shiny file organization Coming Soon! "],["shiny-tricks.html", "4.2 Shiny Tricks", " 4.2 Shiny Tricks 4.2.1 Mandatory Shiny Inputs Section Contact: Connor Brogan (connor.brogan@deq.virginia.gov) Youre creating a new Shiny app that you will be sharing with several co-workers. Your supervisor has asked that you make it user friendly so that it can be given to new employees without creating any error or issues with the teams workflow. At first, this seemed easy. You created the app, tested it, and got it all working. But then you realized that the program has a number of inputs that are required to get it to work. When the user doesnt enter these, the program spits out a generic warning message. Your other teammates dont have any issues with this since they know all the science behind-the-scenes and know which inputs are required. But your new users are left scratching their heads. So what do you do? You could turn to the shinyjs package and its disable() function. This would block users from accessing certain inputs until the required conditions are met. But, without detailed help menus or a user manual, new users may still be confused why they cant get the program to work. Maybe you take the verbose strategy. You write a detailed error message for every fork in your QC function such that your program tells the user exactly whats wrong. This is usually an acceptable solution, but if your program has too many inputs it can leave your users frustrated as they try to identify the input linked to the warning. In these cases, I like to set-up Mandatory Inputs. You are likely familiar with these from nFORM, Google forms, or any number of other applications. When you click run, the program checks if all required inputs have been entered. Those that havent been are marked with a red asterisk (*). This is best used when paired with verbose error messages so the user can quickly identify what is wrong with their inputs! In the example below, the input is marked as required as soon as you press the Check if Number is Even button, but only if you didnt enter a number or otherwise entered an odd number. library(shiny) library(shinyjs) #First, we define a function, labelMandatory, that will output a tagList that #includes a label and a span HTML element. The tagList will group these elements #together for us to use as an input to a Shiny input function. The inputs here #are the label that we want displayed for that Shiny input and the id of the #parent Shiny input labelMandatory &lt;- function(label,id) { #The tagList out will be used as a label for a Shiny input. The label input #will be displayed, but the span will targeted by shinyjs::hideElement() when #it is not required out &lt;- tagList( label, #Give the span an id. This allows us to target the span with hide- and #showElement() in shinyjs span(&quot;*&quot;, id = paste0(id,&quot;-span&quot;),class = &quot;mandatory_star&quot;) ) return(out) } #Now we will build a basic shiny app that has a numericInput and checks to see #if the entered number is even ui &lt;- fluidPage( #In line styles to mark the asterisk as red and position it better within the #label. These are not necessary, but make it look prettier and using #display:none; ensures the mandatory asterisks are hidden at first tags$head( tags$style(HTML( &quot;body{ background-color:aliceblue; } .mandatory_star{ color: red; display:none; vertical-align:sub; font-weight:bold; font-size:large; }&quot; )) ), #Need to use shinyjs within the ui useShinyjs(), #We create a text input. We set the label using labelMandatory. We need to #provide the label and the id of the input numericInput(inputId = &quot;numericInput&quot;, label = labelMandatory(&quot;Please enter a number:&quot;,&quot;numericInput&quot;), value = NULL), #An actionButton, used for demonstration purposes. When clicked, it will #trigger the shinyjs command to hide or show the asterisk in the text input #label actionButton(&quot;goButton&quot;,&quot;Check if Number is Even&quot;) )#End of fluidPage server &lt;- function(input, output, session) { #When the user clicks goButton, check to see if numericInput has been entered. #If it is not even or not entered, output a message and mark it as mandatory observeEvent(input$goButton,{ #Hide all mandatory stars to reset the UI. Only those that cause problems #will be displayed. We can use the wild selector to target all span elements #of class mandatory_star hideElement(selector=&quot;span.mandatory_star&quot;) #Check to see if input has been entered: if(!isTruthy(input$numericInput)){ #Warning message showNotification(type = &quot;error&quot;, ui = &quot;Please enter a number&quot;) #Show the span within the input field by passing the correct span id showElement(id = paste0(&quot;numericInput&quot;,&quot;-span&quot;)) }else if(input$numericInput %% 2 == 1){ #Warning message showNotification(type = &quot;error&quot;, ui = &quot;The number must be even&quot;) #Show the span within the input field by passing the correct span id showElement(id = paste0(&quot;numericInput&quot;,&quot;-span&quot;)) }else{ #Warning message showNotification(type = &quot;message&quot;, ui = &quot;The number is even. Mission Accomplished.&quot;) } })#End of observeEvent }#End of Server function #Call Shiny App shinyApp(ui, server) The above example works using a little bit of shinyjs and a bit of HTML/css. If youre not familiar with those concepts, fret not. I will try to explain them as I go, although if you want to learn more I do recommend this basic course on html and the corresponding course on css. The first necessary piece of code is part of the package shinyjs. After calling the package and building it into our apps user-interface (UI) via a useShinyjs() call, we leverage the very powerful functions hideElement() and showElement(). These functions do almost verbatim what their name implies: they use Javascript (js, as in shinyjs) to literally hide or show/display an HTML element. Essentially, they send a custom javascript command that causes these elements to change their display css property from its default (often set to the inline value) to the value none. So, hideElement() will give an HTML element the property display:none; and showElement() will give it display:inline; or something similar. Thats pretty technical, but what is important is that hideElement() removes something from the users view and showElement() adds it back. Returning to the Shiny app example above, these are the functions that hide and show the mandatory * that is in the input label! These functions can target HTML elements by either using an id/shiny tag or with a css selector. Using an id/shiny tag is an effective and quick way to target individual elements. So, for instance, lets say you want to hide or display a single numericInput(). Here we could just use the ID that we gave numericInput: ui &lt;- fluidPage( #Need to use shinyjs within the ui useShinyjs(), tags$head( tags$style(HTML( &quot;body{ background-color:aliceblue; }&quot; )) ), #We create a numeric input that we want to display or hide based on the button numericInput(inputId = &quot;hideShowNumericInput&quot;, label = &quot;This input will be hidden or displayed by clicking the button&quot;, value = NULL), #An actionButton, used for demonstration purposes. When clicked, it will #trigger the shinyjs command to hide or show the numeric input actionButton(&quot;goButton&quot;,&quot;Hide/Show Element&quot;) )#End of fluidPage server &lt;- function(input, output, session) { #When the user clicks goButton, hide or show hideShowNumericInput observeEvent(input$goButton,{ #If the user clicks the button, toggle the display state of #hideShowNumericInput. If the number of times the button has been pressed is #odd, hide the element. Otherwise, display it! if(input$goButton %% 2 == 1){ hideElement(id = &quot;hideShowNumericInput&quot;) }else{ showElement(id = &quot;hideShowNumericInput&quot;) } })#End of observeEvent }#End of Server function #Call Shiny App shinyApp(ui, server) Rather than hiding/showing a single HTML element, we may want to do something a little more powerful and target groups of inputs. In these cases, we would want to target any number of our required inputs AND we only want to target the label. We dont need to hide/show the entire Shiny input, but just the little asterisk in the label. So, we would instead want to give hide/showElement() a css selector. Using this, we can target a group of HTML elements based on our argument. For instance, in the below app, we target all label HTML elements to hide/show the labels of the Shiny inputs. We can use toggleElement() to switch from hiding to showing and vice versa without needing to write it all out. ui &lt;- fluidPage( #Need to use shinyjs within the ui useShinyjs(), tags$head( tags$style(HTML( &quot;body{ background-color:aliceblue; }&quot; )) ), fluidRow( column(6, #We create a numeric input that we want to display or hide based on #the button numericInput(inputId = &quot;numericInput1&quot;, label = &quot;This label will be hidden/displayed by clicking the button&quot;, value = NULL), ), column(6, #We create a text input that we want to display or hide based on the #button textInput(inputId = &quot;numericInput2&quot;, label = &quot;This label will also be hidden/displayed by clicking the button&quot;, value = &quot;&quot;), ) ), #An actionButton, used for demonstration purposes. When clicked, it will #trigger the shinyjs command to hide or show the shiny input labels actionButton(&quot;goButton&quot;,&quot;Hide/Show Element&quot;) )#End of fluidPage server &lt;- function(input, output, session) { #When the user clicks goButton, hide or show hideShowNumericInput observeEvent(input$goButton,{ #If the user clicks the button, toggle the display state of #all the Shiny input labels! toggleElement(selector = &quot;label&quot;) })#End of observeEvent }#End of Server function #Call Shiny App shinyApp(ui, server) Now, how can we leverage hide/showElement() to help us with our mandatory inputs? The above example clearly illustrates that we can hide or display the entire label of a Shiny input, but what if we just want to target a portion of it like our asterisk? This is where we need our custom function labelMandatory() (which is taken from Dean Attaliss R blog). #First, we define a function, labelMandatory, that will output a tagList that #includes a label and a span HTML element. The tagList will group these elements #together for us to use as an input to a Shiny input function. The inputs here #are the label that we want displayed for that Shiny input and the id of the #parent Shiny input labelMandatory &lt;- function(label,id) { #The tagList out will be used as a label for a Shiny input. The label input #will be displayed, but the span will targeted by shinyjs::hideElement() when #it is not required out &lt;- tagList( label, #Give the span an id. This allows us to target the span with hide- and #showElement() in shinyjs span(&quot;*&quot;, id = paste0(id,&quot;-span&quot;),class = &quot;mandatory_star&quot;) ) return(out) } labelMandatory() creates a shiny tagList that contains both a label and a span HTML element (of class mandatory_star, which is css that we write into our app) that holds an asterisk. The span element is given an ID equivalent to the user input pasted with -span. This can be fed right into the id = argument of any Shiny input function to create a label that holds text and this span element. Why does that matter? Well, it means we can now target the span elements within the labels using hide/showElements()! We just have to set the selector to look for all span HTML elements of class mandatory_star (i.e. selector = span.mandatory_star). Or, we can toggle the visibility of individual elements based on the ID that we give labelMandatory(). Below, we add some css to our app to define the mandatory_star css class used in labelMandatory() which allows us to make the asterisk appear in bold and red. #First, we define a function, labelMandatory, that will output a tagList that #includes a label and a span HTML element. The tagList will group these elements #together for us to use as an input to a Shiny input function. The inputs here #are the label that we want displayed for that Shiny input and the id of the #parent Shiny input labelMandatory &lt;- function(label,id) { #The tagList out will be used as a label for a Shiny input. The label input #will be displayed, but the span will targeted by shinyjs::hideElement() when #it is not required out &lt;- tagList( label, #Give the span an id. This allows us to target the span with hide- and #showElement() in shinyjs span(&quot;*&quot;, id = paste0(id,&quot;-span&quot;),class = &quot;mandatory_star&quot;) ) return(out) } ui &lt;- fluidPage( #In line styles to mark the asterisk as red and position it better within the #label. These are not necessary, but make it look prettier and using #display:none; ensures the mandatory asterisks are hidden at first tags$head( tags$style(HTML( &quot;body{ background-color:aliceblue; } .mandatory_star{ color: red; display:none; vertical-align:sub; font-weight:bold; font-size:large; }&quot; )) ), #Need to use shinyjs within the ui useShinyjs(), fluidRow( column(6, #We create a numeric input that we want to display or hide based on #the button numericInput(inputId = &quot;numericInput1&quot;, label = labelMandatory(label = &quot;This label&#39;s asterisk will be hidden/displayed by clicking the All button&quot;, id = &quot;numericInput1&quot;), value = NULL), ), column(6, #We create a text input that we want to display or hide based on the #button textInput(inputId = &quot;numericInput2&quot;, label = labelMandatory(label = &quot;This label&#39;s asterisk will be hidden/displayed by clicking the All button&quot;, id = &quot;numericInput2&quot;), value = &quot;&quot;), ) ), fluidRow( column(6, #An actionButton, used for demonstration purposes. When clicked, it #will trigger the shinyjs command to hide or show all labels actionButton(&quot;goButton1&quot;,&quot;Hide/Show All Asterisks&quot;) ), column(6, #An actionButton, used for demonstration purposes. When clicked, it #will trigger the shinyjs command to hide or show all labels actionButton(&quot;goButton2&quot;,&quot;Hide/Show The Second asterisk&quot;) ) ), )#End of fluidPage server &lt;- function(input, output, session) { #When the user clicks goButton1, hide or show all mandatory stars by targeting #span elements of the appropriate css class observeEvent(input$goButton1,{ #If the user clicks the button, toggle the display state of #all the Shiny input labels! toggleElement(selector = &quot;span.mandatory_star&quot;) })#End of observeEvent #When the user clicks goButton2, hide or show just the mandatory star of the #second input by targeting it&#39;s span id observeEvent(input$goButton2,{ #If the user clicks the button, toggle the display state of #all the Shiny input labels! toggleElement(id = &quot;numericInput2-span&quot;) })#End of observeEvent }#End of Server function #Call Shiny App shinyApp(ui, server) Neat! So, we can use the custom function labelMandatory() to add these mandatory star asterisks into shiny input labels. We can style them with css that we feed into our user interface code. Then, we can target the span elements created by labelMandatory() to hide/show elements with the functions from shinyjs. This approach will work well for smaller applications where there are few mandatory inputs. However, for bigger applications or those with many required inputs, it can be tedious to write all the toggle/hide/showElement() calls you need. For these, I rely on a slightly different approach. I will typically include a verbose quality control function that reviews all the inputs at once and assesses any potential issues with the user entries. Each time the function finds an issue, it notes the names of the elements causing the issue in a list, dataframe, or vector. It returns this list to the app server, which can then use mapply() to hide/show any of the asterisks associated with the problem elements. This is how I created the below app, which has several inputs. #Now we will build a basic shiny app that has a numericInput and checks to see #if the entered number is even ui &lt;- fluidPage( #In line styles to mark the asterisk as red and position it better within the #label. These are not necessary, but make it look prettier and using #display:none; ensures the mandatory asterisks are hidden at first tags$head( tags$style(HTML( &quot;body{ background-color:aliceblue; } .mandatory_star{ color: red; display:none; vertical-align:sub; font-weight:bold; font-size:large; }&quot; )) ), #Need to use shinyjs within the ui useShinyjs(), #Six different Shiny inputs, each of which will need to be filled out for the #program to run successfully fluidRow( column(6, numericInput(inputId = &quot;input1&quot;, label = labelMandatory(label = &quot;Numeric Input #1&quot;, id = &quot;input1&quot;), value = NULL), dateInput(inputId = &quot;input2&quot;, value = NA, label = labelMandatory(label = &quot;Date Input #1&quot;, id = &quot;input2&quot;)), textInput(inputId = &quot;input3&quot;, label = labelMandatory(label = &quot;Text Input #1&quot;, id = &quot;input3&quot;)) ), column(6, radioButtons(inputId = &quot;input4&quot;,inline = TRUE, choices = 1:4, label = labelMandatory(label = &quot;Radio Buttons #1&quot;, id = &quot;input4&quot;), selected = character()), textInput(inputId = &quot;input5&quot;, label = labelMandatory(label = &quot;Text Input #2&quot;, id = &quot;input5&quot;)), numericInput(inputId = &quot;input6&quot;, label = labelMandatory(label = &quot;Numeric Input #2&quot;, id = &quot;input6&quot;), value = NULL), ) ), #An actionButton, used for demonstration purposes. When clicked, it will #trigger the shinyjs command to hide or show the asterisk in the text input #label actionButton(&quot;goButton&quot;,&quot;Check if Number is Even&quot;) )#End of fluidPage server &lt;- function(input, output, session) { #Create a function that can ensure all require shiny inputs have been entered. #If some are missing, return their inputIds in a list QCFunction &lt;- function(input){ #Error list will hold the inputIds of any shiny inputs that failed to pass #the QC errorList &lt;- character() #Err will be TRUE if the QC finds any issues with the user inputs, which in #this case occurs when any shinyInput has not been entered err &lt;- FALSE #A vector of the element ids of the various ShinyInputs shinyInputs &lt;- c(input$input1,input$input2,input$input3, input$input4,input$input5,input$input6) shinyInputIds &lt;- paste0(&quot;input&quot;,1:6) #Check to see if any of the inputs are not &quot;truthy&quot;, which is to say that #they generally have not been entered or have not been entered with useable #information. Not perfect, but useful for demonstrations here. areFalsey &lt;- !mapply(FUN = isTruthy,shinyInputs) #If any of the inputs are falsey, the program should fail to run err &lt;- any(areFalsey) if(err){ showNotification(type = &quot;error&quot;,&quot;One of the inputs has not been entered.&quot;) #Set the errorList to contain the names of any of the inputs that failed #the QC (e.g. those that were falsey) errorList &lt;- shinyInputIds[areFalsey] } return(list( err = err, errorList = errorList )) } #Create a reactiveVal to store the results of errorList. reactiveVal&#39;s and #reactiveValue&#39;s are the only global variables in Shiny apps that can pass #reactive data from one portion of the app to the next errorList &lt;- reactiveVal(character()) #When the user clicks goButton, run the program QC to generate the errorList. #if the QC is passed, show a successful message. Otherwise, do nothing observeEvent(input$goButton,{ QC &lt;- QCFunction(input) #If QC$err is FALSE, no issues were encountered in the QC function. Show a #successful message. Otherwise, update errorList if(!QC$err){ showNotification(type = &quot;message&quot;,&quot;All inputs entered, program complete!&quot;) #QC was passed, reset errorList() reactiveVal errorList(character()) }else{ #If the QC fails, store the errorList output from the QC function within #the reactiveVal for use throughout the shiny server code errorList(QC$errorList) } })#End of observeEvent #Each time the errorList reactiveVal changes, show the mandatory stars next to #any Shiny inputs that the user entered incorrectly observeEvent(errorList(),ignoreInit = TRUE,{ #First, hide all mandatory stars. This ensures the fields marked #mandatory are reset in between each press of input$comp hideElement(selector=&quot;span.mandatory_star&quot;) #Now, find all input IDs that are TRUE within errorList(). These will #need to be marked as mandatory inputIDs &lt;- errorList() #Create the span ID spanInputIDs &lt;- paste0(inputIDs,&quot;-span&quot;) #Use shinyjs::showElement to mark each input as mandatory based on the #spanIDs that were created by labelMandatory() mapply(FUN = showElement, id = spanInputIDs) }) }#End of Server function #Call Shiny App shinyApp(ui, server) More Coming Soon! "],["markdown-and-user-friendly-output-files.html", "4.3 Markdown and User-Friendly Output Files", " 4.3 Markdown and User-Friendly Output Files Coming Soon! "],["bestPractices.html", "Chapter 5 Best Practices", " Chapter 5 Best Practices All programming languages have certain best practices that should be upheld to ensure users are upholding established procedures that are deemed most effective for interaction with said programming language. R is no stranger to rules around best practices. Below are a few resources to help familiarize yourself with some common best practices for R (many of which are universal to computing environments in general). Data Carpentry Example Best Practices USGS Water Data Example Best Practices R Bloggers Example Best Practices Best Coding Practices for R This is a whole book of best practices! The links provided are by no means comprehensive; best practices change over time and can be altered depending on use cases. Here at DEQ, we have identified a few best practices that we use daily to better manage our projects and day to day interactions with the R environment. Again, the following articles are by no means comprehensive of all best practices employed within the agency. "],["versionControl.html", "5.1 Version Control", " 5.1 Version Control Section Contacts: Emma Jones (emma.jones@deq.virginia.gov) &amp; Joe Famularo (joe.famularo@deq.virginia.gov) 5.1.1 What is Version Control? Version control is essentially a method for tracking all the changes that occur to any saved code. It is also a very effective way of collaborating with other programmers/developers on projects. No worries if that idea still is not entirely clear. Lets go through an example in a non-programming environment to better explain this concept. Say you are working on writing a report with two colleagues. You start the process and get a full draft written and save the document as DraftReport.docx. Right before you email this version to your colleagues for their review you rethink how you want to present a paragraph. Well, shoot. You deleted the way you worded it when you were writing the draft. Okay, you rewrite that section and save your document as DraftReport1.docx and email it to your colleagues. Then, each colleague edits your draft and sends you back a document with tracked changes. These are named DraftReport2.docx and DraftReport_withEdits.docx. Great, now its up to you to manually go through each document and incorporate their edits. You save this edited version as DraftReport_withEdits2.docx. You give the report a little break so you can come back to it with fresh eyes. When you go back to the report, you struggle to remember which was the last version you were editing, at this point there are four version of this report in your directory (hey, at least you put all the reports in one project directory, thats a start!). You figure out which report to edit and decide to get smart about naming conventions. You edit the report and save it as FinalReport.docx and ship that off to your colleagues for a last review. Guess what? They each return you a report with edits and a new document name. We are at seven report versions, but you still need to condense these edits into one document. Thats document number eight, called FinalFinalReport.docx. You mean business now! You send this version to your manager for a super final review. They are very professional and send you back a report with edits named FinalReport.docx. Well shoot, if you bring that into your project directory then you will overwrite your existing FinalReport.docx, so you instead name this version FinalReport_myInitials.docx. After a bit more review, you are finally ready to share your report with the world, but what do we call such a document? Lets call the final final final report something special so future us can know what we published, EndAllToBeAllReport.docx. If you are keeping count, thats the tenth version of your report in the directory. But, of course you will know which version was the last version of the document if ever you need to review it again. I mean, you wrote the thing, right? Fast forward a year. You need to review the final report. You open the directory with your reports and start deciphering which version is the final final final version. Get the idea? I know, its painful to see our mistakes laid bare. We have all been there more times that we would like to admit. Version control can help solve most of these project management issues. By implementing a version control strategy into the above example workflow, you could easily refer or revert back to previous versions of your document (commit), see line by line changes made at each step of your process (diff), maintain one main version of your document with easy ways to incorporate changes from your colleagues (branches, merge, issues), and share the whole project and project history with colleagues in case they need to know what happened step-by-step during the development process (repository). 5.1.1.1 What type of Version Control? There are many ways to use version control, but we will focus just on Git and DEQs GitLab in this article. In short, Git is a software used to track file changes and GitLab is a platform that allows you to host projects in a secure location to track changes (read: this is a backup of your local system). GitHub is a free, open source version of GitLab that is open to the world. We at DEQ use GitLab for more secure version control system behind our firewall (you need to be on VPN to access GitLab). 5.1.2 Git Basics There are plenty of tutorials that can get you going with Git. Instead of recreating the wheel, we recommend reviewing these links to get started with Git. RWorkflow: Intro to Git and GitHub Lecture Video Data Carpentry: A quick introduction to Git and GitHub useR 2016 Conference Tutorial Bookdown Reference: Happy Git and GitHub for the useR Note: the tutorial author Jenny Bryan is an R legend, her materials are excellent resources! 5.1.2.1 How to Connect To DEQs GitLab First, you need an account. Unlike GitHub, you need an administrator to create a GitLab account for you. Please email Connor Brogan (connor.brogan@deq.virginia.gov), Joe Famularo (joe.famularo@deq.virginia.gov), and Emma Jones (emma.jones@deq.virginia.gov) to get this process started. Next, you will need to set up a secure connection to the GitLab environment to your local environment via ssh. This guide goes into many details, the highlights are shown below. Please consult the guide before reaching out to OIS staff for help troubleshooting your connection. See if you already have an existing SSH key pair If not, Generate an SSH key pair Add SSH key to your GitLab account Verify that you can connect A successful connection will look like this: 5.1.2.2 How to Implement Git in your Workflow Below we will detail two ways of using Git in your normal workflow. These include Bash (command line; read: more control, larger learning curve) and using an RStudio GUI (Graphical User Interface in the IDE- Integrated Development Environment; read: much more user-friendly at the start). Neither method is more correct than the other. Do whatever you feel most comfortable with and just use it! 5.1.2.3 Bash Method: For Users that like a Challenge First, you need to create a local directory where you will house all your local projects that you will link to GitLab. Below is an example file structure. Then, in GitLab, create a new repository for your project. Always add a README file and gitignore. Next, you need to get this remote repository to your local environment so you can actually do stuff. In GitLab in a browser window, navigate to the new repository and locate the Clone drop down button. Click the copy button next to Clone with SSH. Keep this on your clipboard. Right click in your GitLab directory in your Windows File Explorer program. Click on Git Bash Here to open a terminal window in your current GitLab directory. Use the following script to clone the remote repository to your local environment. Pro Tip: The Ctrl+V paste shortcut doesnt work in a command prompt. Use Shift+Insert instead to paste the link from your clipboard or right click the line you are editing and select Paste. git clone [link from GitLab] # Example #git clone git@gitlab.deq.virginia.gov:deq_water/wqm/testproject.git You were successful if you now have a directory in your GitLab directory named the same as your GitLab repo. Open the new directory and start working. When you are ready to make your first commit to the remote repository, right click in your Windows File Explorer window and choose Git Bash Here to open a command prompt in your current location. Stage updates, commit, push, pull, etc. using the following script examples. # Stage all files (not excluded in the gitignore) for commit git add -A # Commit all files staged and add a commit message git commit -a -m &#39;update readme&#39; # Push commits to remote repo (main branch) git push origin main This is not a comprehensive list of all git commands one could or should use. Simple google queries can answer most questions on using bash with Git. 5.1.2.4 RStudio Method: Point and Click Magic Cloning a git repository in RStudio is relatively easy and may be a good place to start if youre new to using version control. Open RStudio and navigate to File &gt; New Project Select Version Control. Select Git. You should see a screen with inputs for Repository URL, Project directory name, and Create project as a subdirectory of. Now youll need to provide the repository URL from GitLab. This is a common step between the RStudio method and the Bash method described above. Navigate to your repository and select the Clone drop down button. Click the copy button next to Clone with SSH. Paste this in the Repository URL input in RStudio. The project directory name should be automatically populated with the repository name. Navigate to the location where you want this repository to rest using the Browse button Select Create Project. If successful, RStudio will open your new project. Thats it! Youre repository is now cloned in your local environment and you can begin writing code. Open this project as you would any other and you should see a Git tab where your Environment, History, Connectitons, etc. are located. To commit, push, or pull your code navigate to this Git tab. Select commit and the Review Changes window will open. Here you can compare changes from your last commit, add a commit message, select the files youd like to commit, and also push or pull your code. "],["miscTips.html", "Chapter 6 Miscellaeous Tips and Tricks ", " Chapter 6 Miscellaeous Tips and Tricks "],["regular-expression-and-pattern-matching.html", "6.1 Regular expression and pattern matching", " 6.1 Regular expression and pattern matching Section Contact: Connor Brogan (connor.brogan@deq.virginia.gov) Lets imagine a scenario where we have a series of water quality data that is filled with manually inputted character fields and notes. Although the notes are consistent in their structure, they arent easily read by R. For instance, we may have an entire column of field notes that look like Flow was 98 cfs. Temperature was 20 deg C. Recorded by LL. We need to get the flow, temperature, and field scientist from all 1200 records. Obviously, it would take a lot of time to break this apart manually. So how do we get R to find the data we need? The answer lies in regular expressions. Regular expression (often short-handed as Regex) is a method of matching patterns in string (or, in R, character) type data. Regular expression is powerful and mastering it can greatly enhance your applications and tools. You will be able to reliably find dates, flow values, station names, and more from long, poorly formatted data. However, regular expression is built on specific sequence structures that are different from typical R syntax. It is not a formal language, but is instead a series of key words and quantifiers. Because of this, R will not output any warnings or errors when working with regex. And in this way developing the right regular expression for your application can be extremely vexing. You will either create the perfect pattern to accomplish your goal, or you will not. So, if we get the pattern just right, we can extract 98 as the streamflow from the string Flow was 98 cfs. Temperature was 20 deg C. Recorded by LL. But if we mess it up, we may end up with Flow was 98 or 98 cfs. Temperature was 20 deg C. Recorded by LL. and R wont give us any information as to why. This section has been written to cover basic regular expression structure and key words. It concludes with a few examples of patterns that are commonly needed when parsing water quality data. Additional resources are listed at the end of the chapter. 6.1.1 Example Data Overview For this chapter, the following dataset will be used to demonstrate how we can use Regular Expression to extract data from long, complicated, or inconsistent character data. Lets read in the data and see examine what we are working with. A copy of this dataset is available here: library(tidyverse) library(DT) FieldData &lt;- read_csv(myDataPath,col_types = cols()) datatable(FieldData,rownames = F) This is a messy dataset. Notice that all three fields are character. The dates were recorded as full sentences. The FieldNotes column contains a lot of useful data, but it is all mashed together in one string. The streamflow is recorded as either MGD, cfs, or gpm. Some notes include the ambient air temperature, but others do not. The observations were taken by many different people. Finally, it appears that streamflow was adjusted by a constant scaling factor, but again the factor was recorded within a full sentence in the FlowAdjustment column. We want to extract the day of the week each sample was taken, the date, the flow of the stream, the water temperature, and the flow scaling factor. To do this, we are going to need to understand regex structure and how to apply it in R. We will first learn a couple of R commands that take regex as an input and then we will dive into writing our own regex statements to manipulate our dataset FieldData. 6.1.2 Common R Commands and How to Use Them We will cover what commands that find matching patterns, that extract or replace matching patterns, and those that return logical vectors for matching patterns. We will demonstrate both Tidyverse and Base R functions. 6.1.2.1 Tidyverse There are three main functions available in the stringr library that are extremely useful for parsing string data. Many other functions exist within stringr that can speed up more complicated analyses so it may be worth reviewing the basic documentation of stringr. The function str_detect will tell us if our entries contain a pattern or not. It will return a logical vector that we can use to assess which data entries match our pattern. It takes a vector of strings and a pattern to be matched as input. For now, lets use a basic pattern. We can search for all dates that happened on Friday by simply trying to match Friday. haveFridays &lt;- FieldData %&gt;% select(Date) %&gt;% unlist() %&gt;% str_detect(&quot;Friday&quot;) head(haveFridays) ## [1] FALSE FALSE FALSE FALSE FALSE TRUE FieldData %&gt;% filter(haveFridays) %&gt;% datatable(rownames=F,options=list(lengthMenu = list(c(3,5,10,-1),c(&quot;3&quot;,&quot;5&quot;,&quot;10&quot;,&quot;All&quot;)))) In the previous example we used str_detect to create a logical vector that was TRUE for all data within the Date column that contained the word Friday. What if we wanted to find out which entries contained the word Friday? In that case, we can use str_which. haveFridays &lt;- FieldData %&gt;% select(Date) %&gt;% unlist() %&gt;% str_which(&quot;Friday&quot;) head(haveFridays) ## [1] 6 17 27 30 31 50 datatable(FieldData[haveFridays,],options=list(lengthMenu = list(c(3,5,10,-1),c(&quot;3&quot;,&quot;5&quot;,&quot;10&quot;,&quot;All&quot;))),rownames=F) Here, the vector haveFridays contains the index of each entry that contains the word Friday. In some applications, this vector can be extremely useful. For instance, we can find every data point that immediately follows one containing Friday by doing: haveFridaysData &lt;- FieldData[(haveFridays+1),] head(haveFridaysData) %&gt;% datatable(rownames=F,options=list(lengthMenu = list(c(3,5,10,-1),c(&quot;3&quot;,&quot;5&quot;,&quot;10&quot;,&quot;All&quot;)))) Both str_detect and str_which perform similar tasks. They both match a pattern and return, in some form, a list of vectors matching the pattern. But, what if we want to extract or replace the matched pattern? In our example dataset, we know there was a computer error and all entries signed LWO are meant to read WLO. errataData &lt;- FieldData %&gt;% select(FieldNotes) %&gt;% unlist() %&gt;% str_which(&quot;LWO&quot;) datatable(FieldData[errataData,],options = list(dom = &#39;t&#39;),rownames=F) We can quickly find this pattern and replace it with the correct one using str_replace, which takes a vector of character data, a pattern to replace, and the data to replace it with: FieldData[errataData,] %&gt;% select(FieldNotes) %&gt;% unlist() %&gt;% str_replace(&quot;LWO&quot;,&quot;WLO&quot;) ## [1] &quot;Streamflow is 0.2 gpm. Ambient Air Temperature was 28 deg C. Temperature was 28.9 deg C. Recorded by WLO.&quot; ## [2] &quot;flow was 102.5 cfs. Ambient Air Temperature was 28 deg C. Temperature was 25.1 deg C. Recorded by WLO.&quot; ## [3] &quot;Stream flow at 0.2 gpm. Wadable conditions. Temperature was 29.8 deg C. Recorded by WLO.&quot; As we learn to detect patterns using Regex, we will continually return to these three commands. 6.1.2.2 Base R There are three main functions available in base R that are extremely useful for parsing string data. The function grepl will tell us if our entries contain a pattern or not. It will return a logical vector that we can use to assess which data entries match our pattern. It takes a vector of strings and a pattern to be matched as input. For now, lets use a basic pattern. We can search for all dates that happened on Friday by simply trying to match Friday. haveFridays &lt;- grepl(&quot;Friday&quot;,FieldData$Date) head(haveFridays) ## [1] FALSE FALSE FALSE FALSE FALSE TRUE datatable(FieldData[haveFridays,],options=list(lengthMenu = list(c(3,5,10,-1),c(&quot;3&quot;,&quot;5&quot;,&quot;10&quot;,&quot;All&quot;))),rownames=F) In the previous example we used grepl to create a logical vector that was TRUE for all data within the Date column that contained the word Friday. What if we wanted to find out which entries contained the word Friday? In that case, we can use grep. haveFridays &lt;- grep(&quot;Friday&quot;,FieldData$Date) head(haveFridays) ## [1] 6 17 27 30 31 50 datatable(FieldData[haveFridays,],options=list(lengthMenu = list(c(3,5,10,-1),c(&quot;3&quot;,&quot;5&quot;,&quot;10&quot;,&quot;All&quot;))),rownames=F) Here, the vector haveFridays contains the index of each entry that contains the word Friday. In some applications, this vector can be extremely useful. For instance, we can find every data point that immediately follows one containing Friday by doing: haveFridaysData &lt;- FieldData[(haveFridays+1),] datatable(head(haveFridaysData),rownames=F,options=list(lengthMenu = list(c(3,5,10,-1),c(&quot;3&quot;,&quot;5&quot;,&quot;10&quot;,&quot;All&quot;)))) Both grepl and grep perform similar tasks. They both match a pattern and return, in some form, a list of vectors matching the pattern. But, what if we want to extract or replace the matched pattern? In our example dataset, we know there was a computer error and all entries signed LWO are meant to read WLO. errataData &lt;- grep(&quot;LWO&quot;,FieldData$FieldNotes) datatable(FieldData[errataData,],options = list(dom = &#39;t&#39;),rownames=F) We can quickly find this pattern and replace it with the correct one using gsub, which takes a vector of character data, a pattern to replace, and the data to replace it with: gsub(&quot;LWO&quot;,&quot;WLO&quot;,FieldData$FieldNotes[errataData]) ## [1] &quot;Streamflow is 0.2 gpm. Ambient Air Temperature was 28 deg C. Temperature was 28.9 deg C. Recorded by WLO.&quot; ## [2] &quot;flow was 102.5 cfs. Ambient Air Temperature was 28 deg C. Temperature was 25.1 deg C. Recorded by WLO.&quot; ## [3] &quot;Stream flow at 0.2 gpm. Wadable conditions. Temperature was 29.8 deg C. Recorded by WLO.&quot; As we learn to detect patterns using Regex, we will continually return to these three commands. 6.1.3 Basic Regular Expressions As we saw above in our demonstrations of the core string matching commands, we can use words as the pattern to be matched. We can also use whole phrases. Below, note that grep() finds three entries containing Friday but only the one when we search for It is Friday. And it cant detect the last entry because it is searching for the whole pattern Friday and the last two are broken up by punctuation or capitalized. Regular expression is extremely case sensitive! notes &lt;- c( &quot;It is Friday today, hooray&quot;, &quot;It will never be Friday&quot;, &quot;Is today Friday or Frie-day?&quot;, &quot;I like mine with lettuce and tomato&quot;, &quot;F.r.i.d.a.y.&quot;, &quot;FRIDAY&quot; ) grep(&quot;Friday&quot;,notes) ## [1] 1 2 3 grep(&quot;It is Friday&quot;,notes) ## [1] 1 We can search for multiple phrases at the same time using the pipe symbol |: grep(&quot;Friday|FRIDAY&quot;,notes) ## [1] 1 2 3 6 grep(&quot;It is Friday|never be Friday&quot;,notes) ## [1] 1 2 6.1.4 Character Classes When it comes to crafting regex expressions, there are a couple of characters and character classes that are essential to matching complex character data. We can use character classes to quickly extract basic data from long strings. For simple data extraction, there are often a number of solutions to get at the data we want. Returning to our example, we can easily obtain the flow scaling factor from the FlowAdjustment column of FieldData by using character classes: #Here, we remove everything that is NOT a digit or a literal &quot;.&quot; gsub(&quot;[^\\\\.0-9]&quot;,&quot;&quot;,FieldData$FlowAdjustment[1:3]) ## [1] &quot;0.3&quot; &quot;0.9&quot; &quot;0.1&quot; #Here, we remove everything that is an alphabetic character or a blank space gsub(&quot;[[:alpha:]]|[[:space:]]&quot;,&quot;&quot;,FieldData$FlowAdjustment[1:3]) ## [1] &quot;0.3&quot; &quot;0.9&quot; &quot;0.1&quot; With our example data, we can use character classes to get the date if we are careful about what string we extract. #Here, we remove everything that is not the digits 0-9 as well as the &#39;dash&#39; symbol gsub(&quot;[^0-9\\\\-]&quot;,&quot;&quot;,FieldData$Date[1:3]) ## [1] &quot;2021-07-28&quot; &quot;2019-03-27&quot; &quot;2018-06-05&quot; #Here, we remove everything that is an alphabetic character or a blank space gsub(&quot;[[:alpha:]]|[[:space:]]|,&quot;,&quot;&quot;,FieldData$Date[1:3]) ## [1] &quot;2021-07-28&quot; &quot;2019-03-27&quot; &quot;2018-06-05&quot; So, we can easily use character classes to find the date and flow adjustment within our dataset. For simple data extraction, character classes provide an easy way to find data within long strings. #We can add the date and flow adjustment as a new columns to FieldData FieldData$ActualDate &lt;- as.Date(gsub(&quot;[^0-9\\\\-]&quot;,&quot;&quot;,FieldData$Date)) #Here, we remove everything that is NOT a digit or a literal &quot;.&quot; FieldData$AdjustmentRatio &lt;- as.numeric(gsub(&quot;[^\\\\.0-9]&quot;,&quot;&quot;,FieldData$FlowAdjustment)) datatable(FieldData,rownames=F,options=list(lengthMenu = list(c(5,10,-1),c(&quot;5&quot;,&quot;10&quot;,&quot;All&quot;)))) 6.1.5 Quantifiers As we saw above, character classes can be used in regex to easily extract data that stands out from the rest of the string. We can use them to find the date within a sentence or we can get the single number from a statement. However, they are not useful by themselves when there is mixed data within a single string.For instance, if we extract all numbers from field notes, we can a conglomerate that is not easy to understand. #We can&#39;t find the stream temperature just by extracting all digits: FieldData$FieldNotes[1:3] ## [1] &quot;Streamflow is 107.7 cfs. Wadable conditions. Temperature was 24.3 deg C. Recorded by EFA.&quot; ## [2] &quot;flow was 100 cfs. Wadable conditions. Temperature was 29 deg C. Recorded by MAB.&quot; ## [3] &quot;Streamflow is 135.8 MGD. Ambient Air Temperature was 28 deg C. Temperature was 28.4 deg C. Recorded by GVN.&quot; gsub(&quot;\\\\D&quot;,&quot;&quot;,FieldData$FieldNotes[1:3]) ## [1] &quot;1077243&quot; &quot;10029&quot; &quot;135828284&quot; To fix this problem, regex has a structure of quantifiers that allows us to search for particular patterns after some number of characters or previous matches. Regex always matches from left to right and takes the last possible match (or, in regex terms, it is greedy), so these quantifiers can help us find the second number in a sentence, for instance. We can use quantifiers on a character or a group of characters, as indicated by a set of parentheses. You will notice that in the table below, several of the quantifiers return the same value. Quantifiers are often used in tandem with anchors, which can be used to refine where in a string a pattern is expect. See Anchors for more information. dataToMatch&lt;-c(&#39;Q=4250 cfs&#39;,&#39;Q=210 cfs&#39;,&#39;Q=10 cfs&#39;,&#39;Q=45980 cfs&#39;,&#39;Q=0 cfs&#39;) Lets see if we can use quantifiers to address our example problem. We used character classes Character Classes to find the date and the flow adjustment ratio. We may be able to use quantifiers to find the stream flow, unit, water temperature, and field scientist. We will need to rely on groups of characters using parentheses. These groups will help us define a pattern and can also be called by gsub to return only the matched characters: #If we replace any number of characters that repeat any number of times and are followed #by &#39;Recorded by &#39; and replace all periods with &#39;&#39;, then we get the field scientist FieldData$FieldScientist&lt;- gsub(&quot;.*Recorded by |\\\\.&quot;,&quot;&quot;,FieldData$FieldNotes) head(FieldData$FieldScientist) ## [1] &quot;EFA&quot; &quot;MAB&quot; &quot;GVN&quot; &quot;LPX&quot; &quot;EHK&quot; &quot;NHL&quot; We cant find temperature or flow using this strategy because air temperature and the different flow units makes this challenging. We would instead need to write a long statement with a bunch of ors. This will work, but it is not ideal. The following statement looks for Stream flow, Streamflow, and flow by using the ? quantifier and also looks for is, was, or at. It also matches the streamflow units, followed by any number of any character. gsub replaces these with zero, leaving only the streamflow gsub(&quot;((Stream)? ?flow) (is|was|at) | (cfs|MGD|gpm).*&quot;,&quot;&quot;,FieldData$FieldNotes[1:10]) ## [1] &quot;107.7&quot; &quot;100&quot; &quot;135.8&quot; &quot;164.5&quot; &quot;0.2&quot; &quot;123.1&quot; &quot;0.2&quot; &quot;96.3&quot; &quot;91.4&quot; ## [10] &quot;134.1&quot; Similar to the example above, we can use groups to find the stream temperature but it will take a lot of or statements and you will need to be very familiar with the exact format of every string of data (e.g. in the above example, we knew in advance there were several units and several ways to write stream flow). We can get around this using anchors, in combination with numeric quantifiers. 6.1.6 Anchors Anchors are use to specify the location of a sequence of characters within a string. They are used to match patterns that occur at the beginning or end of a string or word. Used in combination with quantifiers and character classes, anchors can help match any kind of pattern within a string. dataToMatch &lt;- c(&quot;streamflow = 20 cfs in february&quot;,&quot;No streamflow recorded in january&quot;,&quot;No streamflow recorded in april by Eujane&quot;,&quot;flow = 26 cfs in jan&quot;) Now, we can set about finding the stream flow, water temperature, and the day of the week the sample was taken from our example data using anchors, character classes, quantifiers, and groups. Note that we can return a group by using gsub and enter \\\\1 as the replacement vector! This is useful because we can build the group into a pattern that will match the whole string and then replace it with just the group. Effectively, this will allow us to easily capture groups if we match the whole string! #WEEKDAY: #Here, we replace &#39;Date Assessed &#39;, the comma, and everything after the comma with the #group consisting of any character, any number of times. Because we are familiar with the #data string, we know this will capture the weekday! Note that we use &quot;\\\\1&quot; to return the group #in the parentheses. Our pattern &quot;Date Assessed (.*),.*&quot; matches the ENTIRE string so we are #using gsub to replace the whole string with just the group in the parentheses! FieldData$Weekday &lt;- gsub(&quot;Date Assessed (.*),.*&quot;,&quot;\\\\1&quot;,FieldData$Date) #We could alternatively replace everything that is not in the area we expect the weekday to be FieldData$Weekday &lt;- gsub(&quot;(Date Assessed )|(,.*)&quot;,&quot;&quot;,FieldData$Date) #Or we could just search for each individual day of the week and return it! #Here we list each weekday in the following form Monday|Tuesday|...|Sunday which allows us #to look for any day of the week. But we will need this to be in a group and to #remove any character any number of times before and after the day of the week WeekdaySearch&lt;-paste(weekdays((Sys.Date()+0:6)),collapse=&quot;|&quot;) FieldData$Weekday &lt;- gsub(paste0(&quot;.*(&quot;,WeekdaySearch,&quot;).*&quot;),&quot;\\\\1&quot;,FieldData$Date) #FLOW # This statement is a little complicated, so let&#39;s break it down into it&#39;s pieces. # The gsub command will look for the pattern and replace it with the first group (as indicated # by the &quot;\\\\1&quot;!) If our pattern matches the whole string, we can replace it all with our group! #The pattern consists of: #&quot;.* &quot; = look for any character any number of times until a literal space #&quot;(\\\\d*\\\\.*\\\\d* \\\\w*)\\\\.&quot; = look any number of digits, followed by zero or more periods, #followed by any number of digits, a space, adn then any number of word characters that are #followed by a period. This will match our streamflow with unit! #&quot;.*(Temperature.*\\\\d*\\\\.*\\\\d*)?&quot; = look for any number of any character follow by zero or one #instances of &#39;Temperature&#39; followed by some characters and another digit, decimal, digit #combination. #&quot;.*Temperature.*\\\\d+(\\\\.)?\\\\d* deg C.* = Look for the temperature followed by some number of any #character followed by a digit, a potential decimal, another 0 or more digits, and deg C, followed #by any character any number of times FieldData$Flow &lt;- gsub(&quot;.* (\\\\d*\\\\.*\\\\d* \\\\w*)\\\\..*(Temperature.*\\\\d*\\\\.*\\\\d*)?.*Temperature.*\\\\d+(\\\\.)?\\\\d* deg C.*&quot;,&quot;\\\\1&quot;,FieldData$FieldNotes) #Stream Temperature: #The gsub command below will find the pattern and replace it with the first group (as indicated by #the &quot;\\\\1&quot;!) The pattern begins with any character for any number of times followed by a literal #space. The group (to be returned by &quot;\\\\1&quot;) is any digit one or more times, potentially followed #by a period and any number of digits. After the group, the pattern will match any character any #number of times.This pattern exactly matches the entire Field notes column and replaces it with #group 1, returned by gsub FieldData$WaterTemperature &lt;- as.numeric(gsub(&quot;.* (\\\\d+\\\\.*\\\\d*).*&quot;,&quot;\\\\1&quot;,FieldData$FieldNotes)) FieldData_clean &lt;- FieldData[,c(4,7,6,8,9,5)] datatable(FieldData_clean,rownames=F,caption=&quot;Field Data Modifed with Regular Expression&quot;) 6.1.7 Common or Useful Patterns Below is a common set of regrex commands to use for day to day tasks: The following command escapes all special regex characters from a string. This can be built into a function and used to validate user-input strings in a Shiny app! inputString&lt;-&quot;Now we have learned regex\\\\Regular Expression. What&#39;s Next? Application$&quot; gsub(&quot;([.|()\\\\^{}+$*?]|\\\\[|\\\\])&quot;, &quot;\\\\\\\\\\\\1&quot;,inputString) ## [1] &quot;Now we have learned regex\\\\\\\\Regular Expression\\\\. What&#39;s Next\\\\? Application\\\\$&quot; #this command can be useful as a function to check Shiny input fields for illegal characters: escapeCharacters&lt;-function(inputString){ outputString&lt;-gsub(&quot;([.|()\\\\^{}+$*?]|\\\\[|\\\\])&quot;, &quot;\\\\\\\\\\\\1&quot;,inputString) } To get the day, month, or year from a standard date string, we can use a simple gsub command! inputString&lt;-&quot;04/05/2063&quot; gsub(&quot;\\\\d+/(\\\\d+)/\\\\d+&quot;,&quot;\\\\1&quot;,inputString)#Day = Replace the whole pattern with the month ## [1] &quot;05&quot; gsub(&quot;/.*&quot;,&quot;&quot;,inputString)#Month = Replace everything after the first &quot;/&quot; ## [1] &quot;04&quot; gsub(&quot;^.*/&quot;,&quot;&quot;,inputString)#Year = Replace everything up until the last &quot;/&quot; ## [1] &quot;2063&quot; To get a file type from a given directory or to check the file type against an expected value: inputString&lt;-&quot;C:/Users/JTKirk/Desktop/SpO_CK.csv&quot; #Is the input file path of the correct file type? Check everything after last &quot;.&quot; gsub(&quot;.*(\\\\..*)$&quot;,&quot;\\\\1&quot;,inputString) ## [1] &quot;.csv&quot; grepl(&quot;.csv$&quot;,inputString) ## [1] TRUE We can use the . character class to check for empty strings: inputString&lt;-&quot;&quot; #Is the input string empty? Search to see if string contains any character grepl(&quot;.&quot;,inputString) ## [1] FALSE We can use gsub to find whole numbers and decimal numbers from a large numeric data string: inputString&lt;-&quot;23.952 25 26.859 98 -101 58.582&quot; #Get whole numbers or decimal numbers by checking for digits surrounding a literal &quot;.&quot; gsub(&quot;(-?\\\\d*\\\\.\\\\d* ?)&quot;,&quot;&quot;,inputString) ## [1] &quot;25 98 -101 &quot; #Decimal numbers are trickier, and its best to find them by breaking the string #into its component numbers: AllNumbers&lt;-strsplit(inputString,&quot; &quot;) AllNumbers&lt;-unlist(AllNumbers) #After splitting the string, we can find those with/without a decimal point AllNumbers[grepl(&quot;\\\\.&quot;,AllNumbers)]#Find all numbers containing a literal &quot;.&quot; ## [1] &quot;23.952&quot; &quot;26.859&quot; &quot;58.582&quot; wholeNumbers&lt;-AllNumbers[!grepl(&quot;\\\\.&quot;,AllNumbers)] 6.1.8 Additional Resources R help menu, e.g. ?regex https://ryanstutorials.net/regular-expressions-tutorial/regular-expressions-cheat-sheet.php https://www.tutorialspoint.com/vb.net/vb.net_regular_expressions.htm https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285 "],["non-standard-evaluation.html", "6.2 Non standard evaluation", " 6.2 Non standard evaluation "],["accessAPIfromR.html", "6.3 Accessing APIs from R", " 6.3 Accessing APIs from R Section Contact: Emma Jones (emma.jones@deq.virginia.gov) 6.3.1 What is an API Whether or not you realize it, you have been using application programming interfaces, or APIs, for years. APIs are a set of rules for building and integrating software so computer services and servers can communicate and share information efficiently. APIs are where users go to pull data from various entities. That said, some APIs are well documented and easily explain how users might access open data (e.g. the USGS StreamStats Service Documentation). However, many entities that make data available via an API dont have the resources to maintain robust documentation, but valuable data can still be acquired with a bit of thought and data exploration (and politely emailing the web service maintainer if all else fails!). 6.3.2 CMC API Use Case This example will overview the process of using R to scrape or acquire data from (at the time of writing) a less documented API. The Chesapeake Monitoring Cooperative (CMC) coordinates with citizen monitoring groups to share data resources collected from the Chesapeake Bay watershed. Thanks to a grant, DEQ and the CMC are expanding data resources to incorporate non-Bay citizen data into the database such that DEQ may point citizen groups throughout Virginia to a single data entry portal. This expansion will allow DEQ to pull all citizen monitoring data conducted for biannual Integrated Reports from one location in one data format. By automating the process of scraping the CMC API, DEQ staff will benefit from significant time savings acquiring and organizing data, as well as tagging data a different citizen monitoring data levels or tiers. 6.3.3 Required Packages In order to scrape APIs, you will need the following R packages installed and loaded into your environment. library(tidyverse) library(rmarkdown) # just for pretty tables in this document library(httr) library(jsonlite) 6.3.4 Explore the endpoint in a browser Like all good data analysis tasks, we must first understand what data are available and how they are formated before operating on the data. The CMC API is available off the odata endpoint. We can view the data available by pasting this URL into a browser window: https://cmc.vims.edu/odata/ . This tells us about the structure of the database and the names of the different datasets available from the API. It is helpful to explore a few datasets to understand what data are contained where and how to connect datasets. To demonstrate how to explore a dataset, we will first show the browser method and then how to bring back the data into R. Lets investigate the first dataset called Groups. To do so in a browser, we would simply augment our original URL to include Groups endpoint, e.g. https://cmc.vims.edu/odata/Groups . This returns a json dataset that is somewhat easy to understand, but lets pull it back into our R environment to make data exploration even easier. 6.3.5 Query API with R To use R to perform the same query, we need to feed the same URL from above into httr::GET(). If you were to print the results of the CMCgroupsQuery object you will see metadata on when the operation was executed, the status (200 if it is successful), data returned (json format), and data size. This isnt super helpful for actually working with the data, so lets convert this response to an R object. The nested functions fromJSON(rawToChar(CMCgroupsQuery$content))$value unpack the json data returned from the API endpoint into a data format you may be more familiar with. You can explore this step on your own and dig into more articles that explain the ins and outs of APIs and webscraping with R. CMCgroupsQuery &lt;- GET(&quot;https://cmc.vims.edu/odata/Groups&quot;) CMCgroups &lt;- fromJSON(rawToChar(CMCgroupsQuery$content))$value CMCgroups %&gt;% datatable(rownames = F, options = list(dom = &#39;lftip&#39;, pageLength = 5, scrollX = TRUE)) Thats much better. We can now see this dataset contains information about the individual citizen groups that provide data to the CMC database. The Id field will be important when we want to query data from groups from the Samples dataset (see above). But first, lets build our first real query to the database. As with all databases, it is best practice to ask the database itself to perform the data simplification instead of bringing back unnecessary data. Lets identify all groups that are based in Virginia. To do so, we will add a filter statement to our URL and limit the data pulled back to just rows with State = VA. You can test this in a browser with the following URL: https://cmc.vims.edu/odata/Groups?$filter=State eq Va . Now lets try that same call with R. You will immediately notice a lot of wonky characters in this URL. When you send spaces or  to the browser, these will be converted to the appropriate character string for you. Note \"\" will return an error, so use . VAgroupsQuery &lt;- GET(&quot;https://cmc.vims.edu/odata/Groups?$filter=State%20eq%20%27Va%27&quot;) VAgroups &lt;- fromJSON(rawToChar(VAgroupsQuery$content))$value paged_table(VAgroups) Lets practice querying one more time on a different endpoint: Stations. This dataset contains all of the stations within the CMC database. For our task, we dont need any stations outside of Virginia, so lets only bring back stations within Virginia in this next query. The query will look like so: https://cmc.vims.edu/odata/Stations?$filter=State eq Virginia . VAstationsQuery &lt;- GET(&quot;https://cmc.vims.edu/odata/Stations?$filter=State%20eq%20%27Virginia%27&quot;) VAstations &lt;- fromJSON(rawToChar(VAstationsQuery$content))$value paged_table(VAstations) How did we know that in this dataset we needed to type out Virginia instead of Va like the above example? We first explored the Stations dataset in our browser to see how that dataset was structured, noticed the State field, and identified that each state name was spelled out completely. 6.3.6 Example: Query CMC API for Assessment Uses The problem outlined above will be completed using R in the following steps. We will use the previously created VAgroups object to identify which groups provided data in Virginia. Then we will query each of those GroupIds to query sample data within the assessment window of interest. Lastly, we will double check that all stations returned are in Virginia and pass necessary QA flags. First, lets make sure we can query data from the Sample endpoint. We can perform a very basic query by pulling back all the sample events collected by a certain group (e.g. 64) in the CMC database like so. Note the expand statement. This links the Samples dataset to the Parameter, Qualifier, Problem, and Event datasets in the database like a join would. By adding this expand step, we can avoid pulling each individual dataset into our environment and joining it to the relevant sample information. It is always best to ask the database to perform operations to minimize the amount of data returned into your environment. Note the filter statement is performed on the GroupId field from the joined in Event dataset. This field is represented in the dataset as Event.GroupId, but we need to communicate the . as a / in our URL. If you copy/paste the URL below into a browser, you will see the json dataset below returned. https://cmc.vims.edu/odata/Samples?$expand=Parameter,Qualifier,Problem,Event($expand=Station,Group)&amp;$filter=Event/GroupId eq 64 json sampleEventQuery &lt;- GET(&quot;https://cmc.vims.edu/odata/Samples?$expand=Parameter,Qualifier,Problem,Event($expand=Station,Group)&amp;$filter=Event/GroupId%20eq%2064&quot;) sampleEvent &lt;- fromJSON(rawToChar(sampleEventQuery$content))$value paged_table(sampleEvent) Now that we can query sample data by for one GroupId, lets only return data within a given temporal window. From this point on, we will parse URLs into more manageable pieces to make queries more understandable. See the Consuming GIS REST Services in R section for more information on parsing URL statements. Note that any date needs to be communicated in json datetime format. We are querying all samples collected by GroupId 64 from January 1, 2022 to March 15, 2022. # base URL for endpoint sampleParmeterQualifierEventGroupFilter &lt;- &quot;https://cmc.vims.edu/odata/Samples?$expand=Parameter,Qualifier,Event($expand=Station,Group)&amp;$filter=Event/GroupId%20eq%20&quot; # time window jsonFormatDateWindow &lt;- c(&quot;2022-01-01T00:00:00Z&quot;,&quot;2022-03-15T23:59:59Z&quot;) # One group groupId &lt;- 64 # just a helper and &lt;- &quot;%20and%20&quot; # Date filter statement inDateWindowFilter &lt;- paste0(&quot;Event/DateTime%20ge%20&quot;,jsonFormatDateWindow[1],and, &quot;Event/DateTime%20lt%20&quot;,jsonFormatDateWindow[2]) stationByDateFilter &lt;- fromJSON( rawToChar( GET( paste0(sampleParmeterQualifierEventGroupFilter, groupId, and, inDateWindowFilter) )$content))$value paged_table(stationByDateFilter) Now lets scale this solution to iterate through the GroupIds in Virginia (VAgroups) to compile a dataset of sample data within the 2022 Integrated Report data window (January 1, 2015 to December 31, 2020). We know that there are 37 unique GroupIds for Virginia, thats a lot of data to pull back at once. For this minimal example, we will only iterate through the first two groups, but the real assessment process would include all potential Virginia groups. We will demonstrate this using a for loop for clarity, but we offer that loops are slow and not best practices for iterating through analyses in a functional programming language like R. The better solution would be to build the operation into a function, but that is beyond the scope of this article. ### Establish all your important building blocks outside your loop # assessment period in a R format (for QA later) assessmentPeriod &lt;- as.POSIXct(c(&quot;2015-01-01 00:00:00 UTC&quot;,&quot;2020-12-31 23:59:59 UTC&quot;),tz=&#39;UTC&#39;) # assessment period in json format (for URL) jsonFormatAssessmentPeriod &lt;- c(&quot;2015-01-01T00:00:00Z&quot;,&quot;2020-12-31T23:59:59Z&quot;)# base URL for endpoint # base URL for endpoint sampleParmeterQualifierEventGroupFilter &lt;- &quot;https://cmc.vims.edu/odata/Samples?$expand=Parameter,Qualifier,Event($expand=Station,Group)&amp;$filter=Event/GroupId%20eq%20&quot; # just a helper and &lt;- &quot;%20and%20&quot; # Date filter statement inDateWindowFilter &lt;- paste0(&quot;Event/DateTime%20ge%20&quot;,jsonFormatAssessmentPeriod[1],and, &quot;Event/DateTime%20lt%20&quot;,jsonFormatAssessmentPeriod[2]) ## And provide a place to store output information dataOut &lt;- tibble() ## Now for the loop for (i in unique(VAgroups$Id)[1:2]){ stationByDateFilter &lt;- fromJSON( rawToChar( GET( paste0(sampleParmeterQualifierEventGroupFilter, i, and, inDateWindowFilter) )$content))$value dataOut &lt;- bind_rows(dataOut, stationByDateFilter) } # What was returned? glimpse(dataOut) ## Observations: 20,146 ## Variables: 17 ## $ Id &lt;int&gt; 21042, 21043, 21044, 21045, 21046, 21047, 21048, 21049... ## $ Value &lt;dbl&gt; 23.00, 8.60, 8.80, 130.00, 0.30, 120.00, 24.00, 25.20,... ## $ Depth &lt;dbl&gt; 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, NA, NA, NA, NA, NA,... ## $ SampleId &lt;int&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, ... ## $ Comments &lt;chr&gt; &quot;It is not letting me save due to \\&quot;errors\\&quot; from sect... ## $ EventId &lt;int&gt; 3272, 3272, 3272, 3272, 3272, 3272, 3272, 3272, 3272, ... ## $ ParameterId &lt;int&gt; 217, 227, 227, 229, 253, 264, 266, 268, 269, 269, 287,... ## $ ProblemId &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA... ## $ QualifierId &lt;int&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA,... ## $ QaFlagId &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ... ## $ CreatedBy &lt;chr&gt; &quot;c390249e-a157-4927-b0ee-e796f57cd173&quot;, &quot;c390249e-a157... ## $ CreatedDate &lt;chr&gt; &quot;2017-11-21T21:42:48.44Z&quot;, &quot;2017-11-21T21:42:48.44Z&quot;, ... ## $ ModifiedBy &lt;chr&gt; &quot;2473b5dc-f75b-47ea-a902-869bb804c21a&quot;, &quot;2473b5dc-f75b... ## $ ModifiedDate &lt;chr&gt; &quot;2018-01-09T20:58:08.577Z&quot;, &quot;2018-01-09T20:58:08.577Z&quot;... ## $ Parameter &lt;df[,31]&gt; &lt;data.frame[26 x 31]&gt; ## $ Qualifier &lt;df[,3]&gt; &lt;data.frame[26 x 3]&gt; ## $ Event &lt;df[,12]&gt; &lt;data.frame[26 x 12]&gt; Not bad! Within a minute or so (depending on your network speed) we have returned 20146 records for citizen data in Virginia. If you are an astute observer, you will notice that the Parameter, Qualifier, and Event columns are in fact list-columns used to efficiently store data. A full explanation of list-columns is beyond the scope of this article, but this snippet of code will unpack the data in a way we can use it for other purposes. unnested_dataOut &lt;- dataOut %&gt;% tidyr::unnest(Parameter, names_sep = &quot;.&quot;, keep_empty = TRUE) %&gt;% tidyr::unnest(Qualifier, names_sep = &quot;.&quot;, keep_empty = TRUE) %&gt;% tidyr::unnest(Event, names_sep = &quot;.&quot;, keep_empty = TRUE) names(unnested_dataOut) ## [1] &quot;Id&quot; ## [2] &quot;Value&quot; ## [3] &quot;Depth&quot; ## [4] &quot;SampleId&quot; ## [5] &quot;Comments&quot; ## [6] &quot;EventId&quot; ## [7] &quot;ParameterId&quot; ## [8] &quot;ProblemId&quot; ## [9] &quot;QualifierId&quot; ## [10] &quot;QaFlagId&quot; ## [11] &quot;CreatedBy&quot; ## [12] &quot;CreatedDate&quot; ## [13] &quot;ModifiedBy&quot; ## [14] &quot;ModifiedDate&quot; ## [15] &quot;Parameter.Id&quot; ## [16] &quot;Parameter.Code&quot; ## [17] &quot;Parameter.Name&quot; ## [18] &quot;Parameter.Units&quot; ## [19] &quot;Parameter.Method&quot; ## [20] &quot;Parameter.Tier&quot; ## [21] &quot;Parameter.DeqLevel&quot; ## [22] &quot;Parameter.Matrix&quot; ## [23] &quot;Parameter.Tidal&quot; ## [24] &quot;Parameter.NonTidal&quot; ## [25] &quot;Parameter.AnalyticalMethod&quot; ## [26] &quot;Parameter.ApprovedProcedure&quot; ## [27] &quot;Parameter.Equipment&quot; ## [28] &quot;Parameter.Precision&quot; ## [29] &quot;Parameter.Accuracy&quot; ## [30] &quot;Parameter.Range&quot; ## [31] &quot;Parameter.QcCriteria&quot; ## [32] &quot;Parameter.InspectionFreq&quot; ## [33] &quot;Parameter.InspectionType&quot; ## [34] &quot;Parameter.CalibrationFrequency&quot; ## [35] &quot;Parameter.StandardOrCalInstrumentUsed&quot; ## [36] &quot;Parameter.TierIIAdditionalReqs&quot; ## [37] &quot;Parameter.HoldingTime&quot; ## [38] &quot;Parameter.SamplePreservation&quot; ## [39] &quot;Parameter.requiresSampleDepth&quot; ## [40] &quot;Parameter.requiresDuplicate&quot; ## [41] &quot;Parameter.isCalibrationParameter&quot; ## [42] &quot;Parameter.Status&quot; ## [43] &quot;Parameter.Description&quot; ## [44] &quot;Parameter.NonfatalUpperRange&quot; ## [45] &quot;Parameter.NonfatalLowerRange&quot; ## [46] &quot;Qualifier.Id&quot; ## [47] &quot;Qualifier.Code&quot; ## [48] &quot;Qualifier.Description&quot; ## [49] &quot;Event.Id&quot; ## [50] &quot;Event.DateTime&quot; ## [51] &quot;Event.Project&quot; ## [52] &quot;Event.Comments&quot; ## [53] &quot;Event.StationId&quot; ## [54] &quot;Event.GroupId&quot; ## [55] &quot;Event.CreatedBy&quot; ## [56] &quot;Event.CreatedDate&quot; ## [57] &quot;Event.ModifiedBy&quot; ## [58] &quot;Event.ModifiedDate&quot; ## [59] &quot;Event.Station&quot; ## [60] &quot;Event.Group&quot; Lets do a little QA quickly to make sure that our data is in fact from Virginia stations (VAstations object from above) unnested_dataOut$Event.StationId[!unnested_dataOut$Event.StationId %in% VAstations$Id] ## [1] 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 ## [20] 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 ## [39] 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 252 ## [58] 252 252 252 252 252 252 252 our data is within the appropriate date window (assessmentPeriod object from above) unnested_dataOut %&gt;% # fix Event.DateTime (character) field to actual date time for QA mutate(`Event.DateTime2` = as.POSIXct( str_replace_all(Event.DateTime,c(&#39;T&#39;= &#39; &#39;, &#39;Z&#39; = &#39;&#39;)), format = &quot;%Y-%m-%d %H:%M:%S&quot;) ) %&gt;% summarise(min(Event.DateTime2), max(Event.DateTime2)) ## # A tibble: 1 x 2 ## `min(Event.DateTime2)` `max(Event.DateTime2)` ## &lt;dttm&gt; &lt;dttm&gt; ## 1 2015-01-02 14:00:00 2020-12-29 13:45:00 Interesting results! this is why it is always prudent to check and clean your data! First a note on the QA methods, the examples show both a base R and tidy method for QAing your results. One method is not more correct than the other. They each have utility depending on your use case and your personal syntax preferences. So we found that there was one station pulled back that wasnt in fact in Virginia. This tells us that querying on GroupId may not be the best method to get back only the data we want. The lesson learned is that some groups sample across state boundaries and we should use the VAstations object as a better starting point for this specific task. The second QA check demonstrated that we pulled back data from just the date range we wanted, great! This is something to check again when we change our Query to use unique StationIds as a starting point. 6.3.7 Parting Challenge Can you adjust the method above to answer the original question? Can you replace the loop with a function to complete the task? "],["scrapeDataFromFTP.html", "6.4 Scrape Data from an FTP Site", " 6.4 Scrape Data from an FTP Site Section Contact: Emma Jones (emma.jones@deq.virginia.gov) 6.4.1 What is an FTP? File transfer protocol (FTP) is a mechanism for transferring data, usually large amounts of data, over a network connection. Typically, an FTP server (sometimes called the remote host) will host data that users (sometimes called the client or local host) want to source locally. FTP protocols allow the client to connect to the remote server, identify the desired data, and download the data for local use. FTP sites can be open access or require authentication, depending on the host server configuration. Accessing data through FTP is typically reserved to older data systems as REST and API file transfer protocols have gained popularity due to privacy concerns associated with FTP systems. To understand APIs, see the Accessing APIs from R section. 6.4.2 TIGER Dataset Use Case The US Census Bureau distributes the Topologically Integrated Geographic Encoding and Referencing system (TIGER) spatial datasets for researchers to integrate into their work. Geospatial products include shapefiles, KML, and file geodatabases representing annual updates to spatial representations of geographic boundaries (congressional districts, counties, zip codes, etc.) and features (roads, railroads, coastlines, etc.). Users may use a web interface or FTP protocols to access the data. The web interface is user-friendly when smaller amounts of data are required, but for large datasets and repeated tasks, using the FTP site is optimal to automate and standardize data retrieved. The Freshwater Probabilistic Monitoring program utilizes the TIGER roads dataset, among other spatial datasets, to calculate road length, density, stream and road crossings, etc. within watersheds and riparian zones for all sites sampled statewide. This process requires analysts to retrieve new road layers every year. These road layers are stored by state and county FIPS code, so using the web interface would be challenging to retrieve all data for Virginia, but watersheds extend beyond state boundaries, so all counties with drainages to Virginia must also be compiled from North Carolina, Tennessee, West Virginia, Maryland, and the District of Columbia. It would be very easy to miss a geographic region without automation from year to year. 6.4.3 Required Packages In order to pull data from the US Census FTP site using R, users must have the following packaged loaded into your environment. library(rvest) library(tidyverse) library(stringr) library(sf) 6.4.4 Build Base FTP Web Address Call Next, we will build the web address call that will send our desired data request to the FTP server. It is best to test these URL requests iteratively in a web browser. One can parse the URL call to operationalize the logic for more automated use by separating out the year statement like the example below. The chunk also establishes the name of a directory where all data retrieved will be stored using the object name dirName. It is important to create this directory locally before pulling data or the operation will not complete because it has nowhere to place the data. # Identify year to pull data from. This is critical to getting the FTP web address correct for # all subsequent analyses. If the web address fed to the subsequent function is incorrect, then # no data can be pulled. # e.g. https://www2.census.gov/geo/tiger/TIGER2018/ is the appropriate web address for 2018 data. year &lt;- 2021 FTPaddress &lt;- paste0(&quot;https://www2.census.gov/geo/tiger/TIGER&quot;,year,&quot;/ROADS&quot;) dirName &lt;- &#39;tigerRoadsPull&#39; # create a folder in the project directory with this exact name to store all downloaded zip files 6.4.5 Custom TIGERroads FTP function Building this workflow into a custom function makes this process more reproducible and efficient. The below function uses the year and dirName objects created above to efficiently pull data from the TIGER FTP site. This function will go to the area of the TIGER FTP site specified in the custom URL, look for the file provided in the fileName argument, and download that file to the directory specified in the outDirectory argument. This function is built to handle multiple requests, so users can provided multiple fileName arguments in the form of a vector for the embedded loop to iterate over. downloadTigerRoadsByYear &lt;- function(year, # year we want data from fileName, # structured file name to retrieve outDirectory # where we want to save the results ){ for(i in 1:length(fileName)){ download.file(url = paste0(&#39;https://www2.census.gov/geo/tiger/TIGER&#39;, year,&#39;/ROADS/&#39;, fileName[i]), destfile = paste0(outDirectory,&#39;/&#39;, fileName[i])) } } To use the downloadTigerRoadsByYear() function, users need to provide one or more files to look for on the FTP site. The FTP stores each road file by concatenated state and county FIPS codes. For this example, we will pull three counties in Virginia by the appropriate FIPS codes: 51775, 51121, 51735, which equate to Salem City, Montgomery County, and Poquoson City. FIPScodes &lt;- c(&quot;51775&quot;, &quot;51121&quot;, &quot;51735&quot;) downloadTigerRoadsByYear(year, paste0(&#39;tl_&#39;,year,&#39;_&#39;, as.character(FIPScodes),&#39;_roads.zip&#39;), paste0(dirName,&#39;/&#39;, year)) When the function has completed running, the directory the data was placed in will be filled with three zip files. 6.4.6 Parse Data into Single File To use this data efficiently, we will extract each .zip file and combine them into a single spatial dataset. First, create a new directory named unzipped inside the dirName directory to house all the unzipped files. Inside this new directory named unzipped, create a new directory named the year specified in the year object. This next chunk will unzip all the files downloaded from the FTP site and place them inside the appropriate year directory in the unzipped directory. # Unzip and combine all files in the dirName directory into one shapefile filenames &lt;- list.files( paste0(dirName,&#39;/&#39;, year), pattern=&quot;*.zip&quot;, full.names=TRUE) lapply(filenames, unzip, exdir=paste0(&#39;tigerRoadsPull/unzipped/&#39;,year)) # unzip all filenames that end in zip into folder named unzipped filenames_slim &lt;- gsub(&#39;.zip&#39;, &#39;&#39; , gsub(paste0(&#39;tigerRoadsPull/&#39;,year,&#39;/&#39;),&#39;&#39;, filenames )) When pulling lots of data, there can be momentary interruptions in server connectivity that could result in a file or files not downloaded. In this chunk, we will double check that all desired files were downloaded and extracted. # check to make sure all downloaded files were unzipped correctly filenamesUnzipped &lt;- list.files(paste0(dirName,&#39;/unzipped/&#39;,year), pattern=&quot;*.shx&quot;, full.names=F) # search by .cpg bc .shp has a few extension options and duplicates unique names filenamesUnzipped_slim &lt;- gsub(&#39;.shx&#39;,&#39;&#39;,filenamesUnzipped) all(filenames_slim %in% filenamesUnzipped_slim ) # if true then cool The unzipped directory will now look like this: If files are missing, you should rerun the downloadTigerRoadsByYear() function with just the missing files, extract the data, and rerun the QA check. Here is a bit of code to help you identify which files might be missing. # if not then find out which missing filenames_slim [!(filenames_slim %in% filenamesUnzipped_slim )] # an output of character(0) is good because it means there are none missing Lastly, we will combine all the individual county files into a single shapefile to make the dataset most useful. The purrr library is very useful to map the sf::st_read function across all the files in the directory, parsing them into a single object. The last step in this chunk saves the shapefiles object locally using a specified directory schema as a shapefile for use in further analyses. You should update the directory schema to match your local environment. # Read in unzipped files and combine to single object filenamesUnzipped &lt;- paste0(dirName,&#39;/unzipped/&#39;,year,&#39;/&#39;,gsub(&#39;.shx&#39;,&#39;.shp&#39;, filenamesUnzipped)) shapefiles &lt;- filenamesUnzipped %&gt;% map(st_read) %&gt;% reduce(rbind) # Save out shapefile st_write(shapefiles, paste0(&#39;GISdata/TIGERroads/&#39;, year, &#39;tigerRoads.shp&#39;)) "]]
